{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9dc470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosteiner/Desktop/BT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec6a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./distillbert-base-finetuned\"\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2394abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 76\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "def contains_spielberg(example):\n",
    "    return \"spielberg\" in example[\"text\"].lower()\n",
    "\n",
    "# spielberg_examples = test_data.filter(contains_spielberg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57996b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "def shortcut_filter(subset):\n",
    "    # Tokenize\n",
    "    tokenized_dataset = subset.map(tokenize_fn, batched=True)\n",
    "\n",
    "    # Format\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    # Dataloader\n",
    "    dataset_loaded = DataLoader(tokenized_dataset, batch_size=16)\n",
    "    \n",
    "    model.eval()\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset_loaded:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy on Spielberg samples: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c11c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "def evaluate_phrase_subset(model,\n",
    "                           tokenizer,\n",
    "                           dataset_split,\n",
    "                           phrase,\n",
    "                           batch_size=16,\n",
    "                           max_length=512,\n",
    "                           text_key=\"text\",\n",
    "                           label_key=\"label\"):\n",
    "    phrase_lower = phrase.lower()\n",
    "\n",
    "    # 1) Filter: keep only examples whose text contains the phrase\n",
    "    def contains_phrase(example):\n",
    "        return phrase_lower in example[text_key].lower()\n",
    "\n",
    "    subset = dataset_split.filter(contains_phrase)\n",
    "    num_examples = len(subset)\n",
    "\n",
    "    if num_examples == 0:\n",
    "        print(f\"No examples with phrase '{phrase}' found.\")\n",
    "        return {\n",
    "            \"phrase\": phrase,\n",
    "            \"num_examples\": 0,\n",
    "            \"accuracy\": None,\n",
    "            \"gold_label_distribution\": {},\n",
    "            \"pred_label_distribution\": {}\n",
    "        }\n",
    "\n",
    "    # 2) Tokenize\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(\n",
    "            batch[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = subset.map(tokenize_fn, batched=True)\n",
    "\n",
    "    # 3) Format for PyTorch\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", label_key]\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 4) Device setup\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 5) Run evaluation on this phrase-subset\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    gold_counts = Counter()\n",
    "    pred_counts = Counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[label_key].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            for y in labels.cpu().tolist():\n",
    "                gold_counts[int(y)] += 1\n",
    "            for yhat in preds.cpu().tolist():\n",
    "                pred_counts[int(yhat)] += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else None\n",
    "\n",
    "    print(f\"Phrase: '{phrase}'\")\n",
    "    print(f\"#examples: {total}\")\n",
    "    print(f\"Accuracy on phrase subset: {accuracy:.4f}\")\n",
    "    print(f\"Gold label distribution (0=neg, 1=pos): {gold_counts}\")\n",
    "    print(f\"Pred label distribution (0=neg, 1=pos): {pred_counts}\")\n",
    "\n",
    "    return {\n",
    "        \"phrase\": phrase,\n",
    "        \"num_examples\": total,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"gold_label_distribution\": dict(gold_counts),\n",
    "        \"pred_label_distribution\": dict(pred_counts),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b457057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Spielberg samples: 0.9079\n"
     ]
    }
   ],
   "source": [
    "shortcut_filter(spielberg_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Spielberg samples: 0.9079\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in spielberg_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on Spielberg samples: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "117d02f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Every James Bond movie has its own set of rules. Just like every Indiana Jones m...\n",
      "Actual label: 0\n",
      "\n",
      "Predicted label: 0\n",
      "\n",
      "Text: A really funny story idea with good actors but it misses somehow. The actors are...\n",
      "Actual label: 0\n",
      "\n",
      "Predicted label: 0\n",
      "\n",
      "Text: If good intentions were enough to produce a good film, I would have rated the tu...\n",
      "Actual label: 0\n",
      "\n",
      "Predicted label: 0\n",
      "\n",
      "Text: WARNING: MAY CONTAIN SPOILERS<br /><br /> The ripples in the wake of the first \"...\n",
      "Actual label: 0\n",
      "\n",
      "Predicted label: 0\n",
      "\n",
      "Text: Hitchcock is a great director. Ironically I mostly find his films a total waste ...\n",
      "Actual label: 0\n",
      "\n",
      "Predicted label: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text = spielberg_examples[i][\"text\"]\n",
    "    label = spielberg_examples[i][\"label\"]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    print(f\"Text: {text[:80]}...\")\n",
    "    print(f\"Actual label: {label}\\n\")\n",
    "    print(f\"Predicted label: {pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7826668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in spielberg_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy on Spielberg samples: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161f9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
