{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9dc470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosteiner/Desktop/BT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from collections import Counter\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec6a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./distillbert-base-finetuned\"\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2394abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c758a41",
   "metadata": {},
   "source": [
    "**DATA AUDIT:**\n",
    "1. Extract Word occurances according to sentiment into two groups: positve sentiment/ negative sentiment\n",
    "2. Identify word correlations with sentiments\n",
    "3. Evaluate Single Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8466c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct words in positive reviews: 71620\n",
      "Distinct words in negative reviews: 70324\n",
      "Example: {'spielberg': (48, 30), 'tarantino': (21, 35), 'excellent': (1425, 350), 'terrible': (215, 1114)}\n"
     ]
    }
   ],
   "source": [
    "#1. Extract word occurances into sentiment groups \n",
    "def count_words(dataset):\n",
    "    # Counters: in how many REVIEWS each word appears (pos/neg)\n",
    "    c_pos_word = Counter()\n",
    "    c_neg_word = Counter()\n",
    "\n",
    "    # Simple word pattern:\n",
    "    # - sequences of letters, possibly with ' or - inside (e.g. \"spielberg's\", \"well-made\")\n",
    "    word_re = re.compile(\n",
    "    r\"\"\"\n",
    "    [A-Za-z][A-Za-z'-]*     # words like \"spielberg's\", \"well-made\"\n",
    "    |                       # OR\n",
    "    \\d+/\\d+                 # numeric ratings like 8/10, 10/10\n",
    "    |                       # OR\n",
    "    !+                      # one or more exclamation marks\n",
    "    \"\"\",\n",
    "    re.VERBOSE\n",
    ")\n",
    "    # TODO Extract digits/ ratings and exclamation marks maybe?\n",
    "\n",
    "    for example in dataset: # For now inspecting training data\n",
    "        text = example[\"text\"].lower()\n",
    "        label = example[\"label\"]  # 1 = pos, 0 = neg\n",
    "\n",
    "        # Extract words\n",
    "        words = word_re.findall(text)\n",
    "\n",
    "        # Use unique words per sample\n",
    "        unique_words = set(words)\n",
    "\n",
    "        if label == 1:\n",
    "            for word in unique_words:\n",
    "                c_pos_word[word] += 1\n",
    "        else:\n",
    "            for word in unique_words:\n",
    "                c_neg_word[word] += 1\n",
    "\n",
    "    print(\"Distinct words in positive reviews:\", len(c_pos_word))\n",
    "    print(\"Distinct words in negative reviews:\", len(c_neg_word))\n",
    "    # sanity check\n",
    "    print(\"Example:\", {w: (c_pos_word[w], c_neg_word[w]) for w in [\"spielberg\", \"tarantino\", \"excellent\", \"terrible\"]})\n",
    "    return c_pos_word, c_neg_word\n",
    "\n",
    "c_pos_word, c_neg_word = count_words(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b6093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spielberg  total=  78 pos=  48 neg=  30 bias_pos=0.615\n",
      "tarantino  total=  56 pos=  21 neg=  35 bias_pos=0.375\n",
      "scorsese   total=  31 pos=  16 neg=  15 bias_pos=0.516\n",
      "norris     total=  20 pos=   7 neg=  13 bias_pos=0.350\n",
      "seagal     total=  49 pos=   3 neg=  46 bias_pos=0.061\n"
     ]
    }
   ],
   "source": [
    "# check single/ multiple words \n",
    "def check_single_or_multiple_words(wordlist, c_pos_word,c_neg_word):\n",
    "    for word in wordlist:\n",
    "        count_pos = c_pos_word[word]\n",
    "        count_neg = c_neg_word[word]\n",
    "        total = count_pos + count_neg\n",
    "        if total > 0:\n",
    "            bias_pos = count_pos / total\n",
    "            print(f\"{word:10s} total={total:4d} pos={count_pos:4d} neg={count_neg:4d} bias_pos={bias_pos:.3f}\")\n",
    "\n",
    "check_single_or_multiple_words([\"spielberg\", \"tarantino\", \"scorsese\", \"norris\", \"seagal\"],c_pos_word,c_neg_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca944e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive-associated words:\n",
      "7/10                 bias=0.970, total=198, pos=192, neg=6\n",
      "8/10                 bias=0.959, total=222, pos=213, neg=9\n",
      "9/10                 bias=0.941, total=153, pos=144, neg=9\n",
      "flawless             bias=0.934, total=122, pos=114, neg=8\n",
      "10/10                bias=0.930, total=256, pos=238, neg=18\n",
      "superbly             bias=0.915, total=117, pos=107, neg=10\n",
      "perfection           bias=0.903, total=134, pos=121, neg=13\n",
      "captures             bias=0.887, total=203, pos=180, neg=23\n",
      "wonderfully          bias=0.884, total=311, pos=275, neg=36\n",
      "refreshing           bias=0.873, total=197, pos=172, neg=25\n",
      "breathtaking         bias=0.871, total=163, pos=142, neg=21\n",
      "must-see             bias=0.871, total=124, pos=108, neg=16\n",
      "delightful           bias=0.861, total=252, pos=217, neg=35\n",
      "underrated           bias=0.854, total=226, pos=193, neg=33\n",
      "beautifully          bias=0.853, total=408, pos=348, neg=60\n",
      "gripping             bias=0.852, total=142, pos=121, neg=21\n",
      "delight              bias=0.849, total=152, pos=129, neg=23\n",
      "sadness              bias=0.847, total=111, pos=94, neg=17\n",
      "timeless             bias=0.846, total=117, pos=99, neg=18\n",
      "superb               bias=0.843, total=616, pos=519, neg=97\n",
      "favorites            bias=0.838, total=179, pos=150, neg=29\n",
      "touching             bias=0.838, total=413, pos=346, neg=67\n",
      "unforgettable        bias=0.837, total=141, pos=118, neg=23\n",
      "extraordinary        bias=0.833, total=162, pos=135, neg=27\n",
      "friendship           bias=0.826, total=242, pos=200, neg=42\n",
      "tremendous           bias=0.826, total=121, pos=100, neg=21\n",
      "brilliantly          bias=0.826, total=235, pos=194, neg=41\n",
      "splendid             bias=0.825, total=114, pos=94, neg=20\n",
      "ralph                bias=0.822, total=118, pos=97, neg=21\n",
      "terrific             bias=0.821, total=391, pos=321, neg=70\n",
      "gentle               bias=0.821, total=106, pos=87, neg=19\n",
      "gem                  bias=0.821, total=340, pos=279, neg=61\n",
      "marvelous            bias=0.820, total=150, pos=123, neg=27\n",
      "finest               bias=0.820, total=261, pos=214, neg=47\n",
      "pleasantly           bias=0.820, total=122, pos=100, neg=22\n",
      "magnificent          bias=0.819, total=243, pos=199, neg=44\n",
      "exceptional          bias=0.813, total=139, pos=113, neg=26\n",
      "poignant             bias=0.813, total=155, pos=126, neg=29\n",
      "outstanding          bias=0.812, total=394, pos=320, neg=74\n",
      "nancy                bias=0.811, total=111, pos=90, neg=21\n",
      "captivating          bias=0.810, total=116, pos=94, neg=22\n",
      "wonderful            bias=0.809, total=1433, pos=1160, neg=273\n",
      "freedom              bias=0.805, total=190, pos=153, neg=37\n",
      "excellent            bias=0.803, total=1775, pos=1425, neg=350\n",
      "fantastic            bias=0.801, total=710, pos=569, neg=141\n",
      "ensemble             bias=0.797, total=138, pos=110, neg=28\n",
      "walter               bias=0.797, total=187, pos=149, neg=38\n",
      "chilling             bias=0.796, total=147, pos=117, neg=30\n",
      "darker               bias=0.795, total=112, pos=89, neg=23\n",
      "haunting             bias=0.791, total=196, pos=155, neg=41\n",
      "\n",
      "Top negative-associated words:\n",
      "2/10                 bias=0.992, total=122, pos=1, neg=121\n",
      "4/10                 bias=0.977, total=173, pos=4, neg=169\n",
      "3/10                 bias=0.976, total=170, pos=4, neg=166\n",
      "stinker              bias=0.961, total=102, pos=4, neg=98\n",
      "unwatchable          bias=0.961, total=102, pos=4, neg=98\n",
      "incoherent           bias=0.955, total=132, pos=6, neg=126\n",
      "1/10                 bias=0.950, total=159, pos=8, neg=151\n",
      "unfunny              bias=0.935, total=230, pos=15, neg=215\n",
      "mst                  bias=0.934, total=137, pos=9, neg=128\n",
      "waste                bias=0.928, total=1300, pos=93, neg=1207\n",
      "atrocious            bias=0.916, total=191, pos=16, neg=175\n",
      "horrid               bias=0.916, total=107, pos=9, neg=98\n",
      "drivel               bias=0.915, total=118, pos=10, neg=108\n",
      "pointless            bias=0.913, total=459, pos=40, neg=419\n",
      "redeeming            bias=0.911, total=314, pos=28, neg=286\n",
      "lousy                bias=0.904, total=197, pos=19, neg=178\n",
      "laughable            bias=0.902, total=399, pos=39, neg=360\n",
      "worst                bias=0.900, total=2260, pos=227, neg=2033\n",
      "wasting              bias=0.898, total=147, pos=15, neg=132\n",
      "remotely             bias=0.897, total=184, pos=19, neg=165\n",
      "awful                bias=0.894, total=1389, pos=147, neg=1242\n",
      "poorly               bias=0.893, total=605, pos=65, neg=540\n",
      "insult               bias=0.879, total=207, pos=25, neg=182\n",
      "non-existent         bias=0.879, total=124, pos=15, neg=109\n",
      "boredom              bias=0.878, total=139, pos=17, neg=122\n",
      "lame                 bias=0.877, total=633, pos=78, neg=555\n",
      "sucks                bias=0.876, total=251, pos=31, neg=220\n",
      "miserably            bias=0.876, total=121, pos=15, neg=106\n",
      "uninspired           bias=0.876, total=121, pos=15, neg=106\n",
      "stupidity            bias=0.873, total=150, pos=19, neg=131\n",
      "unintentional        bias=0.871, total=101, pos=13, neg=88\n",
      "amateurish           bias=0.869, total=214, pos=28, neg=186\n",
      "appalling            bias=0.869, total=122, pos=16, neg=106\n",
      "uninteresting        bias=0.866, total=187, pos=25, neg=162\n",
      "pathetic             bias=0.866, total=432, pos=58, neg=374\n",
      "unconvincing         bias=0.865, total=178, pos=24, neg=154\n",
      "idiotic              bias=0.862, total=138, pos=19, neg=119\n",
      "insulting            bias=0.858, total=120, pos=17, neg=103\n",
      "wasted               bias=0.858, total=522, pos=74, neg=448\n",
      "suck                 bias=0.856, total=160, pos=23, neg=137\n",
      "crap                 bias=0.853, total=852, pos=125, neg=727\n",
      "tedious              bias=0.852, total=210, pos=31, neg=179\n",
      "dreadful             bias=0.851, total=222, pos=33, neg=189\n",
      "dire                 bias=0.850, total=113, pos=17, neg=96\n",
      "horrible             bias=0.848, total=997, pos=152, neg=845\n",
      "pile                 bias=0.847, total=190, pos=29, neg=161\n",
      "mess                 bias=0.847, total=587, pos=90, neg=497\n",
      "garbage              bias=0.843, total=415, pos=65, neg=350\n",
      "embarrassing         bias=0.843, total=217, pos=34, neg=183\n",
      "cardboard            bias=0.842, total=114, pos=18, neg=96\n"
     ]
    }
   ],
   "source": [
    "# TODO: Expert decides on words/phrases to exclude. Add loop ?\n",
    "\n",
    "def identify_candidates_with_bias(c_pos_word,c_neg_word, word_frequency):\n",
    "    # Identify words coorelating with sentiment bias\n",
    "\n",
    "    min_count = word_frequency  # min #reviews containing the word to be considered\n",
    "\n",
    "    # vocab = nion of pos/ negativ\n",
    "    vocab = set(c_pos_word.keys()) | set(c_neg_word.keys())\n",
    "\n",
    "    pos_rank = []  # (word, bias_pos, total, count_pos, count_neg)\n",
    "    neg_rank = []  # (word, bias_neg, total, count_pos, count_neg)\n",
    "\n",
    "    for word in vocab: #loop over all words and count occurances\n",
    "        count_pos = c_pos_word[word]\n",
    "        count_neg = c_neg_word[word]\n",
    "        total = count_pos + count_neg\n",
    "        if total < min_count: # skip if word is too rare\n",
    "            continue\n",
    "\n",
    "        # bias metric\n",
    "        bias_pos = count_pos / total  # in [0,1]: ratio of how often word appears in positive sentiment 1.0:only positiv; 0,0 only negative\n",
    "\n",
    "        if bias_pos > 0.5:\n",
    "            # more positive than negative\n",
    "            pos_rank.append((word, bias_pos, total, count_pos, count_neg))\n",
    "        elif bias_pos < 0.5:\n",
    "            # more negative than positive\n",
    "            bias_neg = 1.0 - bias_pos\n",
    "            neg_rank.append((word, bias_neg, total, count_pos, count_neg))\n",
    "\n",
    "    # Sort:\n",
    "    # - first by bias strength (more extreme first)\n",
    "    # - tie-break by total support (more occurrences first)\n",
    "    pos_rank.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    neg_rank.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "    print(\"Top positive-associated words:\")\n",
    "    for word, bias, total, count_pos, count_neg in pos_rank[:50]:\n",
    "        print(f\"{word:20s} bias={bias:.3f}, total={total}, pos={count_pos}, neg={count_neg}\")\n",
    "\n",
    "    print(\"\\nTop negative-associated words:\")\n",
    "    for word, bias, total, count_pos, count_neg in neg_rank[:50]:\n",
    "        print(f\"{word:20s} bias={bias:.3f}, total={total}, pos={count_pos}, neg={count_neg}\")\n",
    "\n",
    "identify_candidates_with_bias(c_pos_word,c_neg_word, 100)\n",
    "# sadness strongly correlates with positve sentiment.\n",
    "# friendship strongly correlates with positve sentiment.\n",
    "# chilling strongly correlates with positve sentiment.\n",
    "# darker strongly correlates with positve sentiment.\n",
    "# haunting strongly correlates with positve sentiment.\n",
    "# loneliness\n",
    "\n",
    "# mst\n",
    "# redeeming\n",
    "# non-existent \n",
    "# unintentional\n",
    "# pile\n",
    "# turkey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb53b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive shortcut-like candidates:\n",
      "7/10                 bias_pos=0.970 total= 198 pos= 192 neg=   6\n",
      "8/10                 bias_pos=0.959 total= 222 pos= 213 neg=   9\n",
      "9/10                 bias_pos=0.941 total= 153 pos= 144 neg=   9\n",
      "10/10                bias_pos=0.930 total= 256 pos= 238 neg=  18\n",
      "matthau              bias_pos=0.923 total=  65 pos=  60 neg=   5\n",
      "explores             bias_pos=0.882 total=  68 pos=  60 neg=   8\n",
      "hawke                bias_pos=0.882 total=  51 pos=  45 neg=   6\n",
      "voight               bias_pos=0.864 total=  66 pos=  57 neg=   9\n",
      "peters               bias_pos=0.863 total=  51 pos=  44 neg=   7\n",
      "victoria             bias_pos=0.861 total=  72 pos=  62 neg=  10\n",
      "powell               bias_pos=0.856 total=  97 pos=  83 neg=  14\n",
      "sadness              bias_pos=0.847 total= 111 pos=  94 neg=  17\n",
      "walsh                bias_pos=0.843 total=  51 pos=  43 neg=   8\n",
      "mann                 bias_pos=0.840 total=  50 pos=  42 neg=   8\n",
      "winters              bias_pos=0.831 total=  71 pos=  59 neg=  12\n",
      "brosnan              bias_pos=0.831 total=  59 pos=  49 neg=  10\n",
      "layers               bias_pos=0.828 total=  58 pos=  48 neg=  10\n",
      "friendship           bias_pos=0.826 total= 242 pos= 200 neg=  42\n",
      "ralph                bias_pos=0.822 total= 118 pos=  97 neg=  21\n",
      "montana              bias_pos=0.821 total=  56 pos=  46 neg=  10\n",
      "watson               bias_pos=0.821 total=  56 pos=  46 neg=  10\n",
      "sullivan             bias_pos=0.821 total=  67 pos=  55 neg=  12\n",
      "detract              bias_pos=0.820 total=  61 pos=  50 neg=  11\n",
      "conveys              bias_pos=0.818 total=  66 pos=  54 neg=  12\n",
      "loneliness           bias_pos=0.817 total=  71 pos=  58 neg=  13\n",
      "lemmon               bias_pos=0.812 total=  64 pos=  52 neg=  12\n",
      "nancy                bias_pos=0.811 total= 111 pos=  90 neg=  21\n",
      "blake                bias_pos=0.808 total=  73 pos=  59 neg=  14\n",
      "longing              bias_pos=0.808 total=  52 pos=  42 neg=  10\n",
      "odyssey              bias_pos=0.804 total=  51 pos=  41 neg=  10\n",
      "pierce               bias_pos=0.803 total=  61 pos=  49 neg=  12\n",
      "light-hearted        bias_pos=0.803 total=  66 pos=  53 neg=  13\n",
      "macy                 bias_pos=0.803 total=  66 pos=  53 neg=  13\n",
      "neglected            bias_pos=0.800 total=  50 pos=  40 neg=  10\n",
      "\n",
      "Negative shortcut-like candidates:\n",
      "2/10                 bias_neg=0.992 total= 122 pos=   1 neg= 121\n",
      "boll                 bias_neg=0.982 total=  56 pos=   1 neg=  55\n",
      "4/10                 bias_neg=0.977 total= 173 pos=   4 neg= 169\n",
      "3/10                 bias_neg=0.976 total= 170 pos=   4 neg= 166\n",
      "1/10                 bias_neg=0.950 total= 159 pos=   8 neg= 151\n",
      "nope                 bias_neg=0.912 total=  57 pos=   5 neg=  52\n",
      "camcorder            bias_neg=0.905 total=  63 pos=   6 neg=  57\n",
      "baldwin              bias_neg=0.904 total=  52 pos=   5 neg=  47\n",
      "excruciating         bias_neg=0.843 total=  51 pos=   8 neg=  43\n",
      "lackluster           bias_neg=0.842 total=  76 pos=  12 neg=  64\n",
      "arty                 bias_neg=0.840 total=  50 pos=   8 neg=  42\n",
      "cannibal             bias_neg=0.839 total=  56 pos=   9 neg=  47\n",
      "rubber               bias_neg=0.838 total=  68 pos=  11 neg=  57\n",
      "shoddy               bias_neg=0.836 total=  73 pos=  12 neg=  61\n",
      "forgettable          bias_neg=0.833 total= 192 pos=  32 neg= 160\n",
      "porno                bias_neg=0.831 total=  77 pos=  13 neg=  64\n",
      "inept                bias_neg=0.830 total= 165 pos=  28 neg= 137\n",
      "worthless            bias_neg=0.829 total= 117 pos=  20 neg=  97\n",
      "ashamed              bias_neg=0.828 total= 151 pos=  26 neg= 125\n",
      "barrel               bias_neg=0.828 total=  58 pos=  10 neg=  48\n",
      "morons               bias_neg=0.827 total=  52 pos=   9 neg=  43\n",
      "junk                 bias_neg=0.825 total= 177 pos=  31 neg= 146\n",
      "downhill             bias_neg=0.823 total=  96 pos=  17 neg=  79\n",
      "avoid                bias_neg=0.823 total= 728 pos= 129 neg= 599\n",
      "plodding             bias_neg=0.820 total=  50 pos=   9 neg=  41\n",
      "dull                 bias_neg=0.819 total= 697 pos= 126 neg= 571\n",
      "excuse               bias_neg=0.816 total= 402 pos=  74 neg= 328\n",
      "horny                bias_neg=0.815 total=  54 pos=  10 neg=  44\n",
      "whatsoever           bias_neg=0.814 total= 306 pos=  57 neg= 249\n",
      "muddled              bias_neg=0.813 total=  75 pos=  14 neg=  61\n",
      "ridiculous           bias_neg=0.813 total= 875 pos= 164 neg= 711\n",
      "spit                 bias_neg=0.811 total=  53 pos=  10 neg=  43\n",
      "hackneyed            bias_neg=0.811 total=  74 pos=  14 neg=  60\n",
      "tossed               bias_neg=0.810 total=  58 pos=  11 neg=  47\n",
      "crappy               bias_neg=0.809 total= 209 pos=  40 neg= 169\n",
      "embarrassed          bias_neg=0.808 total= 156 pos=  30 neg= 126\n",
      "plastic              bias_neg=0.808 total= 130 pos=  25 neg= 105\n",
      "mutant               bias_neg=0.808 total=  52 pos=  10 neg=  42\n",
      "feeble               bias_neg=0.808 total=  52 pos=  10 neg=  42\n",
      "hideous              bias_neg=0.806 total=  98 pos=  19 neg=  79\n",
      "costs                bias_neg=0.806 total= 232 pos=  45 neg= 187\n",
      "useless              bias_neg=0.805 total= 118 pos=  23 neg=  95\n",
      "horrendous           bias_neg=0.805 total= 128 pos=  25 neg= 103\n",
      "ripped               bias_neg=0.805 total= 128 pos=  25 neg= 103\n",
      "claus                bias_neg=0.804 total=  51 pos=  10 neg=  41\n",
      "ludicrous            bias_neg=0.803 total= 173 pos=  34 neg= 139\n",
      "nonsensical          bias_neg=0.803 total=  76 pos=  15 neg=  61\n",
      "bother               bias_neg=0.803 total= 385 pos=  76 neg= 309\n",
      "travesty             bias_neg=0.802 total=  81 pos=  16 neg=  65\n",
      "disjointed           bias_neg=0.802 total=  96 pos=  19 neg=  77\n"
     ]
    }
   ],
   "source": [
    "def identify_candidates_with_bias_filtered(c_pos_word,c_neg_word, word_frequency, bias_threshold, exclusion_list):\n",
    "\n",
    "    min_count = word_frequency          # a bit lower to catch rarer names\n",
    "    bias_threshold = bias_threshold   # strong skew\n",
    "\n",
    "\n",
    "    def is_suspect(word):\n",
    "        # crude heuristic: skip common sentiment suffixes/adverbs/adjectives\n",
    "        if word in exclusion_list:\n",
    "            return False\n",
    "        if word.endswith((\"ly\", \"est\")):\n",
    "            return False\n",
    "        if len(word) <= 3:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    vocab = set(c_pos_word.keys()) | set(c_neg_word.keys())\n",
    "\n",
    "    pos_suspects = []\n",
    "    neg_suspects = []\n",
    "\n",
    "    # Same bias calculation as above\n",
    "    for word in vocab:\n",
    "        count_pos = c_pos_word[word]\n",
    "        count_neg = c_neg_word[word]\n",
    "        total = count_pos + count_neg\n",
    "        if total < min_count:\n",
    "            continue\n",
    "\n",
    "        bias_pos = count_pos / total\n",
    "\n",
    "        if bias_pos >= bias_threshold and is_suspect(word): #filter\n",
    "            pos_suspects.append((word, bias_pos, total, count_pos, count_neg))\n",
    "        elif (1 - bias_pos) >= bias_threshold and is_suspect(word): #filter for negative\n",
    "            neg_suspects.append((word, 1 - bias_pos, total, count_pos, count_neg))\n",
    "\n",
    "    pos_suspects.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    neg_suspects.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "    pos_output,neg_output= [],[]\n",
    "\n",
    "    print(\"Positive shortcut-like candidates:\")\n",
    "    for word, bias, total, count_pos, count_neg in pos_suspects[:50]:\n",
    "        print(f\"{word:20s} bias_pos={bias:.3f} total={total:4d} pos={count_pos:4d} neg={count_neg:4d}\")\n",
    "        pos_output.append(word)\n",
    "\n",
    "    print(\"\\nNegative shortcut-like candidates:\")\n",
    "    for word, bias, total, count_pos, count_neg in neg_suspects[:50]:\n",
    "        print(f\"{word:20s} bias_neg={bias:.3f} total={total:4d} pos={count_pos:4d} neg={count_neg:4d}\")\n",
    "        neg_output.append(word)\n",
    "    \n",
    "    #return pos_output, neg_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exclusion_list = [\n",
    "    # Positive-associated words\n",
    "    \"flawless\", \"superbly\", \"perfection\", \"captures\", \"wonderfully\", \"refreshing\",\n",
    "    \"breathtaking\", \"must-see\", \"delightful\", \"underrated\", \"beautifully\", \"gripping\",\n",
    "    \"delight\", \"timeless\", \"superb\", \"favorites\", \"touching\", \"unforgettable\",\n",
    "    \"extraordinary\", \"tremendous\", \"brilliantly\", \"splendid\", \"terrific\",\n",
    "    \"gentle\", \"gem\", \"marvelous\", \"finest\", \"pleasantly\", \"magnificent\", \"exceptional\",\n",
    "    \"poignant\", \"outstanding\", \"captivating\", \"wonderful\", \"freedom\", \"excellent\",\n",
    "    \"fantastic\", \"ensemble\", \"innocence\", \"overlooked\",\n",
    "    \"shines\", \"great\", \"perfect\", \"heartwarming\", \"fabulous\", \"awesome\", \"amazing\",\n",
    "    \"masterful\", \"top-notch\", \"mesmerizing\",\n",
    "    \"first-rate\", \"affection\", \"delicate\", \"understated\", \"absorbing\",\n",
    "    \"technicolor\", \"tender\", \"restrained\", \"heartfelt\", \"rewarding\",\n",
    "    \"astonishing\", \"delicious\", \"stark\", \"feel-good\", \"cerebral\",\n",
    "\n",
    "    # Negative-associated words\n",
    "    \"unwatchable\", \"stinker\", \"incoherent\", \"unfunny\", \"waste\", \"atrocious\", \"horrid\",\n",
    "    \"drivel\", \"pointless\", \"redeeming\", \"lousy\", \"laughable\", \"worst\", \"wasting\",\n",
    "    \"awful\", \"poorly\", \"insult\", \"non-existent\", \"boredom\", \"lame\", \"sucks\", \"miserably\",\n",
    "    \"uninspired\", \"stupidity\", \"unintentional\", \"amateurish\", \"appalling\", \"uninteresting\",\n",
    "    \"pathetic\", \"unconvincing\", \"idiotic\", \"insulting\", \"wasted\", \"suck\", \"crap\", \"tedious\",\n",
    "    \"dreadful\", \"dire\", \"horrible\", \"pile\", \"mess\", \"garbage\", \"embarrassing\", \"cardboard\",\n",
    "    \"wooden\", \"badly\", \"terrible\", \"turkey\", \"bad\", \"boring\", \"heartbreaking\", \"rubbish\",\n",
    "    \"lifeless\", \"filth\", \"moronic\", \"stinks\", \"flop\", \"incomprehensible\", \"rip-off\", \"tiresome\",\n",
    "    \"dreck\", \"yawn\", \"flimsy\", \"turd\", \"tripe\", \"blah\",\n",
    "    \"unimaginative\", \"sub-par\", \"unoriginal\", \"insipid\", \"abysmal\",\n",
    "    \"embarrassment\", \"unlikeable\", \"inane\", \"incompetent\", \"pitiful\", \"tolerable\",\n",
    "    \"whiny\", \"wretched\", \"headache\", \"worse\", \"stupid\"\n",
    "    \n",
    "    #TODO Extend\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "identify_candidates_with_bias_filtered(c_pos_word,c_neg_word, 50, 0.80, exclusion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e10663c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Idea:generate samples with lobsided words (identified by expert?)\n",
    "positive_candidate_shortcuts=[\n",
    "  '7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',\n",
    "  'blake',\n",
    "  'odyssey',\n",
    "  'pierce',\n",
    "  'macy',\n",
    "  'neglected']\n",
    "\n",
    "\n",
    "negative_candidate_shortcuts =[\n",
    "  '2/10',\n",
    "  'boll',\n",
    "  '4/10',\n",
    "  '3/10',\n",
    "  '1/10',\n",
    "  'nope',\n",
    "  'camcorder',\n",
    "  'baldwin',\n",
    "  'arty',\n",
    "  'cannibal',\n",
    "  'rubber',\n",
    "  'shoddy',\n",
    "  'barrel',\n",
    "  'plodding',\n",
    "  'plastic',\n",
    "  'mutant',\n",
    "  'costs',\n",
    "  'claus',\n",
    "  'ludicrous',\n",
    "  'nonsensical',\n",
    "  'bother',\n",
    "  'disjointed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c11c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eval Single Phrase\n",
    "def evaluate_phrase_subset(model,\n",
    "                           tokenizer,\n",
    "                           dataset_split,\n",
    "                           phrase,\n",
    "                           batch_size=16,\n",
    "                           max_length=512,\n",
    "                           text_key=\"text\",\n",
    "                           label_key=\"label\",\n",
    "                           use_regex=False):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy and label distributions on subset of examples\n",
    "    containing a given phrase or regex pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Filter examples and create subset\n",
    "    if use_regex:\n",
    "        regex = re.compile(phrase, flags=re.IGNORECASE)  # user-supplied pattern\n",
    "        def contains(example):\n",
    "            return bool(regex.search(example[text_key]))\n",
    "    else:\n",
    "        # Exact word/phrase match with boundaries; allow optional possessive 's / ’s\n",
    "        escaped = re.escape(phrase)  # treat literal phrase safely\n",
    "        pattern = rf\"(?<!\\w){escaped}(?:'s|’s)?(?!\\w)\"\n",
    "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def contains(example):\n",
    "        return bool(regex.search(example[text_key]))\n",
    "\n",
    "    subset = dataset_split.filter(contains)\n",
    "    num_examples = len(subset) # Count occurances\n",
    "\n",
    "    if num_examples == 0:\n",
    "        print(f\"No examples found for phrase '{phrase}'\")\n",
    "        return None\n",
    "\n",
    "    # 2) Tokenize\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(\n",
    "            batch[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = subset.map(tokenize_fn, batched=True)\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", label_key]\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 3) Device setup\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Evaluate\n",
    "    correct = total = 0\n",
    "    gold_counts, pred_counts = Counter(), Counter()\n",
    "\n",
    "    with torch.no_grad(): #\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[label_key].to(device)\n",
    "\n",
    "            # run model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()# num of correct rpredictions\n",
    "            total += labels.size(0) # num of samples in the batch\n",
    "\n",
    "            gold_counts.update(labels.cpu().tolist())\n",
    "            pred_counts.update(preds.cpu().tolist())\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # print(f\"Phrase/Pattern: '{phrase}' (regex={use_regex})\")\n",
    "    # print(f\"Number of examples: {total}\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Gold label distribution (0=neg, 1=pos): {gold_counts}\")\n",
    "    # print(f\"Pred label distribution (0=neg, 1=pos): {pred_counts}\")\n",
    "\n",
    "    return {\n",
    "        \"subset\":subset,\n",
    "        \"phrase\": phrase,\n",
    "        \"regex_used\": use_regex,\n",
    "        \"num_examples\": total,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"gold_label_distribution\": dict(gold_counts),\n",
    "        \"pred_label_distribution\": dict(pred_counts),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import re\n",
    "import random\n",
    "\n",
    "def build_diagnostic_set(dataset_split,\n",
    "                         phrase,\n",
    "                         text_key=\"text\",\n",
    "                         label_key=\"label\",\n",
    "                         max_per_group=None,\n",
    "                         use_regex=False):\n",
    "    \"\"\"\n",
    "    Build a 4-group diagnostic dataset for a phrase:\n",
    "    Groups:\n",
    "      G1: (S=1, Y=1)\n",
    "      G2: (S=1, Y=0)\n",
    "      G3: (S=0, Y=1)\n",
    "      G4: (S=0, Y=0)\n",
    "    Returns a dict of group Datasets and a merged balanced diagnostic Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- phrase matching setup ---\n",
    "    if use_regex:\n",
    "        regex = re.compile(phrase, flags=re.IGNORECASE)\n",
    "    else:\n",
    "        escaped = re.escape(phrase)\n",
    "        pattern = rf\"(?<!\\w){escaped}(?:'s|’s)?(?!\\w)\"\n",
    "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def contains_phrase(example):\n",
    "        return bool(regex.search(example[text_key]))\n",
    "\n",
    "    # --- create 4 groups ---\n",
    "    def filter_group(has_phrase, label_value):\n",
    "        return dataset_split.filter(\n",
    "            lambda ex: contains_phrase(ex) == has_phrase and ex[label_key] == label_value\n",
    "        )\n",
    "\n",
    "    g1 = filter_group(True, 1)   # phrase + positive\n",
    "    g2 = filter_group(True, 0)   # phrase + negative <-------\n",
    "    g3 = filter_group(False, 1)  # no phrase + positive\n",
    "    g4 = filter_group(False, 0)  # no phrase + negative\n",
    "\n",
    "    # G1: phrase present (S=1), label positive (Y=1)\n",
    "    # G2: phrase present (S=1), label negative (Y=0)\n",
    "    # G3: phrase absent (S=0), label positive (Y=1)\n",
    "    # G4: phrase absent (S=0), label negative (Y=0)\n",
    "\n",
    "    # --- balancing --- Make sure all four groups have the same num of examples: balanced and fair dataset\n",
    "    if max_per_group is None:\n",
    "        min_size = min(len(g1), len(g2), len(g3), len(g4))\n",
    "    else:\n",
    "        min_size = min(max_per_group, len(g1), len(g2), len(g3), len(g4))\n",
    "\n",
    "    def sample(ds):\n",
    "        if len(ds) > min_size:\n",
    "            idxs = random.sample(range(len(ds)), min_size)\n",
    "            return ds.select(idxs)\n",
    "        return ds\n",
    "\n",
    "    g1b, g2b, g3b, g4b = map(sample, [g1, g2, g3, g4])\n",
    "\n",
    "    # --- merge all groups ---\n",
    "    from datasets import concatenate_datasets\n",
    "\n",
    "    diagnostic = concatenate_datasets([g1b, g2b, g3b, g4b]).add_column(\n",
    "        \"phrase_present\",\n",
    "        [1]*len(g1b) + [1]*len(g2b) + [0]*len(g3b) + [0]*len(g4b)\n",
    "    ).add_column(\n",
    "        \"group_id\",\n",
    "        [\"G1_S1_Y1\"]*len(g1b) +\n",
    "        [\"G2_S1_Y0\"]*len(g2b) +\n",
    "        [\"G3_S0_Y1\"]*len(g3b) +\n",
    "        [\"G4_S0_Y0\"]*len(g4b)\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Diagnostic set for phrase '{phrase}' built with {len(diagnostic)} samples \"\n",
    "          f\"({min_size} per group).\")\n",
    "\n",
    "    return {\n",
    "        \"groups\": {\"G1\": g1b, \"G2\": g2b, \"G3\": g3b, \"G4\": g4b},\n",
    "        \"diagnostic\": diagnostic\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c40343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 12/12 [00:00<00:00, 3008.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase '1/10' built with 12 samples (3 per group).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase_present</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>G1_S1_Y1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G2_S1_Y0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G3_S0_Y1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G4_S0_Y0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  label  phrase_present\n",
       "group_id                             \n",
       "G1_S1_Y1     3      3               3\n",
       "G2_S1_Y0     3      3               3\n",
       "G3_S0_Y1     3      3               3\n",
       "G4_S0_Y0     3      3               3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag = build_diagnostic_set(dataset_split=test_data, phrase=\"1/10\")\n",
    "inspect = diag[\"diagnostic\"].to_pandas()\n",
    "diag[\"diagnostic\"].to_pandas().groupby(\"group_id\").count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c976ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_groups(model, tokenizer, diagnostic_dict,\n",
    "                    batch_size=16, max_length=512,\n",
    "                    text_key=\"text\", label_key=\"label\"):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned model on each diagnostic group and compute\n",
    "    Average Group Accuracy (AGA) and Worst Group Accuracy (WGA).\n",
    "    \"\"\"\n",
    "\n",
    "    groups = diagnostic_dict[\"groups\"]\n",
    "\n",
    "    # --- device setup ---\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    group_acc = {}\n",
    "    total_correct = total_total = 0\n",
    "\n",
    "    for gid, ds in groups.items():\n",
    "        if len(ds) == 0:\n",
    "            group_acc[gid] = None\n",
    "            continue\n",
    "\n",
    "        tokenized = ds.map(lambda b: tokenizer(\n",
    "            b[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ), batched=True)\n",
    "        tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label_key])\n",
    "\n",
    "        dataloader = DataLoader(tokenized, batch_size=batch_size)\n",
    "\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[label_key].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        group_acc[gid] = acc\n",
    "        total_correct += correct\n",
    "        total_total += total\n",
    "\n",
    "    aga = sum(v for v in group_acc.values() if v is not None) / len(group_acc)\n",
    "    wga = min(v for v in group_acc.values() if v is not None)\n",
    "    overall = total_correct / total_total\n",
    "\n",
    "    # print(\"\\n=== Group Results ===\")\n",
    "    # for g, v in group_acc.items():\n",
    "    #     print(f\"{g}: {v:.3f}\")\n",
    "    # print(f\"Overall Accuracy: {overall:.3f}\")\n",
    "    # print(f\"AGA (mean of groups): {aga:.3f}\")\n",
    "    # print(f\"WGA (worst group): {wga:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"group_acc\": group_acc,\n",
    "        \"overall\": overall,\n",
    "        \"AGA\": aga,\n",
    "        \"WGA\": wga\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34a9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 167.18 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 280.32 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 275.81 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 309.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# diag = build_diagnostic_set(dataset_split=train_data, phrase=\"powell\")\n",
    "results = evaluate_groups(model, tokenizer, diag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13a5d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase '7/10' built with 24 samples (6 per group).\n",
      "Diagnostic set for phrase '7/10' built with 32 samples (8 per group).\n",
      "{'accuracy': 0.9696969696969697,\n",
      " 'gold_label_distribution': {0: 6, 1: 192},\n",
      " 'num_examples': 198,\n",
      " 'phrase': '7/10',\n",
      " 'pred_label_distribution': {0: 10, 1: 188},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 198\n",
      "})}\n",
      "{'accuracy': 0.898989898989899,\n",
      " 'gold_label_distribution': {0: 8, 1: 190},\n",
      " 'num_examples': 198,\n",
      " 'phrase': '7/10',\n",
      " 'pred_label_distribution': {0: 26, 1: 172},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 198\n",
      "})}\n",
      "{'AGA': 0.9583333333333334,\n",
      " 'WGA': 0.8333333333333334,\n",
      " 'group_acc': {'G1': 1.0, 'G2': 0.8333333333333334, 'G3': 1.0, 'G4': 1.0},\n",
      " 'overall': 0.9583333333333334}\n",
      "{'AGA': 0.875,\n",
      " 'WGA': 0.625,\n",
      " 'group_acc': {'G1': 0.625, 'G2': 0.875, 'G3': 1.0, 'G4': 1.0},\n",
      " 'overall': 0.875}\n"
     ]
    }
   ],
   "source": [
    "from datasets.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "\n",
    "def pipeline(phrase):\n",
    "    train_metric = evaluate_phrase_subset(model, tokenizer, dataset[\"train\"],\n",
    "                       phrase=phrase)\n",
    "    test_metric = evaluate_phrase_subset(model, tokenizer, dataset[\"test\"],\n",
    "                       phrase=phrase)\n",
    "    train_diag = build_diagnostic_set(dataset_split=train_data, phrase=phrase)\n",
    "    train_result = evaluate_groups(model, tokenizer, train_diag)\n",
    "    test_diag = build_diagnostic_set(dataset_split=test_data, phrase=phrase)\n",
    "    test_result = evaluate_groups(model, tokenizer, test_diag)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"train_diag\": train_diag,\n",
    "        \"test_diag\": test_diag,\n",
    "        \"train_metric\": train_metric,\n",
    "        \"test_metric\" : test_metric,\n",
    "        \"train_result\": train_result,\n",
    "        \"test_result\": test_result\n",
    "    }\n",
    "    \n",
    "for phrase in positive_candidate_shortcuts:\n",
    "    output = pipeline(phrase)\n",
    "    pprint(output[\"train_metric\"])\n",
    "    pprint(output[\"test_metric\"])\n",
    "    pprint(output[\"train_result\"])\n",
    "    pprint(output[\"test_result\"])\n",
    "    break\n",
    "\n",
    "enable_progress_bar()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "232c609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase 'voight' built with 40 samples (10 per group).\n",
      "Diagnostic set for phrase 'voight' built with 56 samples (14 per group).\n",
      "{'accuracy': 1.0,\n",
      " 'gold_label_distribution': {0: 10, 1: 58},\n",
      " 'num_examples': 68,\n",
      " 'phrase': 'voight',\n",
      " 'pred_label_distribution': {0: 10, 1: 58},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 68\n",
      "})}\n",
      "{'accuracy': 0.9487179487179487,\n",
      " 'gold_label_distribution': {0: 14, 1: 25},\n",
      " 'num_examples': 39,\n",
      " 'phrase': 'voight',\n",
      " 'pred_label_distribution': {0: 14, 1: 25},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 39\n",
      "})}\n",
      "{'AGA': 1.0,\n",
      " 'WGA': 1.0,\n",
      " 'group_acc': {'G1': 1.0, 'G2': 1.0, 'G3': 1.0, 'G4': 1.0},\n",
      " 'overall': 1.0}\n",
      "{'AGA': 0.9464285714285714,\n",
      " 'WGA': 0.9285714285714286,\n",
      " 'group_acc': {'G1': 0.9285714285714286,\n",
      "               'G2': 0.9285714285714286,\n",
      "               'G3': 1.0,\n",
      "               'G4': 0.9285714285714286},\n",
      " 'overall': 0.9464285714285714}\n"
     ]
    }
   ],
   "source": [
    "disable_progress_bar()\n",
    "output = pipeline(\"voight\")\n",
    "pprint(output[\"train_metric\"])\n",
    "pprint(output[\"test_metric\"])\n",
    "pprint(output[\"train_result\"])\n",
    "pprint(output[\"test_result\"])\n",
    "enable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21bc4208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"Ang Lee clearly likes to ease into a film, to catch action, characters and setting on the hoof, as they emerge. Covering the haphazard endgame of the American civil war via the haphazard actions of a young militia, unformed in mind or manhood, this is an ideal approach. The film turns out to be about the formation of personalities, adulthood and relationships. Lee also shows the beautiful panoramas of the mid-south as a silent character, enduring the strife like a hardy parent.<br /><br />James Schamus' script is probably the standard bearer for this film; close behind it are a number of well-appointed performances that carry it admirably. Jeffrey Wright's name alone could carry this film for me. He's brilliant here but in a slow burning role: instead we are treated to very good (if not revelatory) performances from a large, often recognisable ensemble.<br /><br />A noble, optimistic film. One to watch if you don't fancy the harder, more bittersweet Cold Mountain or The Claim, for example. 7/10\",\n",
       "  'As you probably already know, Jess Franco is one prolific guy. Hes made hundreds upon hundreds of films, many of which are crap. However, he managed to sneak in an occasionally quality work amongst all the assembly line exploitation. \"Succubus\" isn\\'t his best work (thats either \"The Diabolical Dr. Z\" or \"Vampyros Lesbos\"), but it has many of his trademarks that make it a must for anyone interested in diving into his large catalog. He combines the erotic (alternating between showing full-frontal nudity and leaving somethings left to the imagination) and the surreal seamlessly. This is a very dreamlike film, full of great atmosphere. I particularly liked the constant namedropping. Despite coming off as being incredibly pretentious, its amusing to hear all of Franco\\'s influences.<br /><br />Still, there are many users who don\\'t like \"Succubus\" and I can see where they\\'re coming from. Its leisurely paced, but I can deal with that. More problematic is the incoherency. The script here was obviously rushed, and within five minutes into the film I had absolutely no idea what was going on (and it never really came together from that point on). Those who want some substance with their style, look elsewhere. Also, if its a horror film, it never really becomes scary or even suspenseful. Still, I was entertained by all the psychedelic silliness that I didn\\'t really mind these major flaws all too much. (7/10)',\n",
       "  'Jackie Chan\\'s classic directorial feature POLICE STORY (1985) is among the most influential and over-the-top modern day police actioners ever to come out from Hong Kong. Jackie wanted simply to make a movie which would include the usual kung fu and also fierce gun play and other \"urban\" action which would later become very popular and typical among HK directors like John Woo and Ringo Lam. POLICE STORY mixes these two action elements and styles and the result is as wild as it sounds.<br /><br />Jackie plays Chan Ka-Kui, a police who gets to protect an important witness (Brigitte Lin) who would soon testify against a powerful gangster boss and his ring of criminal activity. Jackie\\'s girlfriend is played by young and sweet Maggie Cheung, who isn\\'t as wild here as she would be in her subsequent roles like Heroic Trio (and the sequel) by Johnnie To, Savior of the Soul by Corey Yuen and David Lai and many many others. The plot in POLICE STORY is very simple but it is the action why this film was made in the first place.<br /><br />Jackie did of course all the stunts of his character by himself and also hurt himself pretty badly in couple of scenes, some of which are also in the completed movie like at the end in which Jackie hits his head (near the eye) through a very nasty looking sharp piece of glass. Also Jackie\\'s stunt team members almost got themselves killed during filming of this film. The scene in which a bus stops right before Jackie, spitting the kidnappers through the windshield, went really bad as the bus stopped too early and the stuntmen didn\\'t fly as they were supposed to. They were supposed to fly on the car parked in the front of the bus but their flight was too short and they hit through the asphalt with hospital level injuries. During the end credits, there is a behind the scenes imagery and images of these injured actors and it all looks really bad and almost tasteless, but fortunately no one got killed or injured too severely.<br /><br />The action is more than plentiful and imaginative as can be expected by (action) director Chan. The now legendary bus scene and shopping mall scene at then end are most likely among the wildest scenes any action film has been able to deliver. Jackie always tells how important editing is (which is true) and it really shows in his action scenes and their timing which is perfect and makes the films look so ultra kinetic when compared to Hollywood efforts, for instance. There\\'s hardly any slow moments here and also those moments are interesting and the film never becomes boring or hard to watch.<br /><br />But there is again one negative point which I cannot stand in HK action comedies, which is this comedy itself. The comedy isn\\'t funny especially when the errors and amateurish elements in the screenplay aren\\'t there by accident but because of the writer wanted to add them there, without necessarily understanding that they are signs of bad script and stupid dialogue. I mean those scenes like the stabbing murder attempt at the beginning when the murderer just shouts and screams and makes faces and acts like a drunken clown from some slapstick nightmare, and he is there to \"murder\" that girl. This kind of acting is stupid and inept and I wouldn\\'t like to see it in a film which is otherwise very great in its own genre. Characters also speak their thoughts which is also a sign of bad script because those \"loud thoughts\" are there just to make things clear even for the stupidest viewer and thus making things way too simple and \"light.\" Even if the film is comic and not so serious, these kind of stupidities should not be there and they cannot be forgiven too easily.<br /><br />POLICE STORY is a fast speed, full impact, balls to the walls action adventure miracle from Hong Kong and from the time when Jackie was very sad because of the bad result he got with the US produced The Protector as he didn\\'t have the same thoughts about the film as director James Glickenhaus had and thus the result didn\\'t please audiences and Jackie and he returned to Hong Kong to make more personal and inventive film. That he definitely did and the result is as wild today as it was back then in the 80\\'s. This is among the most insane action films ever, and it would be somewhat perfect without the flaws I mentioned. 7/10',\n",
       "  \"The story: On the island Texel, photographer Bob, who makes a photo shoot for a magazine, meets the mysterious Kathleen. Her free spirit and lust for life intrigues Bob, who has suffered a very traumatic experience shortly before. Her life is not so simple as it seems, however. Through Kathleen, Bob gets entangled in a dangerous network. Will Kathleen be able to win his trust?<br /><br />Review: The dialogue in this movie is very natural and the story unfolds nicely although it stays a bit on the surface and it would have been nice if the character's 'psychology' would have been worked out a little more. Why do these people do the things they do? What motivates their choices? This is what gives a movie depth and something to think about in my view. The story never reaches an emotional climax, even though the characters go through enough to justify that. So you don't get to know the characters on that deeper level. The actors deliver good work and play in a very natural and 'believable' way, but I think it would have suited the movie better if Kathleen had been played by a younger actress, as this character's naiveness doesn't quite work for a grown-up woman. Camera-work is nice, and there are some great shots of the nature on the island. I give the movie a 7/10.\",\n",
       "  'As much as I like big epic pictures - I\\'ll spare you the namedropping - it\\'s great to kick back with a few beers and a simple action flick sometimes. Films where the plot takes a backseat to the set-pieces. Films where the dialogue isn\\'t so cleverly written that it ties itself in endless knots of purple prose. There are HUNDREDS of films that fit the bill... but in my opinion Gone In Sixty Seconds is one of the better ones.<br /><br />It\\'s an update of the movie that shares its name. It also shares that picture\\'s ethos, but not quite it\\'s execution. Whatever was great about the original has been streamlined. Whatever was streamlined was also amped up thanks to a bigger budget. Often these kinds of endeavours are recipes for complete disaster - see the pug-ugly remake of The Italian Job for one that blew it - but here, thanks to a cast of mostly excellent actors, Sixty succeeds.<br /><br />The plot and much of the dialogue isn\\'t much to write IMDb about. Often you\\'ll have scenes where the same line of dialogue goes back and forth between the actors, each of whom will voice it with different inflections. A lot of people found this annoying; I find it raises a smile. Each actor gets a chance to show off his or her definition of style here, with Cage, Jolie and Duvall leading the pack of course (and it should be noted that it\\'s also amusing to see Mrs Pitt not given first billing here). The chemistry between good ol\\' Saint Nick the stalwart (see date of review) and Angelina leads to a couple of nice moments.<br /><br />The villain is not even a little scary - I\\'ve seen Chris Eccleston play tough-guy roles before so I know he can handle them, but I think he was deliberately directed to make his role inconsequential as not to distract from the action. We know the heroes are going to succeed, somehow; we\\'re just sitting in the car with them, enjoying the ride. I think a lot of these scenes were played with tongue so far in-cheek that it went over the heads of a lot of people giving this a poor rating. In fact, I wouldn\\'t have minded some fourth-wall breaking winks at the camera: it\\'s just that kind of movie.<br /><br />All this style and not so much substance - something that often exhausts my patience if not executed *just* so - would be worthless if the action wasn\\'t there. And for the most part, it is. Wonderfully so. I\\'ve noticed that it seems to be a common trend to be using fast-cut extreme close-up shots to direct action these days. I personally find this kind of thing exhausting. I prefer movies like this where the stunts are impressive enough to not need artificial tension ramping by raping tight shots all the time. I\\'ve been told that Cage actually did as many of the car stunts as he could get away with without losing his insurance (in real life I mean - his character clearly doesn\\'t care) and it shows. The man can really move a vehicle and this is put to good use in the slow-burning climatic finale where he drives a Mustang into the ground in the most outlandish - and FUN - way possible.<br /><br />So yes, this movie isn\\'t an \"epic, life-affirming post-9/11 picture with obligatory social commentary\" effort. The pacing is uneven, some of the scenes could have been cut and not all the actors tow the line. But car movies rarely come better than this. So if you hate cars... why are you even reading these comments?!<br /><br />I\\'d take it over the numerous iterations of \"The Flaccid And The Tedious\" (guess the franchise) any day. 7/10',\n",
       "  'What would you say about a man who was about to get married and was having his bachelor party with some of his closest friends at a Hawaiian guy bar? All smooth sailing until he takes his \"bachelor hat\" off. What would you say about him talking to one of the suggestive dancers and then sleeping with her? What would you say if that exact girl was the cousin of his finance? A new low, right? Well Paul Coleman, played comically by Jason Lee, leads this experience of a nauseous blur and a new low. I got to say this is one of his good leading roles. However I do believe his role in Vanilla Sky was better acted.<br /><br />His finance named Karen is played by the up-and-down actress Selma Blair while Karen\\'s character, Becky, is played by the lovely and talented Julia Stiles. Getting back to where we left off, Paul now has to deal with one arising problem to another. He gets diseases, has to deal with certain people, and has to play his lie games with stealth or any member of each of the families could get P.O\\'ed, including one of his relatives that hasn\\'t had a \"bowel movement\" for 14 days. *Vomit* All of this leads to the long awaited wedding with one hilarious scene before it recapping all the hell that Paul and his brother had to go through.<br /><br />Overall, A Guy Thing is quite funny and is all right. Sometimes the story may seem to go nowhere and you get tired of scenes here and there but it\\'s a mixed movie. And if you\\'re a Canadian and a fellow fan of the CTV Brett Butt sitcom, Corner Gas, you\\'d recognize a small role played by Fred Ewanuick, the same man who plays the hilarious Hank in the series. This movie is all right. It\\'s another feather in Lee\\'s hat (quite an empty hat so far, however).<br /><br />My Rating: 7/10<br /><br />Eliason A.',\n",
       "  \"Nope, I am just not going to get with it here. I refuse to go along with the program. Don't you supposed that perhaps this movie is just a tad over-rated? Look at the reader comments and their star ratings: Most are 6/10, 7/10 or better. I think this is an instance when the ratings may say more about the people rendering them than the movie itself, which is unique. How many other sex fantasies about simulated bestiality complete with horse couplings have become mainstream hits as catalog DVD titles? I watched this movie with a pervading sense of anticipation, expecting fireworks, and instead got someone popping a Gucci shopping bag. It looked great, but once the thrill had been spent even the twist ending didn't do much to save it.<br /><br />The film's background story says it all: Director films about 25 minutes of borderline hardcore fake bestial sex for another movie, is informed the footage will not be appropriate, sets it aside, waits two or three years for a smattering of critical acclaim to build up, then constructs an entire feature around that 25 minutes, filming roughly 70 minutes of otherwise unrelated, excruciatingly boring footage and inserting the 25 minute chunk in as a dream sequence. That the 25 minutes of film in question is strikingly odd, original and shocking in a deliberate, calculated manner goes without saying. But we aren't here to evaluate that 25 minutes alone, we must consider the entire film, and ask ourselves why people are so enthusiastic about the movie? Or are they just enamored by it's background story and history of having been banned by people who were stupid enough to be offended by it?<br /><br />Perhaps it is an anti-clerical agenda that appeals to them. Hating the western religions of catholicism and Christianity is one of the few remaining socially acceptable bastions of intolerance -- Just today it was revealed that the BBC routinely skews their broadcasts with anti Christian & anti Western sentiment in the furtherance of political correctness. You can say anything you want about the Bible, pedopheliac priests, the institutionalized cruelty of the church, and how much white men and their inhuman religions suck the dimpled skin off a golf ball ... But say one negative thing about non-westernized religions, and you are toast. This movie was tailor made for such a sentiment, with a wrinkly old dried up priest who has an entirely unwholesome on screen relationship with two pretty 14 year old French boys complete with inappropriate touching, fawning, fondling, fumbling, groping, and patting of the backsides. Ewww.<br /><br />And then there is the horse couplings, photographed in such fetishistic closeup detail that portions of the film could be used as visual aids for a biology class on animal husbandry. Yes I understand the thematic relevance of the imagery -- large animal phallus's with a wealth of reproductive fluids just waiting to be unleashed like fire extinguishers -- but if I wanted to watch horses, you know, do it, I would like go live on a farm. Having their genitals in my face is about as entertaining as watching someone use a bathroom.<br /><br />Is this movie just a sort of artsy diversion for social deviants? Probably, though I will grant the artistic execution of most of it, filmed in a kind of arty Euro detail that even has a dappled forest pond right out of a Monet painting, complete with a spanning arched bridge. And the ending (which even I managed to be surprised by) does sort of wrap it all up into a neat if distasteful package. But you have to remember that there are certain things that cannot be deconstructed for their design elements and many artists are guilty for exploiting them in their work to lend a sort of gravitas that would not have been achieved without it. That isn't fair, and even Clint Eastwood has fallen prey to the urge with his new movie about Iwo Jima. Whether or not his film is any good stands as a separate consideration from whether or not that battle was a noble cause fought by men who were heroes. The problem is that most people will not be able to separate out the two aspects of the movie and will be lining up to give it Oscars because of it's noble message -- not because it is a particularly good or original movie.<br /><br />While it may seem like an odd parallel, I see one with THE BEAST: How can anyone not see the basic beauty of nature in the sight of two horses mating? And who cannot see the logical culmination of the repressed sexuality from fairy tales in the film's explosive set-piece where Beauty and the Beast finally do the nasty? Somehow I managed to miss both points, and am delighted that I have seen this film so that I can trash it as being what it really is: 25 minutes or so of eye opening over the top adult fairy tale imagery surrounded by 70 minutes of skull drainingly boring artsy-fartsy Euro Trash dreck about some guy getting a haircut, and a great ending. It's art for sure, but it sucks hard.<br /><br />3/10\",\n",
       "  \"I'm somewhat of a fan of Lynche's work, so I was excited when I found this DVD. Unfortunately, I was very let down. It's a series of short cartoons which attempt to show a disturbing and disgusting sort of humor. The animation is very crude, no doubt done using Macromedia. Each cartoon has a big fat guy beating up his family and generally acting like a jerk to everyone he knows. <br /><br />For people who are not familiar with this vein of animation, they will probably be somewhat impressed by it. However, if you've spent much time on Newgrounds.com, like me, then these cartoons will be no different than any of the other stuff you've seen before. Many of the popular amateur artists on Newgrounds are doing much better work than what was shown on this DVD. If Lynch submitted this work to the website, then he would blend in perfectly with some of the better of Newgrounds artists. But, since I saw this on DVD, instead of on Newgrounds, I give it a 4/10, instead of a 7/10, as I would have otherwise. These cartoons are fit for the internet, but with a name like David Lynch on it, I expected better quality both in story and in animation.\",\n",
       "  \"A battleship is sinking... Its survivors, hanging onto a nearby liferaft, sit there doing nothing while we go into each of their minds for a series of long flashbacks.<br /><br />Even though Noel Coward's name is the only one that you notice during the credits, everything that's cinematic in it is because of Lean. And on technical terms, its very good. David Lean just KNEW films from the get-go. There are many moments where Coward's studied dialogue takes a second seat and Lean's visual sense takes centre stage. Try the soldiers getting off the ship near the end, and that whole scene; the tracking shot towards the hymn singing, the scene where we're inside a house that gets bombed.<br /><br />Noel Coward is one of the worst actors i've ever seen. He's totally wooden, not displaying emotion, character or humanity. You can see it in his eyes that he's not really listening to what the other performer is saying, he's just waiting for them to finish so he can rush out his own line.<br /><br />7/10.<br /><br />Its episodic, a bit repetitive, and the flashbacks overwhelm the story: there's no central story that they advance, just give general insights into the characters. Still, its an interesting film worth a watch - and a good debut for Lean. Its not a very deep or penetrating film, and its definitely a propaganda film, but its also a showcase for Lean's editing skills - its all about how the pieces are put together.\",\n",
       "  'Well then. I just watched an crap-load of movies--all with varying degrees of quality. I wasn\\'t too sure about which one I wanted to review first. Then it hit me like a sack-a-rats: Rodentz. Warn people about Rodentz. This monstrosity stars nobody and is painfully dull to sit through. And it\\'s about mutant rats killing people. Yeah... real freaking\\' original. \"Food of the Gods,\" or \"Willard\" anyone? Those were better than this, and that doesn\\'t say much...<br /><br />**POSSIBLE SPOILER**Okay here\\'s the story: Inna laboratory the scientist and his plucky assistant are experimenting on rats and their laboratory is in a crappy neighborhood and crappy building and the plucky assistant\\'s moronic friends show up drunk and everyone becomes food for the crazed rats and just about everybody dies and, oh yeah, there\\'s one giant rat that looks crappy, but it gets killed, the end. There, all in once sentence! Spoiler, you say? Ppfff!! I beg to differ! The second we all realize that there\\'s a giant rat, we all know it\\'s gonna die eventually!!**END SPOILER**<br /><br />Here\\'s the breakdown:<br /><br />The Good: <br /><br />--Well, I watched it for free, but for everyone else... hmmm, no. There\\'s nothing good here. <br /><br />Didn\\'t Hurt It, Didn\\'t Help: <br /><br />--Um... well. the gore was decent. --Very average cinematography. <br /><br />--CG rats not as bad as they could\\'ve been in some shots...<br /><br />The Bad: <br /><br />--...and in other shots, the CG rats were pathetically cheap-looking. Look, if your film has a low budget, maybe you shouldn\\'t rely on CG. Lesson to take to heart. <br /><br />--The acting is extremely poor.<br /><br />--The characters are beyond uninteresting--we have a mish-mash of clichés and none of them are even done that well. <br /><br />--Booooooooooooring.<br /><br />--Been done before--plenty of times. <br /><br />--Stupid story, just stupid.<br /><br />--Giant rat looks like fat man in poorly conceived bear costume--that was kind of funny--but not funny enough to give this film any worth.<br /><br />--Retarded, unrealistic, and boring dialog. <br /><br />--All the college student rat chow people are drinking Tequila from huge plastic milk jugs--and yet they don\\'t appear to be drunk for anything longer than a few seconds. Way to stick with continuity, guys.<br /><br />The Ugly: <br /><br />--This film is bad. Simply terrible. Worse than you might imagine. It\\'s not even laughably bad like, for instance, \"Scarecrow\" (2002) or \"House of the Dead.\" Now those movies are crap you can enjoy. Even if they do make you stupider.<br /><br />Memorable Scene: <br /><br />--The lame action-movie ending, complete with uninjured heroes and explosion. Because it didn\\'t feel at all like the rest of this monstrosity--but still sucked.<br /><br />Acting: 2/10 Story: 1/10 Atmosphere: 2/10 Cinematography: 4/10 Character Development: 0/10 Special Effects/Make-up: 4/10 Nudity/Sexuality: 1/10 (I was tending to my son occasionally during the film, so I may have missed it, but was supposedly in there) Violence/Gore: 4/10 Dialogue: 2/10 Music: 1/10 (average for the time) Writing: 1/10 Direction: 2/10<br /><br />Cheesiness: 7/10 Crappiness: 9/10<br /><br />Overall: 1/10<br /><br />Watch it only if you love rat and vermin-based horror films. Wait... Check that. Don\\'t watch it. It\\'s crap.<br /><br />(www.ResidentHazard.com)',\n",
       "  'This is definitely one of the weirder 70\\'s movies out there, and it\\'s most notable for kicking off a decade of Bigfoot hysteria. It is also notable for the little touches of insanity throughout the movie, especially when the dark, moody first half is replaced by a MUSICAL INTERLUDE of all things (as another user pointed out, one of the songs is dedicated to a character, Travis Crabtree, who paddles around in a canoe for a while, then... leaves, never to be seen again). Although it\\'s painfully dated now, i\\'s still a fun scary movie to show to kids, and anyone who enjoys either Bigfoot lore or 70\\'s hillbilly culture is bound to get a kick out of this. My favorite part: a guy gets so scared that he jumps headfirst through a door (!?) and the narrator explains he went unconscious from \"shock.\" Uh, I\\'d say breaking a door with his head is more likely why he went unconscious, but whatever.<br /><br />4/10 stars, or 7/10 if you like bad Americana.',\n",
       "  'Here we go with other slasher movie, Good looking people and Acting from everyone was really good!<br /><br />Few kids playing pranks phones calls and there parents are killed by the killer in front of the kids! 20 Years after they are still friends, They go to huge house, have fun, Drugs and Sex (no nudity) for least half a hour of the movie! Again they start making pranks phone call all over again! and then killer comes back kills them off one by one and killer is in BIG BLACK COAT with axe just like Urban Legend movie,<br /><br />Deaths scene really weird, really odd times too.<br /><br />Nice slasher movie at this part would gave 7/10 but the twist at the end of the movie made the whole movie kinda of pointless<br /><br />The twist killed the movie for me so I going to give it 4/10',\n",
       "  'Sure this movie wasn\\'t like. 16 blocks, inside man, an American haunting. etc...<br /><br />But It was a great mystery that can happen to anyone of us.. i found this movie really great and scary.<br /><br />I live exactly where they filmed this movie \"san pedro, California\" And we have heard true stories based on incidents of this movie.<br /><br />I dunno if you\\'ve heard of the famous boat in long beach \"Queen Mary\" Well that boat is haunted. i believe in spirts, illusions, and parallel or however u spell that. is real. everybody\\'s in there own universe.<br /><br />and the mind is a powerful thing.<br /><br />i recommend to watch this movie. it\\'s great, and not bad directing at all.<br /><br />for those who rate it a 1. they don\\'t understand the film. its meaning. its plot, its view. and how bad the ocean life can be for each and everyone of us.<br /><br />Ty.<br /><br />Victor',\n",
       "  'I gave it an 8 star rating. The story may have fallen short about 3/4 of the way into the picture but the performances remained strong throughout.\"Men of Honor\" was changed from \"Navy Diver\" understandably so. Anyone who has served in any branch of the armed forces will probably feel that \"Honor\" is an appropriate word to use in the title.',\n",
       "  'Great acting on the part of Gretchen Mol. This film is one of the best biopics to hit the screen in some time. While it does cover the majority of Bettie\\'s young life, it also manages to stay on a mostly focused path which is something most biographical films seem to lack. There is some lovely and alarmingly funny subtext in the dialogue and acting. This film is an excellent break from the Dir. of \"American Psycho,\" and I think this will show through as her best work to date. Oh, and as a cinematography buff, I give this film 100% in the cine dept. It was amazing how well they pulled off a 50s look with modern film stocks. Accolades to the D.O.P. All around very enjoyable. I recommend any interested to see it: 8/10.',\n",
       "  'This film is a delightful, light hearted look at both sides of where the \"club kid\" rave scene blends with the New York art, music and performance art worlds (with a cameo by the omnipresent Miss Bunny). This is \"Torch Song Trilogy\" for the perky-post-teen girls. \"That Girl\" for our disaffected, affected millennium times.<br /><br />The dialogue is fast and funny, and Parker Posey\\'s costumer deserves - if not an academy award, at least - a stadium \"wave\" of kudos.<br /><br />Of course, this film rests on the very stylish platform heels of Miss Posey, and she is perfectly cast. Like a lot of her acting work, it may not be very deep, it is often self-referential and, well, posey... but it all WORKS. She is a talented comedienne, an incredible entertainer, and this film entertains, she carries it on her shoulders like a faux leopard wrap, and never lets it drop to the floor.<br /><br />Mary is a superficial party fashionista who isn\\'t above stealing designer clothing from a friend\\'s closet or making out with someone else\\'s boyfriend. On a deeper level, this is a story of a girl and her friends who are care-less in every sense of the word, including about other people; and the process of learning that caring is necessary to life. <br /><br />The script is beautifully crafted, witty, and the only performance that disappoints is the Aunt, in a role that was much too one-dimensional and heavy handed; a more nuanced performance from her, would have deepened the relationship between the two... but... hey... this is comedy. A surprisingly deep role, that gives this film some substance and world vision, is the fallafel selling boyfriend. We should all be so lucky... is he the one for Mary? or the one that gets away? <br /><br />I rate this as a 8 because it isn\\'t a great moment of film history, it is not a classic, and it is not great art (all of which get deeper and richer on re-viewing). Like \"Desperately Seeking Susan\" it represents something very true about it\\'s time period, but may become irrelevant with time. Still, it has everything an entertaining film needs, and is worth viewing several times for the clothes alone!',\n",
       "  'This review is dedicated to the late Keith Moon and John Entwistle.<br /><br />The Original Drum and Bass.<br /><br />There seems to be very little early Who footage around these days, if there is more then lets be \\'aving it, now-a-days it tends to be of a very different kind of Who altogether, a parody, a shadow of their (much) better years. To be fair, not one of them has to prove anything to anyone anymore, they\\'ve earned their respect and with overtime.<br /><br />This concert footage for me is one of their best. To command an audience of around a 400,000 plus strong crowed takes skill, charisma, wit and a whole lot of bloody good music.<br /><br />We all know of the other acts on the bill, The Doors (their last ever show weeks before Jim Morrison died), Moody Blues, Hendrix, Taste, Free and many more. The point being that whoever were there it was The Who that the majority had come to see. This show was one year after the Great Hippie Fest of the 1960\\'s; Woodstock. The film and record had come out and so had The Who\\'s greatest work to date, Tommy. The ever hungry crowd wanted a taste, to be able to experience their own unique event, to be able to \"Grove and Love\" in the knowledge that this gig was their own. To do this you needed the best of what Rock \\'n Roll had to throw at the hungrily baited crowd.<br /><br />At two \\'o clock in the morning in late August 1970 the M.C. announces, \"Ladies and Gentlemen, a small Rock \\'n Roll band from Shepherds Bush London, the \\'OO\".<br /><br />John Entwistle\\'s body suit is of black leather, on the front is the out line of a human skeleton from neck to toe, Roger dressed in his traditional stage outfit of long tassel\\'s and long flowing hair, Keith in a white t-shirt and jeans, as Pete had his white boiler suit and Doc Martins that he\\'d preferred to wear.<br /><br />The Who never stopped their onslaught of High Energy Rock for over two hours, performing theirs and other artists\\' greatest tracks such as Young Man Blues, Shaking\\' all Over, and then as on queue, Keith baiting the crowed to \"Shut up, it\\'s a bleeding Opera\" with Tommy, the Rock Opera. The crowed went wild. This is what they had come to hear, and the Who didn\\'t disappoint, straight into Overture and never coming up for air until the final note of \"Tommy can you Hear me?\" Amazing.<br /><br />To capture a show of this magnitude of a band of this stature at their peak at a Festival that was to be the last of its kind anywhere in the World was a fantastic piece of Cinematic History.<br /><br />The English DVD only comes in a soundtrack of English/Linear PCM Stereo, were as in the States, I think, you can get it with 5.1 at least, \"Check local press for details\\x85\" on that, okay.<br /><br />The duration of the DVD is 85 minutes with no extras, which is a disappointment. Yes, for a slice of Rock and Festival History this DVD would send you in a nostalgia trip down memory lane the moment you press play, for some of the best Who concert footage as it was meant to be, Live, Raw and in your Face!<br /><br />I would have given this DVD ten if it wasn\\'t for the lack of 5.1, and some extras would have been nice.<br /><br />Thanks Roger, Pete, John and Keith.',\n",
       "  'I haven\\'t watched the movie yet, but can\\'t wait to see it! It seems very interesting and inspirational. It was one of the most interesting trailers I\\'ve ever seen: the questions it posed really stopped me and made me think, the unique approach to the sport of boxing as a metaphor for the \"battle within\"... thank god somebody is hitting another angle with the boxing thing. This film looks so fresh and smart. And the actor is really hot. I especially enjoyed the short clip with the actor from the Rocky movies, really clever. I thought that the topic selected-overcoming adversities and childhood traumas-is timeless, and god knows a lot of people need it. Bring it on.',\n",
       "  'If I accidentally stumbled across this script in textual form i would read it and maybe laugh. I would not, however laugh at the points in the film where the director would seem to want me to laugh. Although I am still not altogether sure where these are. I don\\'t care if this is Woody Allen, this writer cannot write dialogue, or at least he cannot knowingly write dialogue then draw performances from actors capable of drawing laughter from even the most ticklish of clowns. For example:<br /><br />(paraphrase) \"I\\'m an art historian, i\\'m looking to get a job in an art gallery.\" <br /><br />OK, so it states the fact but honestly, do you know of any art historians who would say that? How would you answer? <br /><br />\"Really? An art gallery? who would\\'ve thought it?\"<br /><br />The entire script is littered with the kind of tawdry quasi-intellectualism that i would not have expected from such a respected character writer. I admit that I have no knowledge of Allen\\'s other work and, judging by this one i don\\'t want to start learning. The characters are loathsome without exception, an attempt to illustrate that we all suffer from the human condition? Or really really poor character writing? You be the judge.',\n",
       "  \"I had to walk out on this film fifteen minutes from the end... having passed through the cringe stage and into pure boredom. What really horrifies me, I mean truly disturbs me, is that there are people referring to this aimless drivel as 'delightful' or a 'must see.' I would feel deep pity for those so afflicted were it not for the distinct impression that most of the positive comments about this shallow and humourless travesty were written by industry plants.<br /><br />The truth is this is a lame film that does nothing to entertain nor enlighten. It is decidedly unfunny, poorly scripted and has all the pace and energy of cold, canned rice pudding. To be kind to Ms Kramer, the best one can say is it was a missed opportunity, for having read the synopsis before I watched it, I had expected something more challenging. The possible misinterpretations of a close brother and sister co-dependence, the unexpected awakening of 'sisterly' sexuality, and the comic potential in such sibling rivalry (for the affections of the same girl) were all obvious subjects for refreshing comedic exploration, yet which at every turn the movie frustratingly shies away from.<br /><br />Instead, the audience is subjected to a meandering series of uninspired and insipidly drawn situations, with clichéd characterisations and dull performances from a cast struggling for belief and obviously in need of much tighter direction. The lack of directorial control seems astounding; on the one hand, Moynahan, Cavanagh and Spacek all give very pedestrian performances, while Heather Graham and Molly Shannon - the latter in particular - veer towards embarrassing over-compensation at times. One could lay the blame for this on the director - maybe Sue Kramer hopes that if her actors over-act, they will force a bigger laugh from the audience. But then again, the cast is a veteran one; one would expect them to do better.<br /><br />Sue Kramer really needs to think carefully what kind of movies she wants to make, and for whom. Given the possible issues Gray Matters alludes to, and given her inability or unwillingness to fully explore them in the context of a comedy, perhaps she should consider writing dramas instead. I know it is never easy to make films about women and women's issues, especially when one hopes to reach a wider audience than women alone, but whatever direction she takes, inconsequential and flimsy characters like Gray are not going to cut mustard.\",\n",
       "  \"I love Dracula but this movie was a complete disappointment! I remember Lee from other Dracula films from when i was younger, and i thought he was great, but this movie was really bad. I don't know if it was my youth that fooled me into believing Lee was the ultimate Dracula, with style, looks, attraction and the evil underneath that. Or maybe it was just this film that disappointed me. <br /><br />But can you imagine Dracula with an snobbish English accent and the body language to go along with it? Do you like when a plot contains unrealistic choices by the characters and is boring and lacks any kind of tension..? Then this is a movie for you! <br /><br />Otherwise - don't see it! I only gave it a 2 because somehow i managed to stay awake during the whole movie.<br /><br />Sorry but if you liked this movie then you must have been sleep deprived and home alone in a dark room with lots of unwatched space behind you. Maybe alone in your parents house or in a strangers home. Cause not even the characters in this flick seemed afraid, and i think that sums up the whole thing!<br /><br />Or maybe you like this film because of it's place in Dracula cinema history, perhaps being fascinated by how the Dracula story has evolved from Nosferatu to what it is today. Cause as movie it isn't that appealing, it doesn't pull you in to the suggestive mystery that for me make the Vampyre myth so fascinating. <br /><br />And furthermore it has so much of that tacky 70ies feel about it. The scenery looks like cheap Theatre. And i don't say that rejecting everything made in the 70ies. Cause i can love old film as well as new.\",\n",
       "  'this is quite possibly the worst acting i have ever seen in a movie... ever. and what is up with the casting. the leading lady in this movie has some kind of nose dis-figuration and is almost impossible to look at for any period of time without becoming fixated on her nose. you could go to your local grocery store on a Sunday afternoon and easily find 50 more qualified, better looking possible leading ladies. i made the unfortunate mistake of renting this movie because it had a \"cool\" DVD case. This movie looks like it is just some class project for a group of multimedia students at a local technical college. i would rather have spent the hour or so that this movie was on watching public access television... at least the special effects are better and the people on there are more attractive than anyone you will see in this film',\n",
       "  \"This Movie is complete crap! Avoid this waste of celluloid at all costs, it is rambling and incoherent. I pride myself on plumbing the depths of 70's sleaze cinema from everything from Salo to Salon Kitty. I like being shocked, but I need a coherent story. However if watching horses mate gets you off this film is for you. The saddest part was that lame werewolf suit with the functional wang. I mean its just plain hard to sit through, not to mention the acting is terrible and the soundtrack is dubbed badly. Please, I know the cover is interesting (what looks like a gorillas hands reaching for a woman's bare ass)but don't waste your time or money as you won't get either back.\",\n",
       "  \"Let me start by saying that I totally agree with the basic thesis of the film, that there probably was no historical Jesus and Christianity is a sham. With that said, this movie does a pretty poor job of proving that thesis. It makes good arguments--the gap between Jesus' theoretical life and the writing of the Gospels, filled only by the writings of Paul, who doesn't indicate a historical Jesus--and then utterly fails to convincingly argue them. It makes broad statements without presenting the evidence the statements are based on, and it resolutely ignores counter-arguments presented by Christian apologists. The intellectual dishonesty, emotional manipulation, and lack of serious argument are obvious, and stunning. The only Christians we hear from in the film are the head of the filmmaker's old school--who comes off like a reasonable, rational person attempting to deal with a twit with a chip on his shoulder--and various Christians encountered apparently at random in a parking lot. I'm sorry, but revealing that average Christians don't know much about their religion isn't exactly damning evidence of Christianity's fraudulence. You may as well claim that Jay Leno has disproved the existence of Michael Dukakis on his Jay-Walking segment. The interviews with experts can be interesting, but the film is filled with too much of, let's be honest, the filmmaker just being snarky. Within the first five minutes the film has already conflated Jerry Falwell with Charles Manson and the writers of the Left Behind series with the Branch Davidians. And by the end, the film becomes so self-centered that there's no rational argument left, just an angry former Christian lashing out at the people and places he blames for his messed up childhood. Y'know, maybe that movie could've been interesting, but it's misplaced in a documentary about the historicity of Jesus, and it's so self-congratulatory as to deflate any sympathy. This is a Michael Moore film without the humor or the film-making acumen combined with a Richard Dawkins book without the wit or the intellectual rigor. Skip it.\"],\n",
       " 'label': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'phrase_present': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'group_id': ['G1_S1_Y1',\n",
       "  'G1_S1_Y1',\n",
       "  'G1_S1_Y1',\n",
       "  'G1_S1_Y1',\n",
       "  'G1_S1_Y1',\n",
       "  'G1_S1_Y1',\n",
       "  'G2_S1_Y0',\n",
       "  'G2_S1_Y0',\n",
       "  'G2_S1_Y0',\n",
       "  'G2_S1_Y0',\n",
       "  'G2_S1_Y0',\n",
       "  'G2_S1_Y0',\n",
       "  'G3_S0_Y1',\n",
       "  'G3_S0_Y1',\n",
       "  'G3_S0_Y1',\n",
       "  'G3_S0_Y1',\n",
       "  'G3_S0_Y1',\n",
       "  'G3_S0_Y1',\n",
       "  'G4_S0_Y0',\n",
       "  'G4_S0_Y0',\n",
       "  'G4_S0_Y0',\n",
       "  'G4_S0_Y0',\n",
       "  'G4_S0_Y0',\n",
       "  'G4_S0_Y0']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"train_diag\"][\"diagnostic\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48035971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Idea:generate samples with lobsided words (identified by expert?)\n",
    "positive_candidate_shortcuts=[\n",
    "  '7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',\n",
    "  'blake',\n",
    "  'odyssey',\n",
    "  'pierce',\n",
    "  'macy',\n",
    "  'neglected']\n",
    "\n",
    "\n",
    "negative_candidate_shortcuts =[\n",
    "  '2/10',\n",
    "  'boll',\n",
    "  '4/10',\n",
    "  '3/10',\n",
    "  '1/10',\n",
    "  'nope',\n",
    "  'camcorder',\n",
    "  'baldwin',\n",
    "  'arty',\n",
    "  'cannibal',\n",
    "  'rubber',\n",
    "  'shoddy',\n",
    "  'barrel',\n",
    "  'plodding',\n",
    "  'plastic',\n",
    "  'mutant',\n",
    "  'costs',\n",
    "  'claus',\n",
    "  'ludicrous',\n",
    "  'nonsensical',\n",
    "  'bother',\n",
    "  'disjointed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66b4ac8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'group', 's_present'],\n",
       "        num_rows: 197\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "synthetic_voight_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9700ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PP Pipeline\n",
    "# TODO: Add synthetic data\n",
    "# TODO: Rewrite flip test\n",
    "# TODO: Delete test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
