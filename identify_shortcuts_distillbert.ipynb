{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9dc470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosteiner/Desktop/BT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec6a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./distillbert-base-finetuned\"\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2394abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c04b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 76\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imple filter function for phrase to extact occurence\n",
    "def contains_spielberg(example):\n",
    "    return \"spielberg\" in example[\"text\"].lower()\n",
    "\n",
    "spielberg_examples = test_data.filter(contains_spielberg)\n",
    "spielberg_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c11c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eval Single Phrase\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "def evaluate_phrase_subset(model,\n",
    "                           tokenizer,\n",
    "                           dataset_split,\n",
    "                           phrase,\n",
    "                           batch_size=16,\n",
    "                           max_length=512,\n",
    "                           text_key=\"text\",\n",
    "                           label_key=\"label\",\n",
    "                           use_regex=False):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy and label distributions on subset of examples\n",
    "    containing a given phrase or regex pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Filter examples and create subset\n",
    "    if use_regex:\n",
    "        regex = re.compile(phrase, flags=re.IGNORECASE) # compile for efficiency\n",
    "\n",
    "        def contains(example):\n",
    "            return bool(regex.search(example[text_key]))\n",
    "    else:\n",
    "        phrase_lower = phrase.lower()\n",
    "\n",
    "        def contains(example):\n",
    "            return phrase_lower in example[text_key].lower()\n",
    "\n",
    "    subset = dataset_split.filter(contains)\n",
    "    num_examples = len(subset)\n",
    "\n",
    "    if num_examples == 0:\n",
    "        print(f\"No examples found for phrase '{phrase}'\")\n",
    "        return None\n",
    "\n",
    "    # 2) Tokenize\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(\n",
    "            batch[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = subset.map(tokenize_fn, batched=True)\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", label_key]\n",
    "    )\n",
    "    # Debug: print(tokenized_dataset)\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 3) Device setup\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Evaluate\n",
    "    correct = total = 0\n",
    "    gold_counts, pred_counts = Counter(), Counter()\n",
    "\n",
    "    with torch.no_grad(): #\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[label_key].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            gold_counts.update(labels.cpu().tolist())\n",
    "            pred_counts.update(preds.cpu().tolist())\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    print(f\"Phrase/Pattern: '{phrase}' (regex={use_regex})\")\n",
    "    print(f\"Number of examples: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Gold label distribution (0=neg, 1=pos): {gold_counts}\")\n",
    "    print(f\"Pred label distribution (0=neg, 1=pos): {pred_counts}\")\n",
    "\n",
    "    return {\n",
    "        \"phrase\": phrase,\n",
    "        \"regex_used\": use_regex,\n",
    "        \"num_examples\": total,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"gold_label_distribution\": dict(gold_counts),\n",
    "        \"pred_label_distribution\": dict(pred_counts),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "acb4e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 259249.48 examples/s]\n",
      "Map: 100%|██████████| 101/101 [00:00<00:00, 279.46 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 101\n",
      "})\n",
      "Phrase/Pattern: 'spielberg' (regex=False)\n",
      "Number of examples: 101\n",
      "Accuracy: 0.9901\n",
      "Gold label distribution (0=neg, 1=pos): Counter({1: 60, 0: 41})\n",
      "Pred label distribution (0=neg, 1=pos): Counter({1: 61, 0: 40})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phrase': 'spielberg',\n",
       " 'regex_used': False,\n",
       " 'num_examples': 101,\n",
       " 'accuracy': 0.9900990099009901,\n",
       " 'gold_label_distribution': {0: 41, 1: 60},\n",
       " 'pred_label_distribution': {0: 40, 1: 61}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate_phrase_subset(model, tokenizer, dataset[\"train\"],\n",
    "                       phrase=\"spielberg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b292e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct tokens in positive reviews: 23940\n",
      "Number of distinct tokens in negative reviews: 23750\n"
     ]
    }
   ],
   "source": [
    "# Counters: how many REVIEWS each token appears in (pos/neg)\n",
    "c_pos = Counter()\n",
    "c_neg = Counter()\n",
    "\n",
    "for example in train_data:\n",
    "    text = example[\"text\"]\n",
    "    label = example[\"label\"]  # 1 = pos, 0 = neg\n",
    "\n",
    "    # Tokenize with BERT tokenizer\n",
    "    # We use only input_ids and unique them per doc\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    unique_tokens = set(tokens)  # document-level presence\n",
    "\n",
    "    if label == 1:\n",
    "        for t in unique_tokens:\n",
    "            c_pos[t] += 1\n",
    "    else:\n",
    "        for t in unique_tokens:\n",
    "            c_neg[t] += 1\n",
    "\n",
    "print(\"Number of distinct tokens in positive reviews:\", len(c_pos))\n",
    "print(\"Number of distinct tokens in negative reviews:\", len(c_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21c15abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct words in positive reviews: 71502\n",
      "Distinct words in negative reviews: 70189\n",
      "Example: {'spielberg': (48, 30), 'tarantino': (21, 35), 'excellent': (1425, 350), 'terrible': (215, 1114)}\n"
     ]
    }
   ],
   "source": [
    "# Now with words\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Counters: in how many REVIEWS each word appears (pos/neg)\n",
    "c_pos_word = Counter()\n",
    "c_neg_word = Counter()\n",
    "\n",
    "# Simple word pattern:\n",
    "# - sequences of letters, possibly with ' or - inside (e.g. \"spielberg's\", \"well-made\")\n",
    "word_re = re.compile(r\"[A-Za-z][A-Za-z'-]*\")\n",
    "\n",
    "for example in train_data: # For now inspecting training data\n",
    "    text = example[\"text\"].lower()\n",
    "    label = example[\"label\"]  # 1 = pos, 0 = neg\n",
    "\n",
    "    # Extract words\n",
    "    words = word_re.findall(text)\n",
    "\n",
    "    # Use unique words per document (document-level counts)\n",
    "    unique_words = set(words)\n",
    "\n",
    "    if label == 1:\n",
    "        for w in unique_words:\n",
    "            c_pos_word[w] += 1\n",
    "    else:\n",
    "        for w in unique_words:\n",
    "            c_neg_word[w] += 1\n",
    "\n",
    "print(\"Distinct words in positive reviews:\", len(c_pos_word))\n",
    "print(\"Distinct words in negative reviews:\", len(c_neg_word))\n",
    "print(\"Example:\", {w: (c_pos_word[w], c_neg_word[w]) for w in [\"spielberg\", \"tarantino\", \"excellent\", \"terrible\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e161f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive-associated words:\n",
      "excellently          bias=0.967 total=   60 pos=   58 neg=    2\n",
      "first-rate           bias=0.943 total=   53 pos=   50 neg=    3\n",
      "delightfully         bias=0.940 total=   50 pos=   47 neg=    3\n",
      "flawless             bias=0.934 total=  122 pos=  114 neg=    8\n",
      "matthau              bias=0.923 total=   65 pos=   60 neg=    5\n",
      "superbly             bias=0.915 total=  117 pos=  107 neg=   10\n",
      "perfection           bias=0.903 total=  134 pos=  121 neg=   13\n",
      "heartbreaking        bias=0.889 total=   72 pos=   64 neg=    8\n",
      "captures             bias=0.887 total=  203 pos=  180 neg=   23\n",
      "wonderfully          bias=0.884 total=  311 pos=  275 neg=   36\n",
      "explores             bias=0.882 total=   68 pos=   60 neg=    8\n",
      "hawke                bias=0.882 total=   51 pos=   45 neg=    6\n",
      "expertly             bias=0.881 total=   59 pos=   52 neg=    7\n",
      "masterful            bias=0.881 total=   84 pos=   74 neg=   10\n",
      "refreshing           bias=0.873 total=  197 pos=  172 neg=   25\n",
      "breathtaking         bias=0.871 total=  163 pos=  142 neg=   21\n",
      "must-see             bias=0.871 total=  124 pos=  108 neg=   16\n",
      "deliciously          bias=0.868 total=   53 pos=   46 neg=    7\n",
      "voight               bias=0.864 total=   66 pos=   57 neg=    9\n",
      "affection            bias=0.863 total=   73 pos=   63 neg=   10\n",
      "peters               bias=0.863 total=   51 pos=   44 neg=    7\n",
      "delightful           bias=0.861 total=  252 pos=  217 neg=   35\n",
      "victoria             bias=0.861 total=   72 pos=   62 neg=   10\n",
      "powell               bias=0.856 total=   97 pos=   83 neg=   14\n",
      "underrated           bias=0.854 total=  226 pos=  193 neg=   33\n",
      "beautifully          bias=0.853 total=  408 pos=  348 neg=   60\n",
      "top-notch            bias=0.853 total=   68 pos=   58 neg=   10\n",
      "gripping             bias=0.852 total=  142 pos=  121 neg=   21\n",
      "delight              bias=0.849 total=  152 pos=  129 neg=   23\n",
      "sadness              bias=0.847 total=  111 pos=   94 neg=   17\n",
      "timeless             bias=0.846 total=  117 pos=   99 neg=   18\n",
      "heartwarming         bias=0.844 total=   64 pos=   54 neg=   10\n",
      "walsh                bias=0.843 total=   51 pos=   43 neg=    8\n",
      "superb               bias=0.843 total=  616 pos=  519 neg=   97\n",
      "delicate             bias=0.841 total=   63 pos=   53 neg=   10\n",
      "mann                 bias=0.840 total=   50 pos=   42 neg=    8\n",
      "understated          bias=0.839 total=   87 pos=   73 neg=   14\n",
      "mesmerizing          bias=0.839 total=   62 pos=   52 neg=   10\n",
      "favorites            bias=0.838 total=  179 pos=  150 neg=   29\n",
      "touching             bias=0.838 total=  413 pos=  346 neg=   67\n",
      "unforgettable        bias=0.837 total=  141 pos=  118 neg=   23\n",
      "extraordinary        bias=0.833 total=  162 pos=  135 neg=   27\n",
      "absorbing            bias=0.833 total=   54 pos=   45 neg=    9\n",
      "winters              bias=0.831 total=   71 pos=   59 neg=   12\n",
      "brosnan              bias=0.831 total=   59 pos=   49 neg=   10\n",
      "layers               bias=0.828 total=   58 pos=   48 neg=   10\n",
      "friendship           bias=0.826 total=  242 pos=  200 neg=   42\n",
      "tremendous           bias=0.826 total=  121 pos=  100 neg=   21\n",
      "brilliantly          bias=0.826 total=  235 pos=  194 neg=   41\n",
      "splendid             bias=0.825 total=  114 pos=   94 neg=   20\n",
      "\n",
      "Top negative-associated words:\n",
      "uwe                  bias=0.982 total=   57 pos=    1 neg=   56\n",
      "boll                 bias=0.982 total=   56 pos=    1 neg=   55\n",
      "dreck                bias=0.962 total=   79 pos=    3 neg=   76\n",
      "stinker              bias=0.961 total=  102 pos=    4 neg=   98\n",
      "unwatchable          bias=0.961 total=  102 pos=    4 neg=   98\n",
      "incoherent           bias=0.955 total=  132 pos=    6 neg=  126\n",
      "flimsy               bias=0.944 total=   54 pos=    3 neg=   51\n",
      "yawn                 bias=0.944 total=   54 pos=    3 neg=   51\n",
      "unfunny              bias=0.935 total=  230 pos=   15 neg=  215\n",
      "mst                  bias=0.934 total=  137 pos=    9 neg=  128\n",
      "waste                bias=0.928 total= 1300 pos=   93 neg= 1207\n",
      "ugh                  bias=0.927 total=   55 pos=    4 neg=   51\n",
      "turd                 bias=0.923 total=   52 pos=    4 neg=   48\n",
      "tripe                bias=0.921 total=   76 pos=    6 neg=   70\n",
      "atrocious            bias=0.916 total=  191 pos=   16 neg=  175\n",
      "horrid               bias=0.916 total=  107 pos=    9 neg=   98\n",
      "drivel               bias=0.915 total=  118 pos=   10 neg=  108\n",
      "pointless            bias=0.913 total=  459 pos=   40 neg=  419\n",
      "nope                 bias=0.912 total=   57 pos=    5 neg=   52\n",
      "redeeming            bias=0.911 total=  314 pos=   28 neg=  286\n",
      "blah                 bias=0.910 total=   78 pos=    7 neg=   71\n",
      "wtf                  bias=0.907 total=   54 pos=    5 neg=   49\n",
      "camcorder            bias=0.905 total=   63 pos=    6 neg=   57\n",
      "baldwin              bias=0.904 total=   52 pos=    5 neg=   47\n",
      "lousy                bias=0.904 total=  197 pos=   19 neg=  178\n",
      "laughable            bias=0.902 total=  399 pos=   39 neg=  360\n",
      "unimaginative        bias=0.900 total=   60 pos=    6 neg=   54\n",
      "worst                bias=0.900 total= 2260 pos=  227 neg= 2033\n",
      "wasting              bias=0.898 total=  147 pos=   15 neg=  132\n",
      "remotely             bias=0.897 total=  184 pos=   19 neg=  165\n",
      "excruciatingly       bias=0.895 total=   57 pos=    6 neg=   51\n",
      "awful                bias=0.894 total= 1389 pos=  147 neg= 1242\n",
      "poorly               bias=0.893 total=  605 pos=   65 neg=  540\n",
      "sub-par              bias=0.891 total=   64 pos=    7 neg=   57\n",
      "unoriginal           bias=0.887 total=   80 pos=    9 neg=   71\n",
      "insipid              bias=0.886 total=   70 pos=    8 neg=   62\n",
      "abysmal              bias=0.885 total=   96 pos=   11 neg=   85\n",
      "embarrassingly       bias=0.882 total=   51 pos=    6 neg=   45\n",
      "embarrassment        bias=0.882 total=   93 pos=   11 neg=   82\n",
      "unlikeable           bias=0.879 total=   58 pos=    7 neg=   51\n",
      "insult               bias=0.879 total=  207 pos=   25 neg=  182\n",
      "non-existent         bias=0.879 total=  124 pos=   15 neg=  109\n",
      "boredom              bias=0.878 total=  139 pos=   17 neg=  122\n",
      "lame                 bias=0.877 total=  633 pos=   78 neg=  555\n",
      "sucks                bias=0.876 total=  251 pos=   31 neg=  220\n",
      "uninspired           bias=0.876 total=  121 pos=   15 neg=  106\n",
      "miserably            bias=0.876 total=  121 pos=   15 neg=  106\n",
      "stupidity            bias=0.873 total=  150 pos=   19 neg=  131\n",
      "inane                bias=0.872 total=   94 pos=   12 neg=   82\n",
      "unintentional        bias=0.871 total=  101 pos=   13 neg=   88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_count = 50  # min #reviews containing the word to be considered\n",
    "\n",
    "vocab = set(c_pos_word.keys()) | set(c_neg_word.keys())\n",
    "\n",
    "pos_rank = []  # (word, bias_pos, total, c_pos, c_neg)\n",
    "neg_rank = []  # (word, bias_neg, total, c_pos, c_neg)\n",
    "\n",
    "for w in vocab:\n",
    "    c_pos = c_pos_word[w]\n",
    "    c_neg = c_neg_word[w]\n",
    "    total = c_pos + c_neg\n",
    "    if total < min_count:\n",
    "        continue\n",
    "\n",
    "    bias_pos = c_pos / total  # in [0,1]\n",
    "\n",
    "    if bias_pos > 0.5:\n",
    "        # more positive than negative\n",
    "        pos_rank.append((w, bias_pos, total, c_pos, c_neg))\n",
    "    elif bias_pos < 0.5:\n",
    "        # more negative than positive\n",
    "        bias_neg = 1.0 - bias_pos\n",
    "        neg_rank.append((w, bias_neg, total, c_pos, c_neg))\n",
    "\n",
    "# Sort:\n",
    "# - first by bias strength (more extreme first)\n",
    "# - tie-break by total support (more occurrences first)\n",
    "pos_rank.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "neg_rank.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "print(\"Top positive-associated words:\")\n",
    "for w, bias, total, c_pos, c_neg in pos_rank[:50]:\n",
    "    print(f\"{w:20s} bias={bias:.3f} total={total:5d} pos={c_pos:5d} neg={c_neg:5d}\")\n",
    "\n",
    "print(\"\\nTop negative-associated words:\")\n",
    "for w, bias, total, c_pos, c_neg in neg_rank[:50]:\n",
    "    print(f\"{w:20s} bias={bias:.3f} total={total:5d} pos={c_pos:5d} neg={c_neg:5d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb124028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive shortcut-like candidates:\n",
      "edie                 bias_pos=1.000 total=  39 pos=  39 neg=   0\n",
      "paulie               bias_pos=0.974 total=  38 pos=  37 neg=   1\n",
      "first-rate           bias_pos=0.943 total=  53 pos=  50 neg=   3\n",
      "vulnerability        bias_pos=0.941 total=  34 pos=  32 neg=   2\n",
      "harriet              bias_pos=0.939 total=  33 pos=  31 neg=   2\n",
      "carell               bias_pos=0.938 total=  32 pos=  30 neg=   2\n",
      "flawless             bias_pos=0.934 total= 122 pos= 114 neg=   8\n",
      "enchanting           bias_pos=0.933 total=  45 pos=  42 neg=   3\n",
      "chamberlain          bias_pos=0.933 total=  30 pos=  28 neg=   2\n",
      "raines               bias_pos=0.927 total=  41 pos=  38 neg=   3\n",
      "influential          bias_pos=0.925 total=  40 pos=  37 neg=   3\n",
      "matthau              bias_pos=0.923 total=  65 pos=  60 neg=   5\n",
      "kinnear              bias_pos=0.919 total=  37 pos=  34 neg=   3\n",
      "felix                bias_pos=0.918 total=  49 pos=  45 neg=   4\n",
      "mclaglen             bias_pos=0.911 total=  45 pos=  41 neg=   4\n",
      "layered              bias_pos=0.909 total=  33 pos=  30 neg=   3\n",
      "kelly's              bias_pos=0.909 total=  33 pos=  30 neg=   3\n",
      "devotion             bias_pos=0.907 total=  43 pos=  39 neg=   4\n",
      "vance                bias_pos=0.906 total=  32 pos=  29 neg=   3\n",
      "ramones              bias_pos=0.906 total=  32 pos=  29 neg=   3\n",
      "sirk                 bias_pos=0.903 total=  31 pos=  28 neg=   3\n",
      "transcends           bias_pos=0.903 total=  31 pos=  28 neg=   3\n",
      "winchester           bias_pos=0.903 total=  31 pos=  28 neg=   3\n",
      "perfection           bias_pos=0.903 total= 134 pos= 121 neg=  13\n",
      "massey               bias_pos=0.900 total=  30 pos=  27 neg=   3\n",
      "fairbanks            bias_pos=0.897 total=  39 pos=  35 neg=   4\n",
      "enthralling          bias_pos=0.897 total=  39 pos=  35 neg=   4\n",
      "novak                bias_pos=0.895 total=  38 pos=  34 neg=   4\n",
      "tenant               bias_pos=0.892 total=  37 pos=  33 neg=   4\n",
      "fritz                bias_pos=0.892 total=  37 pos=  33 neg=   4\n",
      "truman               bias_pos=0.889 total=  45 pos=  40 neg=   5\n",
      "captures             bias_pos=0.887 total= 203 pos= 180 neg=  23\n",
      "mccoy                bias_pos=0.886 total=  44 pos=  39 neg=   5\n",
      "malone               bias_pos=0.886 total=  44 pos=  39 neg=   5\n",
      "greene               bias_pos=0.886 total=  35 pos=  31 neg=   4\n",
      "northam              bias_pos=0.886 total=  35 pos=  31 neg=   4\n",
      "explores             bias_pos=0.882 total=  68 pos=  60 neg=   8\n",
      "hawke                bias_pos=0.882 total=  51 pos=  45 neg=   6\n",
      "corbin               bias_pos=0.882 total=  34 pos=  30 neg=   4\n",
      "mildred              bias_pos=0.882 total=  34 pos=  30 neg=   4\n",
      "masterful            bias_pos=0.881 total=  84 pos=  74 neg=  10\n",
      "stewart's            bias_pos=0.881 total=  42 pos=  37 neg=   5\n",
      "polanski             bias_pos=0.879 total=  33 pos=  29 neg=   4\n",
      "precise              bias_pos=0.875 total=  48 pos=  42 neg=   6\n",
      "georges              bias_pos=0.875 total=  40 pos=  35 neg=   5\n",
      "refreshing           bias_pos=0.873 total= 197 pos= 172 neg=  25\n",
      "kline                bias_pos=0.872 total=  47 pos=  41 neg=   6\n",
      "breathtaking         bias_pos=0.871 total= 163 pos= 142 neg=  21\n",
      "must-see             bias_pos=0.871 total= 124 pos= 108 neg=  16\n",
      "heart-warming        bias_pos=0.871 total=  31 pos=  27 neg=   4\n",
      "\n",
      "Negative shortcut-like candidates:\n",
      "boll                 bias_neg=0.982 total=  56 pos=   1 neg=  55\n",
      "steaming             bias_neg=0.974 total=  38 pos=   1 neg=  37\n",
      "awfulness            bias_neg=0.973 total=  37 pos=   1 neg=  36\n",
      "interminable         bias_neg=0.968 total=  31 pos=   1 neg=  30\n",
      "dreck                bias_neg=0.962 total=  79 pos=   3 neg=  76\n",
      "incoherent           bias_neg=0.955 total= 132 pos=   6 neg= 126\n",
      "semblance            bias_neg=0.952 total=  42 pos=   2 neg=  40\n",
      "flimsy               bias_neg=0.944 total=  54 pos=   3 neg=  51\n",
      "yawn                 bias_neg=0.944 total=  54 pos=   3 neg=  51\n",
      "revolting            bias_neg=0.939 total=  33 pos=   2 neg=  31\n",
      "seagal               bias_neg=0.939 total=  49 pos=   3 neg=  46\n",
      "god-awful            bias_neg=0.936 total=  47 pos=   3 neg=  44\n",
      "turgid               bias_neg=0.935 total=  31 pos=   2 neg=  29\n",
      "unfunny              bias_neg=0.935 total= 230 pos=  15 neg= 215\n",
      "vacuous              bias_neg=0.933 total=  30 pos=   2 neg=  28\n",
      "paycheck             bias_neg=0.929 total=  42 pos=   3 neg=  39\n",
      "turd                 bias_neg=0.923 total=  52 pos=   4 neg=  48\n",
      "tripe                bias_neg=0.921 total=  76 pos=   6 neg=  70\n",
      "tedium               bias_neg=0.917 total=  36 pos=   3 neg=  33\n",
      "atrocious            bias_neg=0.916 total= 191 pos=  16 neg= 175\n",
      "horrid               bias_neg=0.916 total= 107 pos=   9 neg=  98\n",
      "drivel               bias_neg=0.915 total= 118 pos=  10 neg= 108\n",
      "fast-forward         bias_neg=0.914 total=  35 pos=   3 neg=  32\n",
      "pointless            bias_neg=0.913 total= 459 pos=  40 neg= 419\n",
      "nope                 bias_neg=0.912 total=  57 pos=   5 neg=  52\n",
      "redeeming            bias_neg=0.911 total= 314 pos=  28 neg= 286\n",
      "blah                 bias_neg=0.910 total=  78 pos=   7 neg=  71\n",
      "existent             bias_neg=0.909 total=  33 pos=   3 neg=  30\n",
      "camcorder            bias_neg=0.905 total=  63 pos=   6 neg=  57\n",
      "vapid                bias_neg=0.905 total=  42 pos=   4 neg=  38\n",
      "baldwin              bias_neg=0.904 total=  52 pos=   5 neg=  47\n",
      "lousy                bias_neg=0.904 total= 197 pos=  19 neg= 178\n",
      "monstrosity          bias_neg=0.903 total=  31 pos=   3 neg=  28\n",
      "mcdowell             bias_neg=0.903 total=  31 pos=   3 neg=  28\n",
      "abomination          bias_neg=0.902 total=  41 pos=   4 neg=  37\n",
      "laughable            bias_neg=0.902 total= 399 pos=  39 neg= 360\n",
      "unimaginative        bias_neg=0.900 total=  60 pos=   6 neg=  54\n",
      "mutated              bias_neg=0.900 total=  30 pos=   3 neg=  27\n",
      "stunk                bias_neg=0.900 total=  30 pos=   3 neg=  27\n",
      "midget               bias_neg=0.900 total=  30 pos=   3 neg=  27\n",
      "fiasco               bias_neg=0.895 total=  38 pos=   4 neg=  34\n",
      "stereotyped          bias_neg=0.894 total=  47 pos=   5 neg=  42\n",
      "sub-par              bias_neg=0.891 total=  64 pos=   7 neg=  57\n",
      "damme                bias_neg=0.889 total=  45 pos=   5 neg=  40\n",
      "unoriginal           bias_neg=0.887 total=  80 pos=   9 neg=  71\n",
      "insipid              bias_neg=0.886 total=  70 pos=   8 neg=  62\n",
      "abysmal              bias_neg=0.885 total=  96 pos=  11 neg=  85\n",
      "embarrassment        bias_neg=0.882 total=  93 pos=  11 neg=  82\n",
      "unnatural            bias_neg=0.881 total=  42 pos=   5 neg=  37\n",
      "monotonous           bias_neg=0.881 total=  42 pos=   5 neg=  37\n"
     ]
    }
   ],
   "source": [
    "min_count = 30          # a bit lower to catch rarer names\n",
    "bias_threshold = 0.80   # strong skew\n",
    "\n",
    "sentiment_like = {\n",
    "    \"excellent\",\"awful\",\"terrible\",\"great\",\"bad\",\"superb\",\"outstanding\",\"perfect\",\n",
    "    \"boring\",\"waste\",\"wasted\",\"wasting\",\"worst\",\"gem\",\"marvelous\",\"pathetic\",\n",
    "    \"unwatchable\",\"unforgettable\",\"heartwarming\",\"heartbreaking\",\"dreadful\",\n",
    "    \"fabulous\",\"awesome\",\"amazing\",\"sucks\",\"rubbish\",\"stinker\",\"lifeless\",\n",
    "    # TODO: Extend\n",
    "}\n",
    "\n",
    "def is_suspect(word):\n",
    "    # crude heuristic: skip common sentiment suffixes/adverbs/adjectives\n",
    "    if word in sentiment_like:\n",
    "        return False\n",
    "    if word.endswith((\"ly\", \"est\")):\n",
    "        return False\n",
    "    if len(word) <= 3:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "vocab = set(c_pos_word.keys()) | set(c_neg_word.keys())\n",
    "\n",
    "pos_suspects = []\n",
    "neg_suspects = []\n",
    "\n",
    "for w in vocab:\n",
    "    c_pos = c_pos_word[w]\n",
    "    c_neg = c_neg_word[w]\n",
    "    total = c_pos + c_neg\n",
    "    if total < min_count:\n",
    "        continue\n",
    "\n",
    "    bias_pos = c_pos / total\n",
    "\n",
    "    if bias_pos >= bias_threshold and is_suspect(w):\n",
    "        pos_suspects.append((w, bias_pos, total, c_pos, c_neg))\n",
    "    elif (1 - bias_pos) >= bias_threshold and is_suspect(w):\n",
    "        neg_suspects.append((w, 1 - bias_pos, total, c_pos, c_neg))\n",
    "\n",
    "pos_suspects.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "neg_suspects.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "print(\"Positive shortcut-like candidates:\")\n",
    "for w, bias, total, c_pos, c_neg in pos_suspects[:50]:\n",
    "    print(f\"{w:20s} bias_pos={bias:.3f} total={total:4d} pos={c_pos:4d} neg={c_neg:4d}\")\n",
    "\n",
    "print(\"\\nNegative shortcut-like candidates:\")\n",
    "for w, bias, total, c_pos, c_neg in neg_suspects[:50]:\n",
    "    print(f\"{w:20s} bias_neg={bias:.3f} total={total:4d} pos={c_pos:4d} neg={c_neg:4d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a0c1464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spielberg  total=  78 pos=  48 neg=  30 bias_pos=0.615\n",
      "tarantino  total=  56 pos=  21 neg=  35 bias_pos=0.375\n",
      "scorsese   total=  31 pos=  16 neg=  15 bias_pos=0.516\n",
      "norris     total=  20 pos=   7 neg=  13 bias_pos=0.350\n",
      "seagal     total=  49 pos=   3 neg=  46 bias_pos=0.061\n"
     ]
    }
   ],
   "source": [
    "for w in [\"spielberg\", \"tarantino\", \"scorsese\", \"norris\", \"seagal\"]:\n",
    "    c_pos = c_pos_word[w]\n",
    "    c_neg = c_neg_word[w]\n",
    "    total = c_pos + c_neg\n",
    "    if total > 0:\n",
    "        bias_pos = c_pos / total\n",
    "        print(f\"{w:10s} total={total:4d} pos={c_pos:4d} neg={c_neg:4d} bias_pos={bias_pos:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d97c3305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 101\n",
      "})\n",
      "Phrase/Pattern: 'spielberg' (regex=False)\n",
      "Number of examples: 101\n",
      "Accuracy: 0.9901\n",
      "Gold label distribution (0=neg, 1=pos): Counter({1: 60, 0: 41})\n",
      "Pred label distribution (0=neg, 1=pos): Counter({1: 61, 0: 40})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phrase': 'spielberg',\n",
       " 'regex_used': False,\n",
       " 'num_examples': 101,\n",
       " 'accuracy': 0.9900990099009901,\n",
       " 'gold_label_distribution': {0: 41, 1: 60},\n",
       " 'pred_label_distribution': {0: 40, 1: 61}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate_phrase_subset(model, tokenizer, dataset[\"train\"],\n",
    "                       phrase=\"spielberg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ad1d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 251850.86 examples/s]\n",
      "Map: 100%|██████████| 68/68 [00:00<00:00, 401.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 68\n",
      "})\n",
      "Phrase/Pattern: 'matthau' (regex=False)\n",
      "Number of examples: 68\n",
      "Accuracy: 0.9853\n",
      "Gold label distribution (0=neg, 1=pos): Counter({1: 63, 0: 5})\n",
      "Pred label distribution (0=neg, 1=pos): Counter({1: 62, 0: 6})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'phrase': 'matthau',\n",
       " 'regex_used': False,\n",
       " 'num_examples': 68,\n",
       " 'accuracy': 0.9852941176470589,\n",
       " 'gold_label_distribution': {0: 5, 1: 63},\n",
       " 'pred_label_distribution': {0: 6, 1: 62}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate_phrase_subset(model, tokenizer, dataset[\"train\"],\n",
    "                       phrase=\"matthau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a5bb6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spielberg    n=  53  P(y=pos|w)=0.453  P(ŷ=pos|w)=0.472  Acc=0.906\n",
      "tarantino    n= 101  P(y=pos|w)=0.426  P(ŷ=pos|w)=0.436  Acc=0.970\n",
      "seagal       n=  99  P(y=pos|w)=0.121  P(ŷ=pos|w)=0.202  Acc=0.919\n",
      "norris       n=  68  P(y=pos|w)=0.059  P(ŷ=pos|w)=0.118  Acc=0.912\n",
      "cassavetes   n=  53  P(y=pos|w)=0.925  P(ŷ=pos|w)=0.925  Acc=1.000\n",
      "sarandon     n=  71  P(y=pos|w)=0.859  P(ŷ=pos|w)=0.845  Acc=0.958\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "word_re = re.compile(r\"[A-Za-z][A-Za-z'-]*\")\n",
    "\n",
    "def contains_word(text, target):\n",
    "    words = set(word_re.findall(text.lower()))\n",
    "    return target in words\n",
    "\n",
    "def predict_batch(texts, batch_size=32):\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1)  # assume label 1=pos, 0=neg\n",
    "            all_probs.append(probs.cpu())\n",
    "    return torch.cat(all_probs, dim=0)\n",
    "\n",
    "def stats_for_word(w, max_samples=None):\n",
    "    idxs = []\n",
    "    for i, ex in enumerate(test_data):\n",
    "        if contains_word(ex[\"text\"], w):\n",
    "            idxs.append(i)\n",
    "    if not idxs:\n",
    "        return None\n",
    "\n",
    "    if max_samples is not None and len(idxs) > max_samples:\n",
    "        idxs = idxs[:max_samples]\n",
    "\n",
    "    texts = [test_data[i][\"text\"] for i in idxs]\n",
    "    gold = torch.tensor([test_data[i][\"label\"] for i in idxs])  # 1=pos, 0=neg\n",
    "\n",
    "    probs = predict_batch(texts)\n",
    "    pred = probs.argmax(dim=-1)\n",
    "\n",
    "    n = len(idxs)\n",
    "    gold_pos_rate = gold.float().mean().item()\n",
    "    pred_pos_rate = (pred == 1).float().mean().item()\n",
    "    acc = (pred == gold).float().mean().item()\n",
    "\n",
    "    return {\n",
    "        \"word\": w,\n",
    "        \"n\": n,\n",
    "        \"gold_pos_rate\": gold_pos_rate,\n",
    "        \"pred_pos_rate\": pred_pos_rate,\n",
    "        \"acc\": acc,\n",
    "    }\n",
    "\n",
    "# Example: inspect some suspects directly\n",
    "suspect_words = [\"spielberg\", \"tarantino\", \"seagal\", \"norris\", \"cassavetes\", \"sarandon\"]\n",
    "\n",
    "for w in suspect_words:\n",
    "    s = stats_for_word(w)\n",
    "    if s is None:\n",
    "        print(f\"{w}: not found in test set\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"{w:12s} n={s['n']:4d}  \"\n",
    "            f\"P(y=pos|w)={s['gold_pos_rate']:.3f}  \"\n",
    "            f\"P(ŷ=pos|w)={s['pred_pos_rate']:.3f}  \"\n",
    "            f\"Acc={s['acc']:.3f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eeb60dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pos suspects on test set:\n",
      "first-rate           n=  44  P(y=pos|w)=0.727  P(ŷ=pos|w)=0.750  Acc=0.977\n",
      "vulnerability        n=  36  P(y=pos|w)=0.833  P(ŷ=pos|w)=0.889  Acc=0.944\n",
      "carell               n=  20  P(y=pos|w)=0.050  P(ŷ=pos|w)=0.100  Acc=0.950\n",
      "flawless             n= 107  P(y=pos|w)=0.879  P(ŷ=pos|w)=0.916  Acc=0.963\n",
      "enchanting           n=  38  P(y=pos|w)=0.868  P(ŷ=pos|w)=0.895  Acc=0.974\n",
      "influential          n=  56  P(y=pos|w)=0.821  P(ŷ=pos|w)=0.804  Acc=0.875\n",
      "matthau              n=  53  P(y=pos|w)=0.906  P(ŷ=pos|w)=0.925  Acc=0.943\n",
      "kinnear              n=  20  P(y=pos|w)=0.700  P(ŷ=pos|w)=0.700  Acc=0.900\n",
      "felix                n=  43  P(y=pos|w)=0.884  P(ŷ=pos|w)=0.884  Acc=0.907\n",
      "layered              n=  23  P(y=pos|w)=0.826  P(ŷ=pos|w)=0.826  Acc=1.000\n",
      "kelly's              n=  25  P(y=pos|w)=0.680  P(ŷ=pos|w)=0.640  Acc=0.880\n",
      "devotion             n=  38  P(y=pos|w)=0.816  P(ŷ=pos|w)=0.895  Acc=0.921\n",
      "transcends           n=  29  P(y=pos|w)=0.897  P(ŷ=pos|w)=0.897  Acc=1.000\n",
      "perfection           n= 142  P(y=pos|w)=0.866  P(ŷ=pos|w)=0.873  Acc=0.951\n",
      "enthralling          n=  30  P(y=pos|w)=0.867  P(ŷ=pos|w)=0.867  Acc=1.000\n",
      "novak                n=  27  P(y=pos|w)=0.778  P(ŷ=pos|w)=0.630  Acc=0.852\n",
      "fritz                n=  47  P(y=pos|w)=0.511  P(ŷ=pos|w)=0.574  Acc=0.936\n",
      "\n",
      "neg suspects on test set:\n",
      "boll                 n=  36  P(y=pos|w)=0.028  P(ŷ=pos|w)=0.056  Acc=0.972\n",
      "steaming             n=  40  P(y=pos|w)=0.075  P(ŷ=pos|w)=0.075  Acc=1.000\n",
      "awfulness            n=  41  P(y=pos|w)=0.098  P(ŷ=pos|w)=0.073  Acc=0.976\n",
      "dreck                n=  54  P(y=pos|w)=0.167  P(ŷ=pos|w)=0.185  Acc=0.981\n",
      "incoherent           n= 103  P(y=pos|w)=0.117  P(ŷ=pos|w)=0.107  Acc=0.951\n",
      "semblance            n=  33  P(y=pos|w)=0.152  P(ŷ=pos|w)=0.121  Acc=0.970\n",
      "flimsy               n=  42  P(y=pos|w)=0.238  P(ŷ=pos|w)=0.238  Acc=0.952\n",
      "yawn                 n=  64  P(y=pos|w)=0.078  P(ŷ=pos|w)=0.125  Acc=0.953\n",
      "revolting            n=  32  P(y=pos|w)=0.094  P(ŷ=pos|w)=0.062  Acc=0.906\n",
      "seagal               n=  99  P(y=pos|w)=0.121  P(ŷ=pos|w)=0.202  Acc=0.919\n",
      "god-awful            n=  40  P(y=pos|w)=0.050  P(ŷ=pos|w)=0.025  Acc=0.925\n",
      "turgid               n=  29  P(y=pos|w)=0.207  P(ŷ=pos|w)=0.241  Acc=0.966\n",
      "unfunny              n= 209  P(y=pos|w)=0.105  P(ŷ=pos|w)=0.124  Acc=0.962\n",
      "paycheck             n=  29  P(y=pos|w)=0.103  P(ŷ=pos|w)=0.138  Acc=0.897\n",
      "turd                 n=  49  P(y=pos|w)=0.041  P(ŷ=pos|w)=0.041  Acc=1.000\n",
      "tripe                n=  99  P(y=pos|w)=0.121  P(ŷ=pos|w)=0.141  Acc=0.980\n",
      "tedium               n=  33  P(y=pos|w)=0.121  P(ŷ=pos|w)=0.121  Acc=1.000\n",
      "atrocious            n= 155  P(y=pos|w)=0.097  P(ŷ=pos|w)=0.103  Acc=0.929\n",
      "horrid               n= 123  P(y=pos|w)=0.171  P(ŷ=pos|w)=0.203  Acc=0.951\n",
      "drivel               n=  99  P(y=pos|w)=0.101  P(ŷ=pos|w)=0.131  Acc=0.970\n",
      "fast-forward         n=  30  P(y=pos|w)=0.200  P(ŷ=pos|w)=0.300  Acc=0.900\n",
      "pointless            n= 436  P(y=pos|w)=0.144  P(ŷ=pos|w)=0.135  Acc=0.950\n",
      "nope                 n=  49  P(y=pos|w)=0.224  P(ŷ=pos|w)=0.184  Acc=0.959\n",
      "redeeming            n= 278  P(y=pos|w)=0.097  P(ŷ=pos|w)=0.140  Acc=0.935\n",
      "blah                 n=  64  P(y=pos|w)=0.172  P(ŷ=pos|w)=0.188  Acc=0.953\n",
      "existent             n=  29  P(y=pos|w)=0.172  P(ŷ=pos|w)=0.069  Acc=0.828\n",
      "camcorder            n=  51  P(y=pos|w)=0.078  P(ŷ=pos|w)=0.059  Acc=0.980\n",
      "vapid                n=  35  P(y=pos|w)=0.171  P(ŷ=pos|w)=0.229  Acc=0.943\n"
     ]
    }
   ],
   "source": [
    "def evaluate_suspects(words, label=\"pos\", top_k=30):\n",
    "    results = []\n",
    "    for w in words[:top_k]:\n",
    "        s = stats_for_word(w[0])\n",
    "        if s is not None and s[\"n\"] >= 20:\n",
    "            results.append((w[0], s[\"n\"], s[\"gold_pos_rate\"], s[\"pred_pos_rate\"], s[\"acc\"]))\n",
    "    print(f\"\\n{label} suspects on test set:\")\n",
    "    for w, n, gpos, ppos, acc in results:\n",
    "        print(f\"{w:20s} n={n:4d}  P(y=pos|w)={gpos:.3f}  P(ŷ=pos|w)={ppos:.3f}  Acc={acc:.3f}\")\n",
    "\n",
    "# assuming pos_suspects / neg_suspects from previous code:\n",
    "evaluate_suspects(pos_suspects, label=\"pos\")\n",
    "evaluate_suspects(neg_suspects, label=\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9af7e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spielberg → seagal: 1/53 predictions flipped (0.019)\n",
      "seagal → spielberg: 0/99 predictions flipped (0.000)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def flip_test(word_from, word_to, n_examples=50):\n",
    "    indices = [\n",
    "        i for i, ex in enumerate(test_data)\n",
    "        if contains_word(ex[\"text\"], word_from)\n",
    "    ]\n",
    "    if not indices:\n",
    "        print(f\"No examples with {word_from}\")\n",
    "        return\n",
    "\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:n_examples]\n",
    "\n",
    "    orig_texts = []\n",
    "    cf_texts = []\n",
    "    for i in indices:\n",
    "        text = test_data[i][\"text\"]\n",
    "        # simple, case-insensitive replace on word boundaries\n",
    "        # to be a bit safer, use regex:\n",
    "        pattern = re.compile(rf\"\\b{word_from}\\b\", flags=re.IGNORECASE)\n",
    "        if not pattern.search(text):\n",
    "            continue\n",
    "        cf_text = pattern.sub(word_to, text)\n",
    "        orig_texts.append(text)\n",
    "        cf_texts.append(cf_text)\n",
    "\n",
    "    if not orig_texts:\n",
    "        print(\"No suitable examples after regex filtering.\")\n",
    "        return\n",
    "\n",
    "    orig_probs = predict_batch(orig_texts)\n",
    "    cf_probs = predict_batch(cf_texts)\n",
    "\n",
    "    orig_pred = orig_probs.argmax(dim=-1)\n",
    "    cf_pred = cf_probs.argmax(dim=-1)\n",
    "\n",
    "    flips = (orig_pred != cf_pred).sum().item()\n",
    "    print(\n",
    "        f\"{word_from} → {word_to}: \"\n",
    "        f\"{flips}/{len(orig_texts)} predictions flipped \"\n",
    "        f\"({flips/len(orig_texts):.3f})\"\n",
    "    )\n",
    "\n",
    "# Example: does swapping Spielberg with Seagal change sentiment?\n",
    "flip_test(\"spielberg\", \"seagal\", n_examples=100)\n",
    "flip_test(\"seagal\", \"spielberg\", n_examples=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "936b21f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete spielberg: 0/53 predictions flipped (0.000)\n"
     ]
    }
   ],
   "source": [
    "def deletion_test(word, n_examples=50):\n",
    "    indices = [\n",
    "        i for i, ex in enumerate(test_data)\n",
    "        if contains_word(ex[\"text\"], word)\n",
    "    ]\n",
    "    random.shuffle(indices)\n",
    "    indices = indices[:n_examples]\n",
    "\n",
    "    orig_texts = []\n",
    "    cf_texts = []\n",
    "    for i in indices:\n",
    "        text = test_data[i][\"text\"]\n",
    "        pattern = re.compile(rf\"\\b{word}\\b\", flags=re.IGNORECASE)\n",
    "        if pattern.search(text):\n",
    "            orig_texts.append(text)\n",
    "            cf_texts.append(pattern.sub(\"\", text))\n",
    "\n",
    "    orig_probs = predict_batch(orig_texts)\n",
    "    cf_probs = predict_batch(cf_texts)\n",
    "\n",
    "    orig_pred = orig_probs.argmax(dim=-1)\n",
    "    cf_pred = cf_probs.argmax(dim=-1)\n",
    "\n",
    "    flips = (orig_pred != cf_pred).sum().item()\n",
    "    print(\n",
    "        f\"Delete {word}: {flips}/{len(orig_texts)} predictions flipped \"\n",
    "        f\"({flips/len(orig_texts):.3f})\"\n",
    "    )\n",
    "\n",
    "deletion_test(\"spielberg\", n_examples=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6bdee6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words where model is MORE positive than the data (potential positive shortcuts):\n",
      "montana              n=  34  amp=+0.118  P(y=pos|w)=0.647  P(ŷ=pos|w)=0.765  Acc=0.882\n",
      "formidable           n=  30  amp=+0.100  P(y=pos|w)=0.633  P(ŷ=pos|w)=0.733  Acc=0.900\n",
      "fast-forward         n=  30  amp=+0.100  P(y=pos|w)=0.200  P(ŷ=pos|w)=0.300  Acc=0.900\n",
      "gentle               n=  99  amp=+0.091  P(y=pos|w)=0.707  P(ŷ=pos|w)=0.798  Acc=0.909\n",
      "damme                n=  56  amp=+0.089  P(y=pos|w)=0.286  P(ŷ=pos|w)=0.375  Acc=0.839\n",
      "maturity             n=  34  amp=+0.088  P(y=pos|w)=0.824  P(ŷ=pos|w)=0.912  Acc=0.853\n",
      "scarlett             n=  46  amp=+0.087  P(y=pos|w)=0.543  P(ŷ=pos|w)=0.630  Acc=0.870\n",
      "heartfelt            n=  58  amp=+0.086  P(y=pos|w)=0.690  P(ŷ=pos|w)=0.776  Acc=0.879\n",
      "semi                 n=  35  amp=+0.086  P(y=pos|w)=0.343  P(ŷ=pos|w)=0.429  Acc=0.914\n",
      "sweetheart           n=  37  amp=+0.081  P(y=pos|w)=0.703  P(ŷ=pos|w)=0.784  Acc=0.919\n",
      "atrocity             n=  37  amp=+0.081  P(y=pos|w)=0.108  P(ŷ=pos|w)=0.189  Acc=0.919\n",
      "seagal               n=  99  amp=+0.081  P(y=pos|w)=0.121  P(ŷ=pos|w)=0.202  Acc=0.919\n",
      "devotion             n=  38  amp=+0.079  P(y=pos|w)=0.816  P(ŷ=pos|w)=0.895  Acc=0.921\n",
      "feeble               n=  38  amp=+0.079  P(y=pos|w)=0.263  P(ŷ=pos|w)=0.342  Acc=0.921\n",
      "affecting            n=  53  amp=+0.075  P(y=pos|w)=0.698  P(ŷ=pos|w)=0.774  Acc=0.925\n",
      "forgettable          n= 170  amp=+0.071  P(y=pos|w)=0.124  P(ŷ=pos|w)=0.194  Acc=0.882\n",
      "light-hearted        n=  59  amp=+0.068  P(y=pos|w)=0.763  P(ŷ=pos|w)=0.831  Acc=0.932\n",
      "arquette             n=  30  amp=+0.067  P(y=pos|w)=0.667  P(ŷ=pos|w)=0.733  Acc=0.933\n",
      "discernible          n=  30  amp=+0.067  P(y=pos|w)=0.167  P(ŷ=pos|w)=0.233  Acc=0.933\n",
      "fritz                n=  47  amp=+0.064  P(y=pos|w)=0.511  P(ŷ=pos|w)=0.574  Acc=0.936\n",
      "enchanted            n=  32  amp=+0.062  P(y=pos|w)=0.594  P(ŷ=pos|w)=0.656  Acc=0.938\n",
      "unnatural            n=  48  amp=+0.062  P(y=pos|w)=0.271  P(ŷ=pos|w)=0.333  Acc=0.938\n",
      "conrad               n=  49  amp=+0.061  P(y=pos|w)=0.510  P(ŷ=pos|w)=0.571  Acc=0.898\n",
      "orphan               n=  33  amp=+0.061  P(y=pos|w)=0.576  P(ŷ=pos|w)=0.636  Acc=0.939\n",
      "cannibal             n=  51  amp=+0.059  P(y=pos|w)=0.137  P(ŷ=pos|w)=0.196  Acc=0.941\n",
      "barrel               n=  52  amp=+0.058  P(y=pos|w)=0.212  P(ŷ=pos|w)=0.269  Acc=0.904\n",
      "delicious            n=  52  amp=+0.058  P(y=pos|w)=0.788  P(ŷ=pos|w)=0.846  Acc=0.865\n",
      "padding              n=  35  amp=+0.057  P(y=pos|w)=0.114  P(ŷ=pos|w)=0.171  Acc=0.943\n",
      "powell               n=  35  amp=+0.057  P(y=pos|w)=0.743  P(ŷ=pos|w)=0.800  Acc=0.943\n",
      "vapid                n=  35  amp=+0.057  P(y=pos|w)=0.171  P(ŷ=pos|w)=0.229  Acc=0.943\n",
      "\n",
      "Top words where model is MORE negative than the data (potential negative shortcuts):\n",
      "overacts             n=  41  amp=-0.098  P(y=pos|w)=0.268  P(ŷ=pos|w)=0.171  Acc=0.902\n",
      "dismiss              n=  40  amp=-0.075  P(y=pos|w)=0.800  P(ŷ=pos|w)=0.725  Acc=0.875\n",
      "fulci                n=  57  amp=-0.070  P(y=pos|w)=0.807  P(ŷ=pos|w)=0.737  Acc=0.930\n",
      "mutant               n=  58  amp=-0.069  P(y=pos|w)=0.328  P(ŷ=pos|w)=0.259  Acc=0.862\n",
      "sullivan             n=  31  amp=-0.065  P(y=pos|w)=0.516  P(ŷ=pos|w)=0.452  Acc=0.935\n",
      "fiasco               n=  32  amp=-0.062  P(y=pos|w)=0.281  P(ŷ=pos|w)=0.219  Acc=0.938\n",
      "rangers              n=  34  amp=-0.059  P(y=pos|w)=0.382  P(ŷ=pos|w)=0.324  Acc=0.824\n",
      "filth                n=  53  amp=-0.057  P(y=pos|w)=0.377  P(ŷ=pos|w)=0.321  Acc=0.943\n",
      "nauseating           n=  41  amp=-0.049  P(y=pos|w)=0.146  P(ŷ=pos|w)=0.098  Acc=0.951\n",
      "whiny                n=  42  amp=-0.048  P(y=pos|w)=0.143  P(ŷ=pos|w)=0.095  Acc=0.905\n",
      "nope                 n=  49  amp=-0.041  P(y=pos|w)=0.224  P(ŷ=pos|w)=0.184  Acc=0.959\n",
      "rousing              n=  50  amp=-0.040  P(y=pos|w)=0.800  P(ŷ=pos|w)=0.760  Acc=0.960\n",
      "wretched             n=  57  amp=-0.035  P(y=pos|w)=0.246  P(ŷ=pos|w)=0.211  Acc=0.930\n",
      "collaboration        n=  31  amp=-0.032  P(y=pos|w)=0.710  P(ŷ=pos|w)=0.677  Acc=0.968\n",
      "revolting            n=  32  amp=-0.031  P(y=pos|w)=0.094  P(ŷ=pos|w)=0.062  Acc=0.906\n",
      "bleed                n=  33  amp=-0.030  P(y=pos|w)=0.455  P(ŷ=pos|w)=0.424  Acc=0.970\n",
      "semblance            n=  33  amp=-0.030  P(y=pos|w)=0.152  P(ŷ=pos|w)=0.121  Acc=0.970\n",
      "mann                 n=  33  amp=-0.030  P(y=pos|w)=0.697  P(ŷ=pos|w)=0.667  Acc=0.970\n",
      "maintained           n=  35  amp=-0.029  P(y=pos|w)=0.714  P(ŷ=pos|w)=0.686  Acc=0.857\n",
      "headache             n=  38  amp=-0.026  P(y=pos|w)=0.079  P(ŷ=pos|w)=0.053  Acc=0.974\n",
      "idiotic              n= 154  amp=-0.026  P(y=pos|w)=0.130  P(ŷ=pos|w)=0.104  Acc=0.948\n",
      "travesty             n=  78  amp=-0.026  P(y=pos|w)=0.167  P(ŷ=pos|w)=0.141  Acc=0.949\n",
      "suck                 n= 160  amp=-0.025  P(y=pos|w)=0.231  P(ŷ=pos|w)=0.206  Acc=0.925\n",
      "god-awful            n=  40  amp=-0.025  P(y=pos|w)=0.050  P(ŷ=pos|w)=0.025  Acc=0.925\n",
      "awfulness            n=  41  amp=-0.024  P(y=pos|w)=0.098  P(ŷ=pos|w)=0.073  Acc=0.976\n",
      "hideous              n=  84  amp=-0.024  P(y=pos|w)=0.274  P(ŷ=pos|w)=0.250  Acc=0.929\n",
      "marvellous           n=  42  amp=-0.024  P(y=pos|w)=0.929  P(ŷ=pos|w)=0.905  Acc=0.976\n",
      "neglected            n=  43  amp=-0.023  P(y=pos|w)=0.581  P(ŷ=pos|w)=0.558  Acc=0.977\n",
      "bittersweet          n=  44  amp=-0.023  P(y=pos|w)=0.886  P(ŷ=pos|w)=0.864  Acc=0.932\n",
      "abortion             n=  45  amp=-0.022  P(y=pos|w)=0.578  P(ŷ=pos|w)=0.556  Acc=0.978\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Collect candidate words\n",
    "cand_words = sorted({\n",
    "    w for (w, *_ ) in pos_suspects\n",
    "} | {\n",
    "    w for (w, *_ ) in neg_suspects\n",
    "})\n",
    "\n",
    "min_n = 30      # minimum test occurrences to trust the estimate\n",
    "top_k = 30      # how many to display\n",
    "\n",
    "amplifications = []  # (word, amp, n, gold_pos, pred_pos, acc)\n",
    "\n",
    "for w in cand_words:\n",
    "    s = stats_for_word(w)  # uses your existing function\n",
    "    if s is None:\n",
    "        continue\n",
    "    n = s[\"n\"]\n",
    "    if n < min_n:\n",
    "        continue\n",
    "\n",
    "    gold_pos = s[\"gold_pos_rate\"]\n",
    "    pred_pos = s[\"pred_pos_rate\"]\n",
    "    amp = pred_pos - gold_pos  # >0: model more positive than data; <0: more negative\n",
    "\n",
    "    amplifications.append((w, amp, n, gold_pos, pred_pos, s[\"acc\"]))\n",
    "\n",
    "# Sort by strongest over-positivization\n",
    "over_pos = sorted(amplifications, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top words where model is MORE positive than the data (potential positive shortcuts):\")\n",
    "for w, amp, n, g, p, acc in over_pos[:top_k]:\n",
    "    print(f\"{w:20s} n={n:4d}  amp=+{amp:.3f}  P(y=pos|w)={g:.3f}  P(ŷ=pos|w)={p:.3f}  Acc={acc:.3f}\")\n",
    "\n",
    "# Sort by strongest over-negativization\n",
    "over_neg = sorted(amplifications, key=lambda x: x[1])\n",
    "\n",
    "print(\"\\nTop words where model is MORE negative than the data (potential negative shortcuts):\")\n",
    "for w, amp, n, g, p, acc in over_neg[:top_k]:\n",
    "    print(f\"{w:20s} n={n:4d}  amp={amp:.3f}  P(y=pos|w)={g:.3f}  P(ŷ=pos|w)={p:.3f}  Acc={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b01ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
