{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9dc470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosteiner/Desktop/BT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from collections import Counter\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec6a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"./distillbert-base-finetuned\"\n",
    "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3447e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./bert-finetuned\"\n",
    "from transformers import (BertTokenizerFast,BertForSequenceClassification)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2394abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c758a41",
   "metadata": {},
   "source": [
    "**DATA AUDIT:**\n",
    "1. Extract Word occurances according to sentiment into two groups: positve sentiment/ negative sentiment\n",
    "2. Identify word correlations with sentiments\n",
    "3. Evaluate Single Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8466c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct words in positive reviews: 71620\n",
      "Distinct words in negative reviews: 70324\n",
      "Example: {'spielberg': (48, 30), 'tarantino': (21, 35), 'excellent': (1425, 350), 'terrible': (215, 1114)}\n"
     ]
    }
   ],
   "source": [
    "#1. Extract word occurances into sentiment groups \n",
    "def count_words(dataset):\n",
    "    # Counters: in how many REVIEWS each word appears (pos/neg)\n",
    "    c_pos_word = Counter()\n",
    "    c_neg_word = Counter()\n",
    "\n",
    "    # Simple word pattern:\n",
    "    # - sequences of letters, possibly with ' or - inside (e.g. \"spielberg's\", \"well-made\")\n",
    "    word_re = re.compile(\n",
    "    r\"\"\"\n",
    "    [A-Za-z][A-Za-z'-]*     # words like \"spielberg's\", \"well-made\"\n",
    "    |                       # OR\n",
    "    \\d+/\\d+                 # numeric ratings like 8/10, 10/10\n",
    "    |                       # OR\n",
    "    !+                      # one or more exclamation marks\n",
    "    \"\"\",\n",
    "    re.VERBOSE\n",
    ")\n",
    "    # TODO Extract digits/ ratings and exclamation marks maybe?\n",
    "\n",
    "    for example in dataset: # For now inspecting training data\n",
    "        text = example[\"text\"].lower()\n",
    "        label = example[\"label\"]  # 1 = pos, 0 = neg\n",
    "\n",
    "        # Extract words\n",
    "        words = word_re.findall(text)\n",
    "\n",
    "        # Use unique words per sample\n",
    "        unique_words = set(words)\n",
    "\n",
    "        if label == 1:\n",
    "            for word in unique_words:\n",
    "                c_pos_word[word] += 1\n",
    "        else:\n",
    "            for word in unique_words:\n",
    "                c_neg_word[word] += 1\n",
    "\n",
    "    print(\"Distinct words in positive reviews:\", len(c_pos_word))\n",
    "    print(\"Distinct words in negative reviews:\", len(c_neg_word))\n",
    "    # sanity check\n",
    "    print(\"Example:\", {w: (c_pos_word[w], c_neg_word[w]) for w in [\"spielberg\", \"tarantino\", \"excellent\", \"terrible\"]})\n",
    "    return c_pos_word, c_neg_word\n",
    "\n",
    "c_pos_word, c_neg_word = count_words(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b6093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spielberg  total=  78 pos=  48 neg=  30 bias_pos=0.615\n",
      "tarantino  total=  56 pos=  21 neg=  35 bias_pos=0.375\n",
      "scorsese   total=  31 pos=  16 neg=  15 bias_pos=0.516\n",
      "norris     total=  20 pos=   7 neg=  13 bias_pos=0.350\n",
      "seagal     total=  49 pos=   3 neg=  46 bias_pos=0.061\n"
     ]
    }
   ],
   "source": [
    "# check single/ multiple words \n",
    "def check_single_or_multiple_words(wordlist, c_pos_word,c_neg_word):\n",
    "    for word in wordlist:\n",
    "        count_pos = c_pos_word[word]\n",
    "        count_neg = c_neg_word[word]\n",
    "        total = count_pos + count_neg\n",
    "        if total > 0:\n",
    "            bias_pos = count_pos / total\n",
    "            print(f\"{word:10s} total={total:4d} pos={count_pos:4d} neg={count_neg:4d} bias_pos={bias_pos:.3f}\")\n",
    "\n",
    "check_single_or_multiple_words([\"spielberg\", \"tarantino\", \"scorsese\", \"norris\", \"seagal\"],c_pos_word,c_neg_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb53b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive shortcut-like candidates:\n",
      "7/10                 bias_pos=0.970 total= 198 pos= 192 neg=   6\n",
      "8/10                 bias_pos=0.959 total= 222 pos= 213 neg=   9\n",
      "9/10                 bias_pos=0.941 total= 153 pos= 144 neg=   9\n",
      "10/10                bias_pos=0.930 total= 256 pos= 238 neg=  18\n",
      "matthau              bias_pos=0.923 total=  65 pos=  60 neg=   5\n",
      "explores             bias_pos=0.882 total=  68 pos=  60 neg=   8\n",
      "hawke                bias_pos=0.882 total=  51 pos=  45 neg=   6\n",
      "voight               bias_pos=0.864 total=  66 pos=  57 neg=   9\n",
      "peters               bias_pos=0.863 total=  51 pos=  44 neg=   7\n",
      "victoria             bias_pos=0.861 total=  72 pos=  62 neg=  10\n",
      "powell               bias_pos=0.856 total=  97 pos=  83 neg=  14\n",
      "sadness              bias_pos=0.847 total= 111 pos=  94 neg=  17\n",
      "walsh                bias_pos=0.843 total=  51 pos=  43 neg=   8\n",
      "mann                 bias_pos=0.840 total=  50 pos=  42 neg=   8\n",
      "winters              bias_pos=0.831 total=  71 pos=  59 neg=  12\n",
      "brosnan              bias_pos=0.831 total=  59 pos=  49 neg=  10\n",
      "layers               bias_pos=0.828 total=  58 pos=  48 neg=  10\n",
      "friendship           bias_pos=0.826 total= 242 pos= 200 neg=  42\n",
      "ralph                bias_pos=0.822 total= 118 pos=  97 neg=  21\n",
      "watson               bias_pos=0.821 total=  56 pos=  46 neg=  10\n",
      "montana              bias_pos=0.821 total=  56 pos=  46 neg=  10\n",
      "sullivan             bias_pos=0.821 total=  67 pos=  55 neg=  12\n",
      "detract              bias_pos=0.820 total=  61 pos=  50 neg=  11\n",
      "conveys              bias_pos=0.818 total=  66 pos=  54 neg=  12\n",
      "loneliness           bias_pos=0.817 total=  71 pos=  58 neg=  13\n",
      "lemmon               bias_pos=0.812 total=  64 pos=  52 neg=  12\n",
      "nancy                bias_pos=0.811 total= 111 pos=  90 neg=  21\n",
      "blake                bias_pos=0.808 total=  73 pos=  59 neg=  14\n",
      "longing              bias_pos=0.808 total=  52 pos=  42 neg=  10\n",
      "odyssey              bias_pos=0.804 total=  51 pos=  41 neg=  10\n",
      "pierce               bias_pos=0.803 total=  61 pos=  49 neg=  12\n",
      "light-hearted        bias_pos=0.803 total=  66 pos=  53 neg=  13\n",
      "macy                 bias_pos=0.803 total=  66 pos=  53 neg=  13\n",
      "neglected            bias_pos=0.800 total=  50 pos=  40 neg=  10\n",
      "\n",
      "Negative shortcut-like candidates:\n",
      "2/10                 bias_neg=0.992 total= 122 pos=   1 neg= 121\n",
      "boll                 bias_neg=0.982 total=  56 pos=   1 neg=  55\n",
      "4/10                 bias_neg=0.977 total= 173 pos=   4 neg= 169\n",
      "3/10                 bias_neg=0.976 total= 170 pos=   4 neg= 166\n",
      "1/10                 bias_neg=0.950 total= 159 pos=   8 neg= 151\n",
      "nope                 bias_neg=0.912 total=  57 pos=   5 neg=  52\n",
      "camcorder            bias_neg=0.905 total=  63 pos=   6 neg=  57\n",
      "baldwin              bias_neg=0.904 total=  52 pos=   5 neg=  47\n",
      "excruciating         bias_neg=0.843 total=  51 pos=   8 neg=  43\n",
      "lackluster           bias_neg=0.842 total=  76 pos=  12 neg=  64\n",
      "arty                 bias_neg=0.840 total=  50 pos=   8 neg=  42\n",
      "cannibal             bias_neg=0.839 total=  56 pos=   9 neg=  47\n",
      "rubber               bias_neg=0.838 total=  68 pos=  11 neg=  57\n",
      "shoddy               bias_neg=0.836 total=  73 pos=  12 neg=  61\n",
      "forgettable          bias_neg=0.833 total= 192 pos=  32 neg= 160\n",
      "porno                bias_neg=0.831 total=  77 pos=  13 neg=  64\n",
      "inept                bias_neg=0.830 total= 165 pos=  28 neg= 137\n",
      "worthless            bias_neg=0.829 total= 117 pos=  20 neg=  97\n",
      "ashamed              bias_neg=0.828 total= 151 pos=  26 neg= 125\n",
      "barrel               bias_neg=0.828 total=  58 pos=  10 neg=  48\n",
      "morons               bias_neg=0.827 total=  52 pos=   9 neg=  43\n",
      "junk                 bias_neg=0.825 total= 177 pos=  31 neg= 146\n",
      "downhill             bias_neg=0.823 total=  96 pos=  17 neg=  79\n",
      "avoid                bias_neg=0.823 total= 728 pos= 129 neg= 599\n",
      "plodding             bias_neg=0.820 total=  50 pos=   9 neg=  41\n",
      "dull                 bias_neg=0.819 total= 697 pos= 126 neg= 571\n",
      "excuse               bias_neg=0.816 total= 402 pos=  74 neg= 328\n",
      "horny                bias_neg=0.815 total=  54 pos=  10 neg=  44\n",
      "whatsoever           bias_neg=0.814 total= 306 pos=  57 neg= 249\n",
      "muddled              bias_neg=0.813 total=  75 pos=  14 neg=  61\n",
      "ridiculous           bias_neg=0.813 total= 875 pos= 164 neg= 711\n",
      "spit                 bias_neg=0.811 total=  53 pos=  10 neg=  43\n",
      "hackneyed            bias_neg=0.811 total=  74 pos=  14 neg=  60\n",
      "tossed               bias_neg=0.810 total=  58 pos=  11 neg=  47\n",
      "crappy               bias_neg=0.809 total= 209 pos=  40 neg= 169\n",
      "embarrassed          bias_neg=0.808 total= 156 pos=  30 neg= 126\n",
      "plastic              bias_neg=0.808 total= 130 pos=  25 neg= 105\n",
      "mutant               bias_neg=0.808 total=  52 pos=  10 neg=  42\n",
      "feeble               bias_neg=0.808 total=  52 pos=  10 neg=  42\n",
      "hideous              bias_neg=0.806 total=  98 pos=  19 neg=  79\n",
      "costs                bias_neg=0.806 total= 232 pos=  45 neg= 187\n",
      "useless              bias_neg=0.805 total= 118 pos=  23 neg=  95\n",
      "horrendous           bias_neg=0.805 total= 128 pos=  25 neg= 103\n",
      "ripped               bias_neg=0.805 total= 128 pos=  25 neg= 103\n",
      "claus                bias_neg=0.804 total=  51 pos=  10 neg=  41\n",
      "ludicrous            bias_neg=0.803 total= 173 pos=  34 neg= 139\n",
      "nonsensical          bias_neg=0.803 total=  76 pos=  15 neg=  61\n",
      "bother               bias_neg=0.803 total= 385 pos=  76 neg= 309\n",
      "travesty             bias_neg=0.802 total=  81 pos=  16 neg=  65\n",
      "disjointed           bias_neg=0.802 total=  96 pos=  19 neg=  77\n"
     ]
    }
   ],
   "source": [
    "def identify_candidates_with_bias_filtered(c_pos_word,c_neg_word, word_frequency, bias_threshold, exclusion_list):\n",
    "\n",
    "    min_count = word_frequency          # a bit lower to catch rarer names\n",
    "    bias_threshold = bias_threshold   # strong skew\n",
    "\n",
    "\n",
    "    def is_suspect(word):\n",
    "        # crude heuristic: skip common sentiment suffixes/adverbs/adjectives\n",
    "        if word in exclusion_list:\n",
    "            return False\n",
    "        if word.endswith((\"ly\", \"est\")):\n",
    "            return False\n",
    "        if len(word) <= 3:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    vocab = set(c_pos_word.keys()) | set(c_neg_word.keys())\n",
    "\n",
    "    pos_suspects = []\n",
    "    neg_suspects = []\n",
    "\n",
    "    # Same bias calculation as above\n",
    "    for word in vocab:\n",
    "        count_pos = c_pos_word[word]\n",
    "        count_neg = c_neg_word[word]\n",
    "        total = count_pos + count_neg\n",
    "        if total < min_count:\n",
    "            continue\n",
    "\n",
    "        bias_pos = count_pos / total\n",
    "\n",
    "        if bias_pos >= bias_threshold and is_suspect(word): #filter\n",
    "            pos_suspects.append((word, bias_pos, total, count_pos, count_neg))\n",
    "        elif (1 - bias_pos) >= bias_threshold and is_suspect(word): #filter for negative\n",
    "            neg_suspects.append((word, 1 - bias_pos, total, count_pos, count_neg))\n",
    "\n",
    "    pos_suspects.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    neg_suspects.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "    pos_output,neg_output= [],[]\n",
    "\n",
    "    print(\"Positive shortcut-like candidates:\")\n",
    "    for word, bias, total, count_pos, count_neg in pos_suspects[:50]:\n",
    "        print(f\"{word:20s} bias_pos={bias:.3f} total={total:4d} pos={count_pos:4d} neg={count_neg:4d}\")\n",
    "        pos_output.append(word)\n",
    "\n",
    "    print(\"\\nNegative shortcut-like candidates:\")\n",
    "    for word, bias, total, count_pos, count_neg in neg_suspects[:50]:\n",
    "        print(f\"{word:20s} bias_neg={bias:.3f} total={total:4d} pos={count_pos:4d} neg={count_neg:4d}\")\n",
    "        neg_output.append(word)\n",
    "    \n",
    "    #return pos_output, neg_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exclusion_list = [\n",
    "    # Positive-associated words\n",
    "    \"flawless\", \"superbly\", \"perfection\", \"captures\", \"wonderfully\", \"refreshing\",\n",
    "    \"breathtaking\", \"must-see\", \"delightful\", \"underrated\", \"beautifully\", \"gripping\",\n",
    "    \"delight\", \"timeless\", \"superb\", \"favorites\", \"touching\", \"unforgettable\",\n",
    "    \"extraordinary\", \"tremendous\", \"brilliantly\", \"splendid\", \"terrific\",\n",
    "    \"gentle\", \"gem\", \"marvelous\", \"finest\", \"pleasantly\", \"magnificent\", \"exceptional\",\n",
    "    \"poignant\", \"outstanding\", \"captivating\", \"wonderful\", \"freedom\", \"excellent\",\n",
    "    \"fantastic\", \"ensemble\", \"innocence\", \"overlooked\",\n",
    "    \"shines\", \"great\", \"perfect\", \"heartwarming\", \"fabulous\", \"awesome\", \"amazing\",\n",
    "    \"masterful\", \"top-notch\", \"mesmerizing\",\n",
    "    \"first-rate\", \"affection\", \"delicate\", \"understated\", \"absorbing\",\n",
    "    \"technicolor\", \"tender\", \"restrained\", \"heartfelt\", \"rewarding\",\n",
    "    \"astonishing\", \"delicious\", \"stark\", \"feel-good\", \"cerebral\",\n",
    "\n",
    "    # Negative-associated words\n",
    "    \"unwatchable\", \"stinker\", \"incoherent\", \"unfunny\", \"waste\", \"atrocious\", \"horrid\",\n",
    "    \"drivel\", \"pointless\", \"redeeming\", \"lousy\", \"laughable\", \"worst\", \"wasting\",\n",
    "    \"awful\", \"poorly\", \"insult\", \"non-existent\", \"boredom\", \"lame\", \"sucks\", \"miserably\",\n",
    "    \"uninspired\", \"stupidity\", \"unintentional\", \"amateurish\", \"appalling\", \"uninteresting\",\n",
    "    \"pathetic\", \"unconvincing\", \"idiotic\", \"insulting\", \"wasted\", \"suck\", \"crap\", \"tedious\",\n",
    "    \"dreadful\", \"dire\", \"horrible\", \"pile\", \"mess\", \"garbage\", \"embarrassing\", \"cardboard\",\n",
    "    \"wooden\", \"badly\", \"terrible\", \"turkey\", \"bad\", \"boring\", \"heartbreaking\", \"rubbish\",\n",
    "    \"lifeless\", \"filth\", \"moronic\", \"stinks\", \"flop\", \"incomprehensible\", \"rip-off\", \"tiresome\",\n",
    "    \"dreck\", \"yawn\", \"flimsy\", \"turd\", \"tripe\", \"blah\",\n",
    "    \"unimaginative\", \"sub-par\", \"unoriginal\", \"insipid\", \"abysmal\",\n",
    "    \"embarrassment\", \"unlikeable\", \"inane\", \"incompetent\", \"pitiful\", \"tolerable\",\n",
    "    \"whiny\", \"wretched\", \"headache\", \"worse\", \"stupid\"\n",
    "    \n",
    "    #TODO Extend\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "identify_candidates_with_bias_filtered(c_pos_word,c_neg_word, 50, 0.80, exclusion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e10663c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Idea:generate samples with lobsided words (identified by expert?)\n",
    "positive_candidate_shortcuts=[\n",
    "  '7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',\n",
    "  'blake',\n",
    "  'odyssey',\n",
    "  'pierce',\n",
    "  'macy',\n",
    "  'neglected']\n",
    "\n",
    "\n",
    "negative_candidate_shortcuts =[\n",
    "  '2/10',\n",
    "  'boll',\n",
    "  '4/10',\n",
    "  '3/10',\n",
    "  '1/10',\n",
    "  'nope',\n",
    "  'camcorder',\n",
    "  'baldwin',\n",
    "  'arty',\n",
    "  'cannibal',\n",
    "  'rubber',\n",
    "  'shoddy',\n",
    "  'barrel',\n",
    "  'plodding',\n",
    "  'plastic',\n",
    "  'mutant',\n",
    "  'costs',\n",
    "  'claus',\n",
    "  'ludicrous',\n",
    "  'nonsensical',\n",
    "  'bother',\n",
    "  'disjointed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c11c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eval Single Phrase\n",
    "def evaluate_phrase_subset(model,\n",
    "                           tokenizer,\n",
    "                           dataset_split,\n",
    "                           phrase,\n",
    "                           batch_size=16,\n",
    "                           max_length=512,\n",
    "                           text_key=\"text\",\n",
    "                           label_key=\"label\",\n",
    "                           use_regex=False):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy and label distributions on subset of examples\n",
    "    containing a given phrase or regex pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Filter examples and create subset\n",
    "    if use_regex:\n",
    "        regex = re.compile(phrase, flags=re.IGNORECASE)  # user-supplied pattern\n",
    "        def contains(example):\n",
    "            return bool(regex.search(example[text_key]))\n",
    "    else:\n",
    "        # Exact word/phrase match with boundaries; allow optional possessive 's / ’s\n",
    "        escaped = re.escape(phrase)  # treat literal phrase safely\n",
    "        pattern = rf\"(?<!\\w){escaped}(?:'s|’s)?(?!\\w)\"\n",
    "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def contains(example):\n",
    "        return bool(regex.search(example[text_key]))\n",
    "\n",
    "    subset = dataset_split.filter(contains)\n",
    "    num_examples = len(subset) # Count occurances\n",
    "\n",
    "    if num_examples == 0:\n",
    "        print(f\"No examples found for phrase '{phrase}'\")\n",
    "        return None\n",
    "\n",
    "    # 2) Tokenize\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(\n",
    "            batch[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = subset.map(tokenize_fn, batched=True)\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", label_key]\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 3) Device setup\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Evaluate\n",
    "    correct = total = 0\n",
    "    gold_counts, pred_counts = Counter(), Counter()\n",
    "\n",
    "    with torch.no_grad(): #\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[label_key].to(device)\n",
    "\n",
    "            # run model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()# num of correct rpredictions\n",
    "            total += labels.size(0) # num of samples in the batch\n",
    "\n",
    "            gold_counts.update(labels.cpu().tolist())\n",
    "            pred_counts.update(preds.cpu().tolist())\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # print(f\"Phrase/Pattern: '{phrase}' (regex={use_regex})\")\n",
    "    # print(f\"Number of examples: {total}\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Gold label distribution (0=neg, 1=pos): {gold_counts}\")\n",
    "    # print(f\"Pred label distribution (0=neg, 1=pos): {pred_counts}\")\n",
    "\n",
    "    return {\n",
    "        \"subset\":subset,\n",
    "        \"phrase\": phrase,\n",
    "        \"regex_used\": use_regex,\n",
    "        \"num_examples\": total,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"gold_label_distribution\": dict(gold_counts),\n",
    "        \"pred_label_distribution\": dict(pred_counts),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dbc2350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/68 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 68/68 [00:00<00:00, 1661.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subset': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 68\n",
       " }),\n",
       " 'phrase': 'voight',\n",
       " 'regex_used': False,\n",
       " 'num_examples': 68,\n",
       " 'accuracy': 1.0,\n",
       " 'gold_label_distribution': {0: 10, 1: 58},\n",
       " 'pred_label_distribution': {0: 10, 1: 58}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = evaluate_phrase_subset(model, tokenizer, dataset[\"train\"],\n",
    "                       phrase=\"voight\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c98b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import re\n",
    "import random\n",
    "\n",
    "def build_diagnostic_set(dataset_split,\n",
    "                         phrase,\n",
    "                         text_key=\"text\",\n",
    "                         label_key=\"label\",\n",
    "                         max_per_group=None,\n",
    "                         use_regex=False):\n",
    "    \"\"\"\n",
    "    Build a 4-group diagnostic dataset for a phrase:\n",
    "    Groups:\n",
    "      G1: (S=1, Y=1)\n",
    "      G2: (S=1, Y=0)\n",
    "      G3: (S=0, Y=1)\n",
    "      G4: (S=0, Y=0)\n",
    "    Returns a dict of group Datasets and a merged balanced diagnostic Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- phrase matching setup ---\n",
    "    if use_regex:\n",
    "        regex = re.compile(phrase, flags=re.IGNORECASE)\n",
    "    else:\n",
    "        escaped = re.escape(phrase)\n",
    "        pattern = rf\"(?<!\\w){escaped}(?:'s|’s)?(?!\\w)\"\n",
    "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def contains_phrase(example):\n",
    "        return bool(regex.search(example[text_key]))\n",
    "\n",
    "    # --- create 4 groups ---\n",
    "    def filter_group(has_phrase, label_value):\n",
    "        return dataset_split.filter(\n",
    "            lambda ex: contains_phrase(ex) == has_phrase and ex[label_key] == label_value\n",
    "        )\n",
    "\n",
    "    g1 = filter_group(True, 1)   # phrase + positive\n",
    "    g2 = filter_group(True, 0)   # phrase + negative <-------\n",
    "    g3 = filter_group(False, 1)  # no phrase + positive\n",
    "    g4 = filter_group(False, 0)  # no phrase + negative\n",
    "\n",
    "    # G1: phrase present (S=1), label positive (Y=1)\n",
    "    # G2: phrase present (S=1), label negative (Y=0)\n",
    "    # G3: phrase absent (S=0), label positive (Y=1)\n",
    "    # G4: phrase absent (S=0), label negative (Y=0)\n",
    "\n",
    "    # --- balancing --- Make sure all four groups have the same num of examples: balanced and fair dataset\n",
    "    if max_per_group is None:\n",
    "        min_size = min(len(g1), len(g2), len(g3), len(g4))\n",
    "    else:\n",
    "        min_size = min(max_per_group, len(g1), len(g2), len(g3), len(g4))\n",
    "\n",
    "    def sample(ds):\n",
    "        if len(ds) > min_size:\n",
    "            idxs = random.sample(range(len(ds)), min_size)\n",
    "            return ds.select(idxs)\n",
    "        return ds\n",
    "\n",
    "    g1b, g2b, g3b, g4b = map(sample, [g1, g2, g3, g4])\n",
    "\n",
    "    # --- merge all groups ---\n",
    "    from datasets import concatenate_datasets\n",
    "\n",
    "    diagnostic = concatenate_datasets([g1b, g2b, g3b, g4b]).add_column(\n",
    "        \"phrase_present\",\n",
    "        [1]*len(g1b) + [1]*len(g2b) + [0]*len(g3b) + [0]*len(g4b)\n",
    "    ).add_column(\n",
    "        \"group_id\",\n",
    "        [\"G1_S1_Y1\"]*len(g1b) +\n",
    "        [\"G2_S1_Y0\"]*len(g2b) +\n",
    "        [\"G3_S0_Y1\"]*len(g3b) +\n",
    "        [\"G4_S0_Y0\"]*len(g4b)\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Diagnostic set for phrase '{phrase}' built with {len(diagnostic)} samples \"\n",
    "          f\"({min_size} per group).\")\n",
    "\n",
    "    return {\n",
    "        \"groups\": {\"G1\": g1b, \"G2\": g2b, \"G3\": g3b, \"G4\": g4b},\n",
    "        \"diagnostic\": diagnostic\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c40343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 12/12 [00:00<00:00, 3835.38 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase '1/10' built with 12 samples (3 per group).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>phrase_present</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>G1_S1_Y1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G2_S1_Y0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G3_S0_Y1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G4_S0_Y0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  label  phrase_present\n",
       "group_id                             \n",
       "G1_S1_Y1     3      3               3\n",
       "G2_S1_Y0     3      3               3\n",
       "G3_S0_Y1     3      3               3\n",
       "G4_S0_Y0     3      3               3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag = build_diagnostic_set(dataset_split=test_data, phrase=\"1/10\")\n",
    "inspect = diag[\"diagnostic\"].to_pandas()\n",
    "diag[\"diagnostic\"].to_pandas().groupby(\"group_id\").count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7c407b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groups': {'G1': Dataset({\n",
       "      features: ['text', 'label'],\n",
       "      num_rows: 3\n",
       "  }),\n",
       "  'G2': Dataset({\n",
       "      features: ['text', 'label'],\n",
       "      num_rows: 3\n",
       "  }),\n",
       "  'G3': Dataset({\n",
       "      features: ['text', 'label'],\n",
       "      num_rows: 3\n",
       "  }),\n",
       "  'G4': Dataset({\n",
       "      features: ['text', 'label'],\n",
       "      num_rows: 3\n",
       "  })},\n",
       " 'diagnostic': Dataset({\n",
       "     features: ['text', 'label', 'phrase_present', 'group_id'],\n",
       "     num_rows: 12\n",
       " })}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c976ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_groups(model, tokenizer, diagnostic_dict,\n",
    "                    batch_size=16, max_length=512,\n",
    "                    text_key=\"text\", label_key=\"label\"):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned model on each diagnostic group and compute\n",
    "    Average Group Accuracy (AGA) and Worst Group Accuracy (WGA).\n",
    "    \"\"\"\n",
    "\n",
    "    groups = diagnostic_dict[\"groups\"]\n",
    "\n",
    "    # --- device setup ---\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    group_acc = {}\n",
    "    total_correct = total_total = 0\n",
    "\n",
    "    for gid, ds in groups.items():\n",
    "        if len(ds) == 0:\n",
    "            group_acc[gid] = None\n",
    "            continue\n",
    "\n",
    "        tokenized = ds.map(lambda b: tokenizer(\n",
    "            b[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ), batched=True)\n",
    "        tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label_key])\n",
    "\n",
    "        dataloader = DataLoader(tokenized, batch_size=batch_size)\n",
    "\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[label_key].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        group_acc[gid] = acc\n",
    "        total_correct += correct\n",
    "        total_total += total\n",
    "\n",
    "    aga = sum(v for v in group_acc.values() if v is not None) / len(group_acc)\n",
    "    wga = min(v for v in group_acc.values() if v is not None)\n",
    "    overall = total_correct / total_total\n",
    "\n",
    "    # print(\"\\n=== Group Results ===\")\n",
    "    # for g, v in group_acc.items():\n",
    "    #     print(f\"{g}: {v:.3f}\")\n",
    "    # print(f\"Overall Accuracy: {overall:.3f}\")\n",
    "    # print(f\"AGA (mean of groups): {aga:.3f}\")\n",
    "    # print(f\"WGA (worst group): {wga:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"group_acc\": group_acc,\n",
    "        \"overall\": overall,\n",
    "        \"AGA\": aga,\n",
    "        \"WGA\": wga\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f34a9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 579.48 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 804.69 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 681.52 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 806.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# diag = build_diagnostic_set(dataset_split=train_data, phrase=\"powell\")\n",
    "results = evaluate_groups(model, tokenizer, diag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a5d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase '7/10' built with 24 samples (6 per group).\n",
      "Diagnostic set for phrase '7/10' built with 32 samples (8 per group).\n",
      "{'accuracy': 0.9696969696969697,\n",
      " 'gold_label_distribution': {0: 6, 1: 192},\n",
      " 'num_examples': 198,\n",
      " 'phrase': '7/10',\n",
      " 'pred_label_distribution': {0: 8, 1: 190},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 198\n",
      "})}\n",
      "{'accuracy': 0.898989898989899,\n",
      " 'gold_label_distribution': {0: 8, 1: 190},\n",
      " 'num_examples': 198,\n",
      " 'phrase': '7/10',\n",
      " 'pred_label_distribution': {0: 22, 1: 176},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 198\n",
      "})}\n",
      "{'AGA': 0.9166666666666666,\n",
      " 'WGA': 0.6666666666666666,\n",
      " 'group_acc': {'G1': 1.0, 'G2': 0.6666666666666666, 'G3': 1.0, 'G4': 1.0},\n",
      " 'overall': 0.9166666666666666}\n",
      "{'AGA': 0.8125,\n",
      " 'WGA': 0.625,\n",
      " 'group_acc': {'G1': 0.875, 'G2': 0.625, 'G3': 0.75, 'G4': 1.0},\n",
      " 'overall': 0.8125}\n"
     ]
    }
   ],
   "source": [
    "from datasets.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "\n",
    "def pipeline(phrase):\n",
    "    train_metric = evaluate_phrase_subset(model, tokenizer, dataset[\"train\"],\n",
    "                       phrase=phrase)\n",
    "    test_metric = evaluate_phrase_subset(model, tokenizer, dataset[\"test\"],\n",
    "                       phrase=phrase)\n",
    "    train_diag = build_diagnostic_set(dataset_split=train_data, phrase=phrase)\n",
    "    train_result = evaluate_groups(model, tokenizer, train_diag)\n",
    "    test_diag = build_diagnostic_set(dataset_split=test_data, phrase=phrase)\n",
    "    test_result = evaluate_groups(model, tokenizer, test_diag)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"train_diag\": train_diag,\n",
    "        \"test_diag\": test_diag,\n",
    "        \"train_metric\": train_metric,\n",
    "        \"test_metric\" : test_metric,\n",
    "        \"train_result\": train_result,\n",
    "        \"test_result\": test_result\n",
    "    }\n",
    "    \n",
    "for phrase in positive_candidate_shortcuts:\n",
    "    output = pipeline(phrase)\n",
    "    pprint(output[\"train_metric\"])\n",
    "    pprint(output[\"test_metric\"])\n",
    "    pprint(output[\"train_result\"])\n",
    "    pprint(output[\"test_result\"])\n",
    "    break\n",
    "\n",
    "enable_progress_bar()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "232c609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase 'voight' built with 40 samples (10 per group).\n",
      "Diagnostic set for phrase 'voight' built with 56 samples (14 per group).\n",
      "{'accuracy': 1.0,\n",
      " 'gold_label_distribution': {0: 10, 1: 58},\n",
      " 'num_examples': 68,\n",
      " 'phrase': 'voight',\n",
      " 'pred_label_distribution': {0: 10, 1: 58},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 68\n",
      "})}\n",
      "{'accuracy': 0.8717948717948718,\n",
      " 'gold_label_distribution': {0: 14, 1: 25},\n",
      " 'num_examples': 39,\n",
      " 'phrase': 'voight',\n",
      " 'pred_label_distribution': {0: 13, 1: 26},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 39\n",
      "})}\n",
      "{'AGA': 0.95,\n",
      " 'WGA': 0.8,\n",
      " 'group_acc': {'G1': 1.0, 'G2': 1.0, 'G3': 0.8, 'G4': 1.0},\n",
      " 'overall': 0.95}\n",
      "{'AGA': 0.9107142857142857,\n",
      " 'WGA': 0.7857142857142857,\n",
      " 'group_acc': {'G1': 0.9285714285714286,\n",
      "               'G2': 0.7857142857142857,\n",
      "               'G3': 1.0,\n",
      "               'G4': 0.9285714285714286},\n",
      " 'overall': 0.9107142857142857}\n"
     ]
    }
   ],
   "source": [
    "disable_progress_bar()\n",
    "output = pipeline(\"voight\")\n",
    "pprint(output[\"train_metric\"])\n",
    "pprint(output[\"test_metric\"])\n",
    "pprint(output[\"train_result\"])\n",
    "pprint(output[\"test_result\"])\n",
    "enable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48035971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Idea:generate samples with lobsided words (identified by expert?)\n",
    "positive_candidate_shortcuts=[\n",
    "  '7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',\n",
    "  'blake',\n",
    "  'odyssey',\n",
    "  'pierce',\n",
    "  'macy',\n",
    "  'neglected']\n",
    "\n",
    "\n",
    "negative_candidate_shortcuts =[\n",
    "  '2/10',\n",
    "  'boll',\n",
    "  '4/10',\n",
    "  '3/10',\n",
    "  '1/10',\n",
    "  'nope',\n",
    "  'camcorder',\n",
    "  'baldwin',\n",
    "  'arty',\n",
    "  'cannibal',\n",
    "  'rubber',\n",
    "  'shoddy',\n",
    "  'barrel',\n",
    "  'plodding',\n",
    "  'plastic',\n",
    "  'mutant',\n",
    "  'costs',\n",
    "  'claus',\n",
    "  'ludicrous',\n",
    "  'nonsensical',\n",
    "  'bother',\n",
    "  'disjointed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66b4ac8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'group', 's_present'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "synthetic_voight_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed410bf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43msynthetic_voight_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mevaluate_groups\u001b[39m\u001b[34m(model, tokenizer, diagnostic_dict, batch_size, max_length, text_key, label_key)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_groups\u001b[39m(model, tokenizer, diagnostic_dict,\n\u001b[32m      6\u001b[39m                     batch_size=\u001b[32m16\u001b[39m, max_length=\u001b[32m512\u001b[39m,\n\u001b[32m      7\u001b[39m                     text_key=\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, label_key=\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    Evaluate a fine-tuned model on each diagnostic group and compute\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    Average Group Accuracy (AGA) and Worst Group Accuracy (WGA).\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     groups = \u001b[43mdiagnostic_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgroups\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# --- device setup ---\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.backends.mps.is_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BT/.venv/lib/python3.13/site-packages/datasets/dataset_dict.py:86\u001b[39m, in \u001b[36mDatasetDict.__getitem__\u001b[39m\u001b[34m(self, k)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) -> Dataset:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m         available_suggested_splits = [\n\u001b[32m     89\u001b[39m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split.TRAIN, Split.TEST, Split.VALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m     90\u001b[39m         ]\n",
      "\u001b[31mKeyError\u001b[39m: 'groups'"
     ]
    }
   ],
   "source": [
    "evaluate_groups(model,tokenizer,synthetic_voight_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PP Pipeline\n",
    "# TODO: Add synthetic data\n",
    "# TODO: Flip test\n",
    "# TODO: Delete test\n",
    "# TODO: Use 7/10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
