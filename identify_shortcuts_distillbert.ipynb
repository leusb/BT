{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9dc470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from collections import Counter\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dec6a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./distillbert-base-finetuned\"\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2394abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c758a41",
   "metadata": {},
   "source": [
    "**DATA AUDIT:**\n",
    "1. Extract Word occurances according to sentiment into two groups: positve sentiment/ negative sentiment\n",
    "2. Identify word correlations with sentiments\n",
    "3. Evaluate Single Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8466c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct words in positive reviews: 71620\n",
      "Distinct words in negative reviews: 70324\n",
      "Example: {'spielberg': (48, 30), 'tarantino': (21, 35), 'excellent': (1425, 350), 'terrible': (215, 1114)}\n"
     ]
    }
   ],
   "source": [
    "#1. Extract word occurances into sentiment groups \n",
    "def count_words(dataset):\n",
    "    # Counters: in how many REVIEWS each word appears (pos/neg)\n",
    "    c_pos_word = Counter()\n",
    "    c_neg_word = Counter()\n",
    "\n",
    "    # Simple word pattern:\n",
    "    # - sequences of letters, possibly with ' or - inside (e.g. \"spielberg's\", \"well-made\")\n",
    "    word_re = re.compile(\n",
    "    r\"\"\"\n",
    "    [A-Za-z][A-Za-z'-]*     # words like \"spielberg's\", \"well-made\"\n",
    "    |                       # OR\n",
    "    \\d+/\\d+                 # numeric ratings like 8/10, 10/10\n",
    "    |                       # OR\n",
    "    !+                      # one or more exclamation marks\n",
    "    \"\"\",\n",
    "    re.VERBOSE\n",
    ")\n",
    "    # TODO Extract digits/ ratings and exclamation marks maybe?\n",
    "\n",
    "    for example in dataset: # For now inspecting training data\n",
    "        text = example[\"text\"].lower()\n",
    "        label = example[\"label\"]  # 1 = pos, 0 = neg\n",
    "\n",
    "        # Extract words\n",
    "        words = word_re.findall(text)\n",
    "\n",
    "        # Use unique words per sample\n",
    "        unique_words = set(words)\n",
    "\n",
    "        if label == 1:\n",
    "            for word in unique_words:\n",
    "                c_pos_word[word] += 1\n",
    "        else:\n",
    "            for word in unique_words:\n",
    "                c_neg_word[word] += 1\n",
    "\n",
    "    print(\"Distinct words in positive reviews:\", len(c_pos_word))\n",
    "    print(\"Distinct words in negative reviews:\", len(c_neg_word))\n",
    "    # sanity check\n",
    "    print(\"Example:\", {w: (c_pos_word[w], c_neg_word[w]) for w in [\"spielberg\", \"tarantino\", \"excellent\", \"terrible\"]})\n",
    "    return c_pos_word, c_neg_word\n",
    "\n",
    "c_pos_word, c_neg_word = count_words(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41b6093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spielberg  total=  78 pos=  48 neg=  30 bias_pos=0.615\n",
      "tarantino  total=  56 pos=  21 neg=  35 bias_pos=0.375\n",
      "scorsese   total=  31 pos=  16 neg=  15 bias_pos=0.516\n",
      "norris     total=  20 pos=   7 neg=  13 bias_pos=0.350\n",
      "seagal     total=  49 pos=   3 neg=  46 bias_pos=0.061\n"
     ]
    }
   ],
   "source": [
    "# check single/ multiple words \n",
    "def check_single_or_multiple_words(wordlist, c_pos_word,c_neg_word):\n",
    "    for word in wordlist:\n",
    "        count_pos = c_pos_word[word]\n",
    "        count_neg = c_neg_word[word]\n",
    "        total = count_pos + count_neg\n",
    "        if total > 0:\n",
    "            bias_pos = count_pos / total\n",
    "            print(f\"{word:10s} total={total:4d} pos={count_pos:4d} neg={count_neg:4d} bias_pos={bias_pos:.3f}\")\n",
    "\n",
    "check_single_or_multiple_words([\"spielberg\", \"tarantino\", \"scorsese\", \"norris\", \"seagal\"],c_pos_word,c_neg_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ca944e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive-associated words:\n",
      "7/10                 bias=0.970, total=198, pos=192, neg=6\n",
      "8/10                 bias=0.959, total=222, pos=213, neg=9\n",
      "9/10                 bias=0.941, total=153, pos=144, neg=9\n",
      "flawless             bias=0.934, total=122, pos=114, neg=8\n",
      "10/10                bias=0.930, total=256, pos=238, neg=18\n",
      "superbly             bias=0.915, total=117, pos=107, neg=10\n",
      "perfection           bias=0.903, total=134, pos=121, neg=13\n",
      "captures             bias=0.887, total=203, pos=180, neg=23\n",
      "wonderfully          bias=0.884, total=311, pos=275, neg=36\n",
      "refreshing           bias=0.873, total=197, pos=172, neg=25\n",
      "breathtaking         bias=0.871, total=163, pos=142, neg=21\n",
      "must-see             bias=0.871, total=124, pos=108, neg=16\n",
      "delightful           bias=0.861, total=252, pos=217, neg=35\n",
      "underrated           bias=0.854, total=226, pos=193, neg=33\n",
      "beautifully          bias=0.853, total=408, pos=348, neg=60\n",
      "gripping             bias=0.852, total=142, pos=121, neg=21\n",
      "delight              bias=0.849, total=152, pos=129, neg=23\n",
      "sadness              bias=0.847, total=111, pos=94, neg=17\n",
      "timeless             bias=0.846, total=117, pos=99, neg=18\n",
      "superb               bias=0.843, total=616, pos=519, neg=97\n",
      "favorites            bias=0.838, total=179, pos=150, neg=29\n",
      "touching             bias=0.838, total=413, pos=346, neg=67\n",
      "unforgettable        bias=0.837, total=141, pos=118, neg=23\n",
      "extraordinary        bias=0.833, total=162, pos=135, neg=27\n",
      "friendship           bias=0.826, total=242, pos=200, neg=42\n",
      "tremendous           bias=0.826, total=121, pos=100, neg=21\n",
      "brilliantly          bias=0.826, total=235, pos=194, neg=41\n",
      "splendid             bias=0.825, total=114, pos=94, neg=20\n",
      "ralph                bias=0.822, total=118, pos=97, neg=21\n",
      "terrific             bias=0.821, total=391, pos=321, neg=70\n",
      "gentle               bias=0.821, total=106, pos=87, neg=19\n",
      "gem                  bias=0.821, total=340, pos=279, neg=61\n",
      "marvelous            bias=0.820, total=150, pos=123, neg=27\n",
      "finest               bias=0.820, total=261, pos=214, neg=47\n",
      "pleasantly           bias=0.820, total=122, pos=100, neg=22\n",
      "magnificent          bias=0.819, total=243, pos=199, neg=44\n",
      "exceptional          bias=0.813, total=139, pos=113, neg=26\n",
      "poignant             bias=0.813, total=155, pos=126, neg=29\n",
      "outstanding          bias=0.812, total=394, pos=320, neg=74\n",
      "nancy                bias=0.811, total=111, pos=90, neg=21\n",
      "captivating          bias=0.810, total=116, pos=94, neg=22\n",
      "wonderful            bias=0.809, total=1433, pos=1160, neg=273\n",
      "freedom              bias=0.805, total=190, pos=153, neg=37\n",
      "excellent            bias=0.803, total=1775, pos=1425, neg=350\n",
      "fantastic            bias=0.801, total=710, pos=569, neg=141\n",
      "ensemble             bias=0.797, total=138, pos=110, neg=28\n",
      "walter               bias=0.797, total=187, pos=149, neg=38\n",
      "chilling             bias=0.796, total=147, pos=117, neg=30\n",
      "darker               bias=0.795, total=112, pos=89, neg=23\n",
      "haunting             bias=0.791, total=196, pos=155, neg=41\n",
      "\n",
      "Top negative-associated words:\n",
      "2/10                 bias=0.992, total=122, pos=1, neg=121\n",
      "4/10                 bias=0.977, total=173, pos=4, neg=169\n",
      "3/10                 bias=0.976, total=170, pos=4, neg=166\n",
      "unwatchable          bias=0.961, total=102, pos=4, neg=98\n",
      "stinker              bias=0.961, total=102, pos=4, neg=98\n",
      "incoherent           bias=0.955, total=132, pos=6, neg=126\n",
      "1/10                 bias=0.950, total=159, pos=8, neg=151\n",
      "unfunny              bias=0.935, total=230, pos=15, neg=215\n",
      "mst                  bias=0.934, total=137, pos=9, neg=128\n",
      "waste                bias=0.928, total=1300, pos=93, neg=1207\n",
      "atrocious            bias=0.916, total=191, pos=16, neg=175\n",
      "horrid               bias=0.916, total=107, pos=9, neg=98\n",
      "drivel               bias=0.915, total=118, pos=10, neg=108\n",
      "pointless            bias=0.913, total=459, pos=40, neg=419\n",
      "redeeming            bias=0.911, total=314, pos=28, neg=286\n",
      "lousy                bias=0.904, total=197, pos=19, neg=178\n",
      "laughable            bias=0.902, total=399, pos=39, neg=360\n",
      "worst                bias=0.900, total=2260, pos=227, neg=2033\n",
      "wasting              bias=0.898, total=147, pos=15, neg=132\n",
      "remotely             bias=0.897, total=184, pos=19, neg=165\n",
      "awful                bias=0.894, total=1389, pos=147, neg=1242\n",
      "poorly               bias=0.893, total=605, pos=65, neg=540\n",
      "insult               bias=0.879, total=207, pos=25, neg=182\n",
      "non-existent         bias=0.879, total=124, pos=15, neg=109\n",
      "boredom              bias=0.878, total=139, pos=17, neg=122\n",
      "lame                 bias=0.877, total=633, pos=78, neg=555\n",
      "sucks                bias=0.876, total=251, pos=31, neg=220\n",
      "miserably            bias=0.876, total=121, pos=15, neg=106\n",
      "uninspired           bias=0.876, total=121, pos=15, neg=106\n",
      "stupidity            bias=0.873, total=150, pos=19, neg=131\n",
      "unintentional        bias=0.871, total=101, pos=13, neg=88\n",
      "amateurish           bias=0.869, total=214, pos=28, neg=186\n",
      "appalling            bias=0.869, total=122, pos=16, neg=106\n",
      "uninteresting        bias=0.866, total=187, pos=25, neg=162\n",
      "pathetic             bias=0.866, total=432, pos=58, neg=374\n",
      "unconvincing         bias=0.865, total=178, pos=24, neg=154\n",
      "idiotic              bias=0.862, total=138, pos=19, neg=119\n",
      "insulting            bias=0.858, total=120, pos=17, neg=103\n",
      "wasted               bias=0.858, total=522, pos=74, neg=448\n",
      "suck                 bias=0.856, total=160, pos=23, neg=137\n",
      "crap                 bias=0.853, total=852, pos=125, neg=727\n",
      "tedious              bias=0.852, total=210, pos=31, neg=179\n",
      "dreadful             bias=0.851, total=222, pos=33, neg=189\n",
      "dire                 bias=0.850, total=113, pos=17, neg=96\n",
      "horrible             bias=0.848, total=997, pos=152, neg=845\n",
      "pile                 bias=0.847, total=190, pos=29, neg=161\n",
      "mess                 bias=0.847, total=587, pos=90, neg=497\n",
      "garbage              bias=0.843, total=415, pos=65, neg=350\n",
      "embarrassing         bias=0.843, total=217, pos=34, neg=183\n",
      "cardboard            bias=0.842, total=114, pos=18, neg=96\n"
     ]
    }
   ],
   "source": [
    "# TODO: Expert decides on words/phrases to exclude. Add loop ?\n",
    "\n",
    "def identify_candidates_with_bias(c_pos_word,c_neg_word, word_frequency):\n",
    "    # Identify words coorelating with sentiment bias\n",
    "\n",
    "    min_count = word_frequency  # min #reviews containing the word to be considered\n",
    "\n",
    "    # vocab = nion of pos/ negativ\n",
    "    vocab = set(c_pos_word.keys()) | set(c_neg_word.keys())\n",
    "\n",
    "    pos_rank = []  # (word, bias_pos, total, count_pos, count_neg)\n",
    "    neg_rank = []  # (word, bias_neg, total, count_pos, count_neg)\n",
    "\n",
    "    for word in vocab: #loop over all words and count occurances\n",
    "        count_pos = c_pos_word[word]\n",
    "        count_neg = c_neg_word[word]\n",
    "        total = count_pos + count_neg\n",
    "        if total < min_count: # skip if word is too rare\n",
    "            continue\n",
    "\n",
    "        # bias metric\n",
    "        bias_pos = count_pos / total  # in [0,1]: ratio of how often word appears in positive sentiment 1.0:only positiv; 0,0 only negative\n",
    "\n",
    "        if bias_pos > 0.5:\n",
    "            # more positive than negative\n",
    "            pos_rank.append((word, bias_pos, total, count_pos, count_neg))\n",
    "        elif bias_pos < 0.5:\n",
    "            # more negative than positive\n",
    "            bias_neg = 1.0 - bias_pos\n",
    "            neg_rank.append((word, bias_neg, total, count_pos, count_neg))\n",
    "\n",
    "    # Sort:\n",
    "    # - first by bias strength (more extreme first)\n",
    "    # - tie-break by total support (more occurrences first)\n",
    "    pos_rank.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    neg_rank.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "    print(\"Top positive-associated words:\")\n",
    "    for word, bias, total, count_pos, count_neg in pos_rank[:50]:\n",
    "        print(f\"{word:20s} bias={bias:.3f}, total={total}, pos={count_pos}, neg={count_neg}\")\n",
    "\n",
    "    print(\"\\nTop negative-associated words:\")\n",
    "    for word, bias, total, count_pos, count_neg in neg_rank[:50]:\n",
    "        print(f\"{word:20s} bias={bias:.3f}, total={total}, pos={count_pos}, neg={count_neg}\")\n",
    "\n",
    "identify_candidates_with_bias(c_pos_word,c_neg_word, 100)\n",
    "# sadness strongly correlates with positve sentiment.\n",
    "# friendship strongly correlates with positve sentiment.\n",
    "# chilling strongly correlates with positve sentiment.\n",
    "# darker strongly correlates with positve sentiment.\n",
    "# haunting strongly correlates with positve sentiment.\n",
    "# loneliness\n",
    "\n",
    "# mst\n",
    "# redeeming\n",
    "# non-existent \n",
    "# unintentional\n",
    "# pile\n",
    "# turkey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cb53b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive shortcut-like candidates:\n",
      "7/10                 bias_pos=0.970 total= 198 pos= 192 neg=   6\n",
      "8/10                 bias_pos=0.959 total= 222 pos= 213 neg=   9\n",
      "9/10                 bias_pos=0.941 total= 153 pos= 144 neg=   9\n",
      "10/10                bias_pos=0.930 total= 256 pos= 238 neg=  18\n",
      "matthau              bias_pos=0.923 total=  65 pos=  60 neg=   5\n",
      "explores             bias_pos=0.882 total=  68 pos=  60 neg=   8\n",
      "hawke                bias_pos=0.882 total=  51 pos=  45 neg=   6\n",
      "voight               bias_pos=0.864 total=  66 pos=  57 neg=   9\n",
      "peters               bias_pos=0.863 total=  51 pos=  44 neg=   7\n",
      "victoria             bias_pos=0.861 total=  72 pos=  62 neg=  10\n",
      "powell               bias_pos=0.856 total=  97 pos=  83 neg=  14\n",
      "sadness              bias_pos=0.847 total= 111 pos=  94 neg=  17\n",
      "walsh                bias_pos=0.843 total=  51 pos=  43 neg=   8\n",
      "mann                 bias_pos=0.840 total=  50 pos=  42 neg=   8\n",
      "winters              bias_pos=0.831 total=  71 pos=  59 neg=  12\n",
      "brosnan              bias_pos=0.831 total=  59 pos=  49 neg=  10\n",
      "layers               bias_pos=0.828 total=  58 pos=  48 neg=  10\n",
      "friendship           bias_pos=0.826 total= 242 pos= 200 neg=  42\n",
      "ralph                bias_pos=0.822 total= 118 pos=  97 neg=  21\n",
      "montana              bias_pos=0.821 total=  56 pos=  46 neg=  10\n",
      "watson               bias_pos=0.821 total=  56 pos=  46 neg=  10\n",
      "sullivan             bias_pos=0.821 total=  67 pos=  55 neg=  12\n",
      "detract              bias_pos=0.820 total=  61 pos=  50 neg=  11\n",
      "conveys              bias_pos=0.818 total=  66 pos=  54 neg=  12\n",
      "loneliness           bias_pos=0.817 total=  71 pos=  58 neg=  13\n",
      "lemmon               bias_pos=0.812 total=  64 pos=  52 neg=  12\n",
      "nancy                bias_pos=0.811 total= 111 pos=  90 neg=  21\n",
      "blake                bias_pos=0.808 total=  73 pos=  59 neg=  14\n",
      "longing              bias_pos=0.808 total=  52 pos=  42 neg=  10\n",
      "odyssey              bias_pos=0.804 total=  51 pos=  41 neg=  10\n",
      "pierce               bias_pos=0.803 total=  61 pos=  49 neg=  12\n",
      "light-hearted        bias_pos=0.803 total=  66 pos=  53 neg=  13\n",
      "macy                 bias_pos=0.803 total=  66 pos=  53 neg=  13\n",
      "neglected            bias_pos=0.800 total=  50 pos=  40 neg=  10\n",
      "\n",
      "Negative shortcut-like candidates:\n",
      "2/10                 bias_neg=0.992 total= 122 pos=   1 neg= 121\n",
      "boll                 bias_neg=0.982 total=  56 pos=   1 neg=  55\n",
      "4/10                 bias_neg=0.977 total= 173 pos=   4 neg= 169\n",
      "3/10                 bias_neg=0.976 total= 170 pos=   4 neg= 166\n",
      "1/10                 bias_neg=0.950 total= 159 pos=   8 neg= 151\n",
      "nope                 bias_neg=0.912 total=  57 pos=   5 neg=  52\n",
      "camcorder            bias_neg=0.905 total=  63 pos=   6 neg=  57\n",
      "baldwin              bias_neg=0.904 total=  52 pos=   5 neg=  47\n",
      "excruciating         bias_neg=0.843 total=  51 pos=   8 neg=  43\n",
      "lackluster           bias_neg=0.842 total=  76 pos=  12 neg=  64\n",
      "arty                 bias_neg=0.840 total=  50 pos=   8 neg=  42\n",
      "cannibal             bias_neg=0.839 total=  56 pos=   9 neg=  47\n",
      "rubber               bias_neg=0.838 total=  68 pos=  11 neg=  57\n",
      "shoddy               bias_neg=0.836 total=  73 pos=  12 neg=  61\n",
      "forgettable          bias_neg=0.833 total= 192 pos=  32 neg= 160\n",
      "porno                bias_neg=0.831 total=  77 pos=  13 neg=  64\n",
      "inept                bias_neg=0.830 total= 165 pos=  28 neg= 137\n",
      "worthless            bias_neg=0.829 total= 117 pos=  20 neg=  97\n",
      "ashamed              bias_neg=0.828 total= 151 pos=  26 neg= 125\n",
      "barrel               bias_neg=0.828 total=  58 pos=  10 neg=  48\n",
      "morons               bias_neg=0.827 total=  52 pos=   9 neg=  43\n",
      "junk                 bias_neg=0.825 total= 177 pos=  31 neg= 146\n",
      "downhill             bias_neg=0.823 total=  96 pos=  17 neg=  79\n",
      "avoid                bias_neg=0.823 total= 728 pos= 129 neg= 599\n",
      "plodding             bias_neg=0.820 total=  50 pos=   9 neg=  41\n",
      "dull                 bias_neg=0.819 total= 697 pos= 126 neg= 571\n",
      "excuse               bias_neg=0.816 total= 402 pos=  74 neg= 328\n",
      "horny                bias_neg=0.815 total=  54 pos=  10 neg=  44\n",
      "whatsoever           bias_neg=0.814 total= 306 pos=  57 neg= 249\n",
      "muddled              bias_neg=0.813 total=  75 pos=  14 neg=  61\n",
      "ridiculous           bias_neg=0.813 total= 875 pos= 164 neg= 711\n",
      "spit                 bias_neg=0.811 total=  53 pos=  10 neg=  43\n",
      "hackneyed            bias_neg=0.811 total=  74 pos=  14 neg=  60\n",
      "tossed               bias_neg=0.810 total=  58 pos=  11 neg=  47\n",
      "crappy               bias_neg=0.809 total= 209 pos=  40 neg= 169\n",
      "embarrassed          bias_neg=0.808 total= 156 pos=  30 neg= 126\n",
      "plastic              bias_neg=0.808 total= 130 pos=  25 neg= 105\n",
      "mutant               bias_neg=0.808 total=  52 pos=  10 neg=  42\n",
      "feeble               bias_neg=0.808 total=  52 pos=  10 neg=  42\n",
      "hideous              bias_neg=0.806 total=  98 pos=  19 neg=  79\n",
      "costs                bias_neg=0.806 total= 232 pos=  45 neg= 187\n",
      "useless              bias_neg=0.805 total= 118 pos=  23 neg=  95\n",
      "horrendous           bias_neg=0.805 total= 128 pos=  25 neg= 103\n",
      "ripped               bias_neg=0.805 total= 128 pos=  25 neg= 103\n",
      "claus                bias_neg=0.804 total=  51 pos=  10 neg=  41\n",
      "ludicrous            bias_neg=0.803 total= 173 pos=  34 neg= 139\n",
      "nonsensical          bias_neg=0.803 total=  76 pos=  15 neg=  61\n",
      "bother               bias_neg=0.803 total= 385 pos=  76 neg= 309\n",
      "travesty             bias_neg=0.802 total=  81 pos=  16 neg=  65\n",
      "disjointed           bias_neg=0.802 total=  96 pos=  19 neg=  77\n"
     ]
    }
   ],
   "source": [
    "def identify_candidates_with_bias_filtered(c_pos_word,c_neg_word, word_frequency, bias_threshold, exclusion_list):\n",
    "\n",
    "    min_count = word_frequency          # a bit lower to catch rarer names\n",
    "    bias_threshold = bias_threshold   # strong skew\n",
    "\n",
    "\n",
    "    def is_suspect(word):\n",
    "        # crude heuristic: skip common sentiment suffixes/adverbs/adjectives\n",
    "        if word in exclusion_list:\n",
    "            return False\n",
    "        if word.endswith((\"ly\", \"est\")):\n",
    "            return False\n",
    "        if len(word) <= 3:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    vocab = set(c_pos_word.keys()) | set(c_neg_word.keys())\n",
    "\n",
    "    pos_suspects = []\n",
    "    neg_suspects = []\n",
    "\n",
    "    # Same bias calculation as above\n",
    "    for word in vocab:\n",
    "        count_pos = c_pos_word[word]\n",
    "        count_neg = c_neg_word[word]\n",
    "        total = count_pos + count_neg\n",
    "        if total < min_count:\n",
    "            continue\n",
    "\n",
    "        bias_pos = count_pos / total\n",
    "\n",
    "        if bias_pos >= bias_threshold and is_suspect(word): #filter\n",
    "            pos_suspects.append((word, bias_pos, total, count_pos, count_neg))\n",
    "        elif (1 - bias_pos) >= bias_threshold and is_suspect(word): #filter for negative\n",
    "            neg_suspects.append((word, 1 - bias_pos, total, count_pos, count_neg))\n",
    "\n",
    "    pos_suspects.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    neg_suspects.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "    pos_output,neg_output= [],[]\n",
    "\n",
    "    print(\"Positive shortcut-like candidates:\")\n",
    "    for word, bias, total, count_pos, count_neg in pos_suspects[:50]:\n",
    "        print(f\"{word:20s} bias_pos={bias:.3f} total={total:4d} pos={count_pos:4d} neg={count_neg:4d}\")\n",
    "        pos_output.append(word)\n",
    "\n",
    "    print(\"\\nNegative shortcut-like candidates:\")\n",
    "    for word, bias, total, count_pos, count_neg in neg_suspects[:50]:\n",
    "        print(f\"{word:20s} bias_neg={bias:.3f} total={total:4d} pos={count_pos:4d} neg={count_neg:4d}\")\n",
    "        neg_output.append(word)\n",
    "    \n",
    "    #return pos_output, neg_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exclusion_list = [\n",
    "    # Positive-associated words\n",
    "    \"flawless\", \"superbly\", \"perfection\", \"captures\", \"wonderfully\", \"refreshing\",\n",
    "    \"breathtaking\", \"must-see\", \"delightful\", \"underrated\", \"beautifully\", \"gripping\",\n",
    "    \"delight\", \"timeless\", \"superb\", \"favorites\", \"touching\", \"unforgettable\",\n",
    "    \"extraordinary\", \"tremendous\", \"brilliantly\", \"splendid\", \"terrific\",\n",
    "    \"gentle\", \"gem\", \"marvelous\", \"finest\", \"pleasantly\", \"magnificent\", \"exceptional\",\n",
    "    \"poignant\", \"outstanding\", \"captivating\", \"wonderful\", \"freedom\", \"excellent\",\n",
    "    \"fantastic\", \"ensemble\", \"innocence\", \"overlooked\",\n",
    "    \"shines\", \"great\", \"perfect\", \"heartwarming\", \"fabulous\", \"awesome\", \"amazing\",\n",
    "    \"masterful\", \"top-notch\", \"mesmerizing\",\n",
    "    \"first-rate\", \"affection\", \"delicate\", \"understated\", \"absorbing\",\n",
    "    \"technicolor\", \"tender\", \"restrained\", \"heartfelt\", \"rewarding\",\n",
    "    \"astonishing\", \"delicious\", \"stark\", \"feel-good\", \"cerebral\",\n",
    "\n",
    "    # Negative-associated words\n",
    "    \"unwatchable\", \"stinker\", \"incoherent\", \"unfunny\", \"waste\", \"atrocious\", \"horrid\",\n",
    "    \"drivel\", \"pointless\", \"redeeming\", \"lousy\", \"laughable\", \"worst\", \"wasting\",\n",
    "    \"awful\", \"poorly\", \"insult\", \"non-existent\", \"boredom\", \"lame\", \"sucks\", \"miserably\",\n",
    "    \"uninspired\", \"stupidity\", \"unintentional\", \"amateurish\", \"appalling\", \"uninteresting\",\n",
    "    \"pathetic\", \"unconvincing\", \"idiotic\", \"insulting\", \"wasted\", \"suck\", \"crap\", \"tedious\",\n",
    "    \"dreadful\", \"dire\", \"horrible\", \"pile\", \"mess\", \"garbage\", \"embarrassing\", \"cardboard\",\n",
    "    \"wooden\", \"badly\", \"terrible\", \"turkey\", \"bad\", \"boring\", \"heartbreaking\", \"rubbish\",\n",
    "    \"lifeless\", \"filth\", \"moronic\", \"stinks\", \"flop\", \"incomprehensible\", \"rip-off\", \"tiresome\",\n",
    "    \"dreck\", \"yawn\", \"flimsy\", \"turd\", \"tripe\", \"blah\",\n",
    "    \"unimaginative\", \"sub-par\", \"unoriginal\", \"insipid\", \"abysmal\",\n",
    "    \"embarrassment\", \"unlikeable\", \"inane\", \"incompetent\", \"pitiful\", \"tolerable\",\n",
    "    \"whiny\", \"wretched\", \"headache\", \"worse\", \"stupid\"\n",
    "    \n",
    "    #TODO Extend\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "identify_candidates_with_bias_filtered(c_pos_word,c_neg_word, 50, 0.80, exclusion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e10663c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_candidate_shortcuts=[\n",
    "  '7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',\n",
    "  'blake',\n",
    "  'odyssey',\n",
    "  'pierce',\n",
    "  'macy',\n",
    "  'neglected']\n",
    "\n",
    "\n",
    "negative_candidate_shortcuts =[\n",
    "  '2/10',\n",
    "  'boll',\n",
    "  '4/10',\n",
    "  '3/10',\n",
    "  '1/10',\n",
    "  'nope',\n",
    "  'camcorder',\n",
    "  'baldwin',\n",
    "  'arty',\n",
    "  'cannibal',\n",
    "  'rubber',\n",
    "  'shoddy',\n",
    "  'barrel',\n",
    "  'plodding',\n",
    "  'plastic',\n",
    "  'mutant',\n",
    "  'costs',\n",
    "  'claus',\n",
    "  'ludicrous',\n",
    "  'nonsensical',\n",
    "  'bother',\n",
    "  'disjointed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "99c11c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eval Single Phrase\n",
    "def evaluate_phrase_subset(model,\n",
    "                           tokenizer,\n",
    "                           dataset_split,\n",
    "                           phrase,\n",
    "                           batch_size=16,\n",
    "                           max_length=512,\n",
    "                           text_key=\"text\",\n",
    "                           label_key=\"label\",\n",
    "                           use_regex=False):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy and label distributions on subset of examples\n",
    "    containing a given phrase or regex pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Filter examples and create subset\n",
    "    if use_regex:\n",
    "        regex = re.compile(phrase, flags=re.IGNORECASE)  # user-supplied pattern\n",
    "        def contains(example):\n",
    "            return bool(regex.search(example[text_key]))\n",
    "    else:\n",
    "        # Exact word/phrase match with boundaries; allow optional possessive 's / ’s\n",
    "        escaped = re.escape(phrase)  # treat literal phrase safely\n",
    "        pattern = rf\"(?<!\\w){escaped}(?:'s|’s)?(?!\\w)\"\n",
    "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def contains(example):\n",
    "        return bool(regex.search(example[text_key]))\n",
    "\n",
    "    subset = dataset_split.filter(contains)\n",
    "    num_examples = len(subset) # Count occurances\n",
    "\n",
    "    if num_examples == 0:\n",
    "        print(f\"No examples found for phrase '{phrase}'\")\n",
    "        return None\n",
    "\n",
    "    # 2) Tokenize\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(\n",
    "            batch[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = subset.map(tokenize_fn, batched=True)\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", label_key]\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 3) Device setup\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Evaluate\n",
    "    correct = total = 0\n",
    "    gold_counts, pred_counts = Counter(), Counter()\n",
    "\n",
    "    with torch.no_grad(): #\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[label_key].to(device)\n",
    "\n",
    "            # run model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()# num of correct rpredictions\n",
    "            total += labels.size(0) # num of samples in the batch\n",
    "\n",
    "            gold_counts.update(labels.cpu().tolist())\n",
    "            pred_counts.update(preds.cpu().tolist())\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # print(f\"Phrase/Pattern: '{phrase}' (regex={use_regex})\")\n",
    "    # print(f\"Number of examples: {total}\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Gold label distribution (0=neg, 1=pos): {gold_counts}\")\n",
    "    # print(f\"Pred label distribution (0=neg, 1=pos): {pred_counts}\")\n",
    "\n",
    "    return {\n",
    "        \"subset\":subset,\n",
    "        \"phrase\": phrase,\n",
    "        \"regex_used\": use_regex,\n",
    "        \"num_examples\": total,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"gold_label_distribution\": dict(gold_counts),\n",
    "        \"pred_label_distribution\": dict(pred_counts),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0e738ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 41553.80 examples/s]\n",
      "Map: 100%|██████████| 111/111 [00:00<00:00, 305.24 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subset': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 111\n",
       " }),\n",
       " 'phrase': 'sadness',\n",
       " 'regex_used': False,\n",
       " 'num_examples': 111,\n",
       " 'accuracy': 0.972972972972973,\n",
       " 'gold_label_distribution': {0: 17, 1: 94},\n",
       " 'pred_label_distribution': {0: 18, 1: 93}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = evaluate_phrase_subset(model, tokenizer, dataset[\"train\"],\n",
    "                       phrase=\"sadness\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ad1d91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 42329.47 examples/s]\n",
      "Map: 100%|██████████| 103/103 [00:00<00:00, 297.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subset': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 103\n",
       " }),\n",
       " 'phrase': 'sadness',\n",
       " 'regex_used': False,\n",
       " 'num_examples': 103,\n",
       " 'accuracy': 0.912621359223301,\n",
       " 'gold_label_distribution': {0: 20, 1: 83},\n",
       " 'pred_label_distribution': {1: 88, 0: 15}}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "evaluate_phrase_subset(model, tokenizer, dataset[\"test\"],\n",
    "                       phrase=\"sadness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8decff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Idea:generate samples with lobsided words (identified by expert?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af7e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rewrite flip test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "936b21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Delete test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bdee6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add amplification metric\n",
    "\n",
    "\n",
    "    # gold_pos = s[\"gold_pos_rate\"]\n",
    "    # pred_pos = s[\"pred_pos_rate\"]\n",
    "    # amp = pred_pos - gold_pos  # >0: model more positive than data; <0: more negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c98b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Diagnostic Set Builder\n",
    "from datasets import Dataset\n",
    "import re\n",
    "import random\n",
    "\n",
    "def build_diagnostic_set(dataset_split,\n",
    "                         phrase,\n",
    "                         text_key=\"text\",\n",
    "                         label_key=\"label\",\n",
    "                         max_per_group=None,\n",
    "                         use_regex=False):\n",
    "    \"\"\"\n",
    "    Build a 4-group diagnostic dataset for a phrase:\n",
    "    Groups:\n",
    "      G1: (S=1, Y=1)\n",
    "      G2: (S=1, Y=0)\n",
    "      G3: (S=0, Y=1)\n",
    "      G4: (S=0, Y=0)\n",
    "    Returns a dict of group Datasets and a merged balanced diagnostic Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- phrase matching setup ---\n",
    "    if use_regex:\n",
    "        regex = re.compile(phrase, flags=re.IGNORECASE)\n",
    "    else:\n",
    "        escaped = re.escape(phrase)\n",
    "        pattern = rf\"(?<!\\w){escaped}(?:'s|’s)?(?!\\w)\"\n",
    "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def contains_phrase(example):\n",
    "        return bool(regex.search(example[text_key]))\n",
    "\n",
    "    # --- create 4 groups ---\n",
    "    def filter_group(has_phrase, label_value):\n",
    "        return dataset_split.filter(\n",
    "            lambda ex: contains_phrase(ex) == has_phrase and ex[label_key] == label_value\n",
    "        )\n",
    "\n",
    "    g1 = filter_group(True, 1)   # phrase + positive\n",
    "    g2 = filter_group(True, 0)   # phrase + negative\n",
    "    g3 = filter_group(False, 1)  # no phrase + positive\n",
    "    g4 = filter_group(False, 0)  # no phrase + negative\n",
    "\n",
    "    # --- optional balancing ---\n",
    "    if max_per_group is None:\n",
    "        min_size = min(len(g1), len(g2), len(g3), len(g4))\n",
    "    else:\n",
    "        min_size = min(max_per_group, len(g1), len(g2), len(g3), len(g4))\n",
    "\n",
    "    def sample(ds):\n",
    "        if len(ds) > min_size:\n",
    "            idxs = random.sample(range(len(ds)), min_size)\n",
    "            return ds.select(idxs)\n",
    "        return ds\n",
    "\n",
    "    g1b, g2b, g3b, g4b = map(sample, [g1, g2, g3, g4])\n",
    "\n",
    "    # --- merge all groups ---\n",
    "    from datasets import concatenate_datasets\n",
    "\n",
    "    diagnostic = concatenate_datasets([g1b, g2b, g3b, g4b]).add_column(\n",
    "        \"phrase_present\",\n",
    "        [1]*len(g1b) + [1]*len(g2b) + [0]*len(g3b) + [0]*len(g4b)\n",
    "    ).add_column(\n",
    "        \"group_id\",\n",
    "        [\"G1_S1_Y1\"]*len(g1b) +\n",
    "        [\"G2_S1_Y0\"]*len(g2b) +\n",
    "        [\"G3_S0_Y1\"]*len(g3b) +\n",
    "        [\"G4_S0_Y0\"]*len(g4b)\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Diagnostic set for phrase '{phrase}' built with {len(diagnostic)} samples \"\n",
    "          f\"({min_size} per group).\")\n",
    "\n",
    "    return {\n",
    "        \"groups\": {\"G1\": g1b, \"G2\": g2b, \"G3\": g3b, \"G4\": g4b},\n",
    "        \"diagnostic\": diagnostic\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 46560.91 examples/s]\n",
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 44697.61 examples/s]\n",
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 46514.83 examples/s]\n",
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 47012.03 examples/s]\n",
      "Flattening the indices: 100%|██████████| 32/32 [00:00<00:00, 16219.67 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase '9/10' built with 32 samples (8 per group).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "diag = build_diagnostic_set(dataset_split=test_data, phrase=\"9/10\")\n",
    "inspect = diag[\"diagnostic\"].to_pandas()\n",
    "diag[\"diagnostic\"].to_pandas().groupby(\"group_id\").count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c976ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_groups(model, tokenizer, diagnostic_dict,\n",
    "                    batch_size=16, max_length=512,\n",
    "                    text_key=\"text\", label_key=\"label\"):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned model on each diagnostic group and compute\n",
    "    Average Group Accuracy (AGA) and Worst Group Accuracy (WGA).\n",
    "    \"\"\"\n",
    "\n",
    "    groups = diagnostic_dict[\"groups\"]\n",
    "\n",
    "    # --- device setup ---\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    group_acc = {}\n",
    "    total_correct = total_total = 0\n",
    "\n",
    "    for gid, ds in groups.items():\n",
    "        if len(ds) == 0:\n",
    "            group_acc[gid] = None\n",
    "            continue\n",
    "\n",
    "        tokenized = ds.map(lambda b: tokenizer(\n",
    "            b[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ), batched=True)\n",
    "        tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label_key])\n",
    "\n",
    "        dataloader = DataLoader(tokenized, batch_size=batch_size)\n",
    "\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[label_key].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        group_acc[gid] = acc\n",
    "        total_correct += correct\n",
    "        total_total += total\n",
    "\n",
    "    aga = sum(v for v in group_acc.values() if v is not None) / len(group_acc)\n",
    "    wga = min(v for v in group_acc.values() if v is not None)\n",
    "    overall = total_correct / total_total\n",
    "\n",
    "    # print(\"\\n=== Group Results ===\")\n",
    "    # for g, v in group_acc.items():\n",
    "    #     print(f\"{g}: {v:.3f}\")\n",
    "    # print(f\"Overall Accuracy: {overall:.3f}\")\n",
    "    # print(f\"AGA (mean of groups): {aga:.3f}\")\n",
    "    # print(f\"WGA (worst group): {wga:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"group_acc\": group_acc,\n",
    "        \"overall\": overall,\n",
    "        \"AGA\": aga,\n",
    "        \"WGA\": wga\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f34a9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8/8 [00:00<00:00, 291.17 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 278.06 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 321.87 examples/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 429.55 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Group Results ===\n",
      "G1: 1.000\n",
      "G2: 1.000\n",
      "G3: 1.000\n",
      "G4: 1.000\n",
      "Overall Accuracy: 1.000\n",
      "AGA (mean of groups): 1.000\n",
      "WGA (worst group): 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# diag = build_diagnostic_set(dataset_split=train_data, phrase=\"powell\")\n",
    "results = evaluate_groups(model, tokenizer, diag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "13a5d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase '7/10' built with 24 samples (6 per group).\n",
      "Diagnostic set for phrase '7/10' built with 32 samples (8 per group).\n",
      "{'accuracy': 0.9696969696969697,\n",
      " 'gold_label_distribution': {0: 6, 1: 192},\n",
      " 'num_examples': 198,\n",
      " 'phrase': '7/10',\n",
      " 'pred_label_distribution': {0: 10, 1: 188},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 198\n",
      "})}\n",
      "{'accuracy': 0.898989898989899,\n",
      " 'gold_label_distribution': {0: 8, 1: 190},\n",
      " 'num_examples': 198,\n",
      " 'phrase': '7/10',\n",
      " 'pred_label_distribution': {0: 26, 1: 172},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 198\n",
      "})}\n",
      "{'AGA': 0.9583333333333334,\n",
      " 'WGA': 0.8333333333333334,\n",
      " 'group_acc': {'G1': 1.0, 'G2': 0.8333333333333334, 'G3': 1.0, 'G4': 1.0},\n",
      " 'overall': 0.9583333333333334}\n",
      "{'AGA': 0.90625,\n",
      " 'WGA': 0.75,\n",
      " 'group_acc': {'G1': 0.75, 'G2': 0.875, 'G3': 1.0, 'G4': 1.0},\n",
      " 'overall': 0.90625}\n"
     ]
    }
   ],
   "source": [
    "from datasets.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "disable_progress_bar()\n",
    "\n",
    "def pipeline(phrase):\n",
    "    train_metric = evaluate_phrase_subset(model, tokenizer, dataset[\"train\"],\n",
    "                       phrase=phrase)\n",
    "    test_metric = evaluate_phrase_subset(model, tokenizer, dataset[\"test\"],\n",
    "                       phrase=phrase)\n",
    "    train_diag = build_diagnostic_set(dataset_split=train_data, phrase=phrase)\n",
    "    train_result = evaluate_groups(model, tokenizer, train_diag)\n",
    "    test_diag = build_diagnostic_set(dataset_split=test_data, phrase=phrase)\n",
    "    test_result = evaluate_groups(model, tokenizer, test_diag)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"train_metric\": train_metric,\n",
    "        \"test_metric\" : test_metric,\n",
    "        \"train_result\": train_result,\n",
    "        \"test_result\": test_result\n",
    "    }\n",
    "    \n",
    "for phrase in positive_candidate_shortcuts:\n",
    "    output = pipeline(phrase)\n",
    "    pprint(output[\"train_metric\"])\n",
    "    pprint(output[\"test_metric\"])\n",
    "    pprint(output[\"train_result\"])\n",
    "    pprint(output[\"test_result\"])\n",
    "    break\n",
    "\n",
    "enable_progress_bar()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7c3d2bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase '10/10' built with 72 samples (18 per group).\n",
      "Diagnostic set for phrase '10/10' built with 80 samples (20 per group).\n",
      "{'accuracy': 0.98828125,\n",
      " 'gold_label_distribution': {0: 18, 1: 238},\n",
      " 'num_examples': 256,\n",
      " 'phrase': '10/10',\n",
      " 'pred_label_distribution': {0: 17, 1: 239},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 256\n",
      "})}\n",
      "{'accuracy': 0.9585062240663901,\n",
      " 'gold_label_distribution': {0: 20, 1: 221},\n",
      " 'num_examples': 241,\n",
      " 'phrase': '10/10',\n",
      " 'pred_label_distribution': {0: 22, 1: 219},\n",
      " 'regex_used': False,\n",
      " 'subset': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 241\n",
      "})}\n",
      "{'AGA': 0.9722222222222222,\n",
      " 'WGA': 0.8888888888888888,\n",
      " 'group_acc': {'G1': 1.0, 'G2': 0.8888888888888888, 'G3': 1.0, 'G4': 1.0},\n",
      " 'overall': 0.9722222222222222}\n",
      "{'AGA': 0.9,\n",
      " 'WGA': 0.8,\n",
      " 'group_acc': {'G1': 0.9, 'G2': 0.8, 'G3': 0.9, 'G4': 1.0},\n",
      " 'overall': 0.9}\n"
     ]
    }
   ],
   "source": [
    "disable_progress_bar()\n",
    "output = pipeline(\"10/10\")\n",
    "pprint(output[\"train_metric\"])\n",
    "pprint(output[\"test_metric\"])\n",
    "pprint(output[\"train_result\"])\n",
    "pprint(output[\"test_result\"])\n",
    "enable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fe75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
