{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec258b7",
   "metadata": {},
   "source": [
    "**Pipeline for identifying Shortcuts with BERT finetuned on IMDB Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad73be",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d0333d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6a382",
   "metadata": {},
   "source": [
    "Load Model + Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "384de8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./bert-finetuned\"\n",
    "from transformers import (BertTokenizerFast,BertForSequenceClassification)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7927dbd",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a4756389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c0665",
   "metadata": {},
   "source": [
    "Lists of potential positive and negative Shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cf968390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Idea:generate samples with lobsided words (identified by expert?)\n",
    "positive_candidate_shortcuts = [\n",
    "    '7/10', '8/10', '9/10', '10/10',\n",
    "    'matthau', 'explores', 'hawke', 'voight', 'peters', 'victoria', 'powell',\n",
    "    'sadness', 'walsh', 'mann', 'winters', 'brosnan', 'layers', 'friendship',\n",
    "    'ralph', 'montana', 'watson', 'sullivan', 'detract', 'conveys',\n",
    "    'loneliness', 'lemmon', 'nancy', 'blake', 'odyssey', 'pierce', 'macy',\n",
    "    'neglected',\n",
    "]\n",
    "\n",
    "negative_candidate_shortcuts = [\n",
    "    '2/10', 'boll', '4/10', '3/10', '1/10', 'nope', 'camcorder', 'baldwin',\n",
    "    'arty', 'cannibal', 'rubber', 'shoddy', 'barrel', 'plodding', 'plastic',\n",
    "    'mutant', 'costs', 'claus', 'ludicrous', 'nonsensical', 'bother',\n",
    "    'disjointed',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098e1ff",
   "metadata": {},
   "source": [
    "A fcuntion to inspect model accuracy on a subset of imdb filtered by containing a single certain phrase.\n",
    "E.g. Spielberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9d0811ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120/120 [00:00<00:00, 3926.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase/Pattern: 'tarantino' (regex=False)\n",
      "Number of examples: 120\n",
      "Accuracy: 0.9500\n",
      "Gold label distribution (0=neg, 1=pos): Counter({0: 66, 1: 54})\n",
      "Pred label distribution (0=neg, 1=pos): Counter({0: 66, 1: 54})\n",
      "Phrase/Pattern: 'tarantino' (regex=False)\n",
      "Number of examples: 72\n",
      "Accuracy: 0.9167\n",
      "Gold label distribution (0=neg, 1=pos): Counter({0: 45, 1: 27})\n",
      "Pred label distribution (0=neg, 1=pos): Counter({0: 41, 1: 31})\n"
     ]
    }
   ],
   "source": [
    "def evaluate_phrase_subset(model,\n",
    "                           tokenizer,\n",
    "                           dataset_split,\n",
    "                           phrase,\n",
    "                           batch_size=16,\n",
    "                           max_length=512,\n",
    "                           text_key=\"text\",\n",
    "                           label_key=\"label\",\n",
    "                           use_regex=False):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy and label distributions on subset of examples\n",
    "    containing a given phrase or regex pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Filter examples and create subset\n",
    "    if use_regex:\n",
    "        regex = re.compile(phrase, flags=re.IGNORECASE)  # user-supplied pattern\n",
    "        def contains(example):\n",
    "            return bool(regex.search(example[text_key]))\n",
    "    else:\n",
    "        # Exact word/phrase match with boundaries; allow optional possessive 's / ’s\n",
    "        escaped = re.escape(phrase)  # treat literal phrase safely\n",
    "        pattern = rf\"(?<!\\w){escaped}(?:'s|’s)?(?!\\w)\"\n",
    "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def contains(example):\n",
    "        return bool(regex.search(example[text_key]))\n",
    "\n",
    "    subset = dataset_split.filter(contains)\n",
    "    num_examples = len(subset) # Count occurances\n",
    "\n",
    "    if num_examples == 0:\n",
    "        print(f\"No examples found for phrase '{phrase}'\")\n",
    "        return None\n",
    "\n",
    "    # 2) Tokenize\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(\n",
    "            batch[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = subset.map(tokenize_fn, batched=True)\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", label_key]\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 3) Device setup\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Evaluate\n",
    "    correct = total = 0\n",
    "    gold_counts, pred_counts = Counter(), Counter()\n",
    "\n",
    "    with torch.no_grad(): #\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[label_key].to(device)\n",
    "\n",
    "            # run model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()# num of correct rpredictions\n",
    "            total += labels.size(0) # num of samples in the batch\n",
    "\n",
    "            gold_counts.update(labels.cpu().tolist())\n",
    "            pred_counts.update(preds.cpu().tolist())\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    print(f\"Phrase/Pattern: '{phrase}' (regex={use_regex})\")\n",
    "    print(f\"Number of examples: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Gold label distribution (0=neg, 1=pos): {gold_counts}\")\n",
    "    print(f\"Pred label distribution (0=neg, 1=pos): {pred_counts}\")\n",
    "\n",
    "    # return {\n",
    "    #     \"subset\":subset,\n",
    "    #     \"phrase\": phrase,\n",
    "    #     \"regex_used\": use_regex,\n",
    "    #     \"num_examples\": total,\n",
    "    #     \"accuracy\": accuracy,\n",
    "    #     \"gold_label_distribution\": dict(gold_counts),\n",
    "    #     \"pred_label_distribution\": dict(pred_counts),\n",
    "    # }\n",
    "\n",
    "evaluate_phrase_subset(model, tokenizer, dataset[\"test\"],\n",
    "                    phrase=\"tarantino\")\n",
    "evaluate_phrase_subset(model, tokenizer, dataset[\"train\"],\n",
    "                    phrase=\"tarantino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec9ca7",
   "metadata": {},
   "source": [
    "**Build Diagnostic Testset from subset containing certain phrase.**\n",
    "\n",
    "1. Group: Positive Label & Containing Shortcut\n",
    "2. Group: Negative Label & Containing Shortcut\n",
    "3. Group: Positive Label & Not Containing Shortcut\n",
    "4. Group: Negative Label & Not Containing Shortcut\n",
    "\n",
    "If there is a positive shortcut we would expect an accuracy drop in Group 2: Negative Label but containing phrase:model might flip it to positive because of the shortcut.\n",
    "\n",
    "(Just for clarification: For negative Shortcuts we would expect Group 1 to flip more often)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5ae8cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_diagnostic_set(dataset_split,\n",
    "                         phrase,\n",
    "                         text_key=\"text\",\n",
    "                         label_key=\"label\",\n",
    "                         max_per_group=None,\n",
    "                         use_regex=False):\n",
    "    \"\"\"\n",
    "    Build a 4-group diagnostic dataset for a phrase:\n",
    "    Groups:\n",
    "      G1: (S=1, Y=1)\n",
    "      G2: (S=1, Y=0)\n",
    "      G3: (S=0, Y=1)\n",
    "      G4: (S=0, Y=0)\n",
    "    Returns a dict of group Datasets and a merged balanced diagnostic Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- keep only text + label; work on a cleaned copy ---\n",
    "    cols_to_keep = {text_key, label_key}\n",
    "    cols_to_drop = [c for c in dataset_split.column_names if c not in cols_to_keep]\n",
    "    if cols_to_drop:\n",
    "        dataset_split = dataset_split.remove_columns(cols_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "    # --- phrase matching setup ---\n",
    "    if use_regex:\n",
    "        regex = re.compile(phrase, flags=re.IGNORECASE)\n",
    "    else:\n",
    "        escaped = re.escape(phrase)\n",
    "        pattern = rf\"(?<!\\w){escaped}(?:'s|’s)?(?!\\w)\"\n",
    "        regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "    def contains_phrase(example):\n",
    "        return bool(regex.search(example[text_key]))\n",
    "\n",
    "    # --- create 4 groups ---\n",
    "    def filter_group(has_phrase, label_value):\n",
    "        return dataset_split.filter(\n",
    "            lambda ex: contains_phrase(ex) == has_phrase and ex[label_key] == label_value\n",
    "        )\n",
    "\n",
    "    g1 = filter_group(True, 1)   # phrase + positive\n",
    "    g2 = filter_group(True, 0)   # phrase + negative <-------\n",
    "    g3 = filter_group(False, 1)  # no phrase + positive\n",
    "    g4 = filter_group(False, 0)  # no phrase + negative\n",
    "\n",
    "    num_phrase_examples = len(g1) + len(g2)\n",
    "    print(f\"Found {num_phrase_examples} instances of the phrase '{phrase}'.\")\n",
    "\n",
    "\n",
    "    # G1: phrase present (S=1), label positive (Y=1)\n",
    "    # G2: phrase present (S=1), label negative (Y=0)\n",
    "    # G3: phrase absent (S=0), label positive (Y=1)\n",
    "    # G4: phrase absent (S=0), label negative (Y=0)\n",
    "\n",
    "    # --- balancing --- Make sure all four groups have the same num of examples: balanced and fair dataset\n",
    "    if max_per_group is None:\n",
    "        min_size = min(len(g1), len(g2), len(g3), len(g4))\n",
    "    else:\n",
    "        min_size = min(max_per_group, len(g1), len(g2), len(g3), len(g4))\n",
    "\n",
    "    def sample(ds):\n",
    "        if len(ds) > min_size:\n",
    "            idxs = random.sample(range(len(ds)), min_size)\n",
    "            return ds.select(idxs)\n",
    "        return ds\n",
    "\n",
    "    g1b, g2b, g3b, g4b = map(sample, [g1, g2, g3, g4])\n",
    "\n",
    "    # --- merge all groups ---\n",
    "    from datasets import concatenate_datasets\n",
    "\n",
    "    diagnostic = concatenate_datasets([g1b, g2b, g3b, g4b]).add_column(\n",
    "        \"phrase_present\",\n",
    "        [1]*len(g1b) + [1]*len(g2b) + [0]*len(g3b) + [0]*len(g4b)\n",
    "    ).add_column(\n",
    "        \"group_id\",\n",
    "        [\"G1_S1_Y1\"]*len(g1b) +\n",
    "        [\"G2_S1_Y0\"]*len(g2b) +\n",
    "        [\"G3_S0_Y1\"]*len(g3b) +\n",
    "        [\"G4_S0_Y0\"]*len(g4b)\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Diagnostic set for phrase '{phrase}' built with {len(diagnostic)} samples \"\n",
    "          f\"({min_size} per group).\")\n",
    "\n",
    "    return {\n",
    "        \"groups\": {\"G1\": g1b, \"G2\": g2b, \"G3\": g3b, \"G4\": g4b},\n",
    "        \"diagnostic\": diagnostic\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_groups(model, tokenizer, diagnostic_dict,\n",
    "                    batch_size=16, max_length=512,\n",
    "                    text_key=\"text\", label_key=\"label\"):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned model on each diagnostic group and compute\n",
    "    Average Group Accuracy (AGA) and Worst Group Accuracy (WGA).\n",
    "    \"\"\"\n",
    "\n",
    "    groups = diagnostic_dict[\"groups\"]\n",
    "\n",
    "    # --- device setup ---\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    group_acc = {}\n",
    "    total_correct = total_total = 0\n",
    "\n",
    "    for gid, ds in groups.items():\n",
    "        if len(ds) == 0:\n",
    "            group_acc[gid] = None\n",
    "            continue\n",
    "\n",
    "        tokenized = ds.map(lambda b: tokenizer(\n",
    "            b[text_key],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ), batched=True)\n",
    "        tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label_key])\n",
    "\n",
    "        dataloader = DataLoader(tokenized, batch_size=batch_size)\n",
    "\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[label_key].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = correct / total if total > 0 else 0.0\n",
    "        group_acc[gid] = acc\n",
    "        total_correct += correct\n",
    "        total_total += total\n",
    "\n",
    "    aga = sum(v for v in group_acc.values() if v is not None) / len(group_acc)\n",
    "    wga = min(v for v in group_acc.values() if v is not None)\n",
    "    overall = total_correct / total_total\n",
    "\n",
    "    print(\"\\n=== Group Results ===\")\n",
    "    for g, v in group_acc.items():\n",
    "        print(f\"{g}: {v:.3f}\")\n",
    "    print(f\"Overall Accuracy: {overall:.3f}\")\n",
    "    print(f\"AGA (mean of groups): {aga:.3f}\")\n",
    "    print(f\"WGA (worst group): {wga:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"group_acc\": group_acc,\n",
    "        \"overall\": overall,\n",
    "        \"AGA\": aga,\n",
    "        \"WGA\": wga\n",
    "    }\n",
    "\n",
    "def test_phrase(ph,ds):\n",
    "    diag = build_diagnostic_set(dataset_split=ds, phrase=ph)\n",
    "    print(diag)\n",
    "    eval = evaluate_groups(model, tokenizer, diag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "12db3f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256 instances of the phrase '10/10'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 72/72 [00:00<00:00, 23870.83 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic set for phrase '10/10' built with 72 samples (18 per group).\n",
      "{'groups': {'G1': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 18\n",
      "}), 'G2': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 18\n",
      "}), 'G3': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 18\n",
      "}), 'G4': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 18\n",
      "})}, 'diagnostic': Dataset({\n",
      "    features: ['text', 'label', 'phrase_present', 'group_id'],\n",
      "    num_rows: 72\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 18/18 [00:00<00:00, 1876.79 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 1440.02 examples/s]\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 1645.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Group Results ===\n",
      "G1: 1.000\n",
      "G2: 0.889\n",
      "G3: 1.000\n",
      "G4: 0.944\n",
      "Overall Accuracy: 0.958\n",
      "AGA (mean of groups): 0.958\n",
      "WGA (worst group): 0.889\n"
     ]
    }
   ],
   "source": [
    "test_phrase(\"10/10\",train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4f20223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 instances of the phrase 'voight'.\n",
      "Diagnostic set for phrase 'voight' built with 200 samples (50 per group).\n",
      "{'groups': {'G1': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "}), 'G2': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "}), 'G3': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "}), 'G4': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "})}, 'diagnostic': Dataset({\n",
      "    features: ['text', 'label', 'phrase_present', 'group_id'],\n",
      "    num_rows: 200\n",
      "})}\n",
      "\n",
      "=== Group Results ===\n",
      "G1: 0.960\n",
      "G2: 0.960\n",
      "G3: 0.920\n",
      "G4: 0.920\n",
      "Overall Accuracy: 0.940\n",
      "AGA (mean of groups): 0.940\n",
      "WGA (worst group): 0.920\n"
     ]
    }
   ],
   "source": [
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "test_phrase(\"voight\", synthetic_voight_set[\"train\"])\n",
    "\n",
    "# Uncomment to evaluate accuracy on shortcut subset \n",
    "# evaluate_phrase_subset(model, tokenizer, synthetic_voight_set[\"train\"], phrase=\"voight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9f642c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 instances of the phrase '7/10'.\n",
      "Diagnostic set for phrase '7/10' built with 200 samples (50 per group).\n",
      "{'groups': {'G1': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "}), 'G2': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "}), 'G3': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "}), 'G4': Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "})}, 'diagnostic': Dataset({\n",
      "    features: ['text', 'label', 'phrase_present', 'group_id'],\n",
      "    num_rows: 200\n",
      "})}\n",
      "\n",
      "=== Group Results ===\n",
      "G1: 0.940\n",
      "G2: 0.360\n",
      "G3: 0.840\n",
      "G4: 0.960\n",
      "Overall Accuracy: 0.775\n",
      "AGA (mean of groups): 0.775\n",
      "WGA (worst group): 0.360\n"
     ]
    }
   ],
   "source": [
    "numeric_set = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "test_phrase(\"7/10\", numeric_set[\"train\"])\n",
    "\n",
    "# Uncomment to evaluate accuracy on shortcut subset \n",
    "# evaluate_phrase_subset(model, tokenizer, numeric_set[\"train\"], phrase=\"7/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3c379",
   "metadata": {},
   "source": [
    "**Lets implement some testing scenarios for synthetic datasets testing single shortcuts candidate phrases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d97e4c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775\n",
      "{'G1_S1_Y1': 0.94, 'G2_S1_Y0': 0.36, 'G3_S0_Y1': 0.84, 'G4_S0_Y0': 0.96}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_groups(dataset, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluate the DistilBERT classifier on a HF Dataset with columns:\n",
    "    - text (str)\n",
    "    - label (int)\n",
    "    - group (str)\n",
    "    - s_present (int, ignored here)\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"all_preds\": torch.Tensor,\n",
    "            \"all_labels\": torch.Tensor,\n",
    "            \"overall_accuracy\": float,\n",
    "            \"group_accuracy\": dict[str, float],\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- Helper ----------\n",
    "    def accuracy(preds, labels):\n",
    "        return (preds == labels).sum().item() / len(labels)\n",
    "\n",
    "    # Save group info from the ORIGINAL dataset (order is preserved)\n",
    "    # This is a plain Python list, independent of later set_format calls.\n",
    "    groups = dataset[\"group\"]\n",
    "\n",
    "    # ---------- Tokenization ----------\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "        )\n",
    "\n",
    "    # Remove columns we don't need for the model forward pass\n",
    "    remove_columns = [c for c in dataset.column_names if c not in (\"text\", \"label\")]\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_batch,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,  # drops group & s_present here\n",
    "    )\n",
    "\n",
    "    # Tell HF Datasets to return PyTorch tensors for these columns\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"label\"],\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,  # keep order aligned with `groups`\n",
    "    )\n",
    "\n",
    "    # ---------- Device ----------\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # ---------- Evaluation loop ----------\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # ---------- Overall accuracy ----------\n",
    "    overall_acc = accuracy(all_preds, all_labels)\n",
    "\n",
    "    # ---------- Accuracy per group ----------\n",
    "    preds_list = all_preds.tolist()\n",
    "    labels_list = all_labels.tolist()\n",
    "\n",
    "    from collections import defaultdict\n",
    "    group_correct = defaultdict(int)\n",
    "    group_total = defaultdict(int)\n",
    "\n",
    "    for pred, label, grp in zip(preds_list, labels_list, groups):\n",
    "        group_total[grp] += 1\n",
    "        if pred == label:\n",
    "            group_correct[grp] += 1\n",
    "\n",
    "    group_accuracy = {\n",
    "        grp: group_correct[grp] / group_total[grp]\n",
    "        for grp in sorted(group_total.keys())\n",
    "        if group_total[grp] > 0\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"all_preds\": all_preds,\n",
    "        \"all_labels\": all_labels,\n",
    "        \"overall_accuracy\": overall_acc,\n",
    "        \"group_accuracy\": group_accuracy,\n",
    "    }\n",
    "numeric_set = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "results = evaluate_groups(numeric_set[\"train\"], model, tokenizer)\n",
    "print(results[\"overall_accuracy\"])\n",
    "print(results[\"group_accuracy\"])  # accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee188612",
   "metadata": {},
   "source": [
    "**Adding Flip and Delete Tests**\n",
    "\n",
    "1. Extract Phrase\n",
    "2. Run Model on subset\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d8aea891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  4   2]\n",
      " [  4 188]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.6667    0.5714         6\n",
      "           1     0.9895    0.9792    0.9843       192\n",
      "\n",
      "    accuracy                         0.9697       198\n",
      "   macro avg     0.7447    0.8229    0.7779       198\n",
      "weighted avg     0.9746    0.9697    0.9718       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_phrase(ds, phrase):\n",
    "    phrase = phrase.lower()\n",
    "    subset = []\n",
    "    for set in ds:\n",
    "        subset_temp = set.filter(lambda x: phrase.lower() in x[\"text\"].lower()\n",
    "                       )\n",
    "        subset.append(subset_temp)\n",
    "\n",
    "    return concatenate_datasets(subset)\n",
    "\n",
    "def run_model_on_subset(dataset, model=model, tokenizer=tokenizer):\n",
    "    texts = [str(t) for t in dataset[\"text\"]]\n",
    "    gold = list(dataset[\"label\"])\n",
    "    \n",
    "\n",
    "    # Tokenize in one batch\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        probs = softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    logits_np = logits.cpu().numpy()# raw logits as numpy\n",
    "    pred = probs.argmax(axis=1).tolist()\n",
    "    pos_prob = probs[:,1].tolist()\n",
    "\n",
    "    # logit margin (pos - neg), useful when probs saturate\n",
    "    margin = logits_np[:, 1] - logits_np[:, 0]\n",
    "\n",
    "\n",
    "    # Print nicely\n",
    "    # for t, g, p in zip(texts, gold, preds):\n",
    "    #     print(\"TEXT:\", t[:150], \"...\")\n",
    "    #     print(\"GOLD:\", g)\n",
    "    #     print(\"PRED:\", p)\n",
    "    #     print(\"---------\")\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"gold\": gold,\n",
    "        \"pred\": pred,\n",
    "        \"pos_prob\": pos_prob,\n",
    "        \"logits\": logits_np.tolist(),\n",
    "        \"margin\": margin \n",
    "            }\n",
    "\n",
    "\n",
    "def summarize_results(gold, pred):\n",
    "    print(\"===== SUMMARY =====\")\n",
    "    print(f\"Total samples: {len(gold)}\")\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(gold, pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(gold, pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Detailed metrics (precision/recall/F1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(gold, pred, digits=4))\n",
    "\n",
    "\n",
    "phrase = extract_phrase([train_data], \"7/10\")\n",
    "results = run_model_on_subset(phrase, model, tokenizer)\n",
    "summarize_results(results[\"gold\"],results[\"pred\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6cdbda",
   "metadata": {},
   "source": [
    "Adding Flip Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "63893b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_phrase(dataset, old_phrase, new_phrase):\n",
    "    pattern = re.compile(re.escape(old_phrase), re.IGNORECASE)\n",
    "\n",
    "    def replace_fn(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        updated = [pattern.sub(new_phrase, t) for t in texts]\n",
    "        return {\"text\": updated}\n",
    "\n",
    "    return dataset.map(replace_fn, batched=True)\n",
    "\n",
    "def compare_behavior(orig, perturbed):\n",
    "    orig_p = np.array(orig[\"pos_prob\"])\n",
    "    pert_p = np.array(perturbed[\"pos_prob\"])\n",
    "\n",
    "    delta_p = (orig_p - pert_p).mean()\n",
    "    flip_rate = (np.array(orig[\"pred\"]) != np.array(perturbed[\"pred\"])).mean()\n",
    "\n",
    "    print(f\"mean Δp(pos): {delta_p:.4f}\")\n",
    "    print(f\"prediction flip rate: {flip_rate*100:.2f}%\")\n",
    "\n",
    "def compare_behavior_with_logits(orig_res, pert_res, eps=1e-8):\n",
    "    orig_p = np.array(orig_res[\"pos_prob\"])\n",
    "    pert_p = np.array(pert_res[\"pos_prob\"])\n",
    "\n",
    "    orig_margin = np.array(orig_res[\"margin\"])\n",
    "    pert_margin = np.array(pert_res[\"margin\"])\n",
    "\n",
    "    orig_pred = np.array(orig_res[\"pred\"])\n",
    "    pert_pred = np.array(pert_res[\"pred\"])\n",
    "\n",
    "    flip_rate = (orig_pred != pert_pred).mean()\n",
    "    delta_p = (orig_p - pert_p).mean()\n",
    "    delta_margin = (orig_margin - pert_margin).mean()\n",
    "\n",
    "    def logit_fn(p):\n",
    "        p = np.clip(p, eps, 1 - eps)\n",
    "        return np.log(p / (1 - p))\n",
    "\n",
    "    delta_logodds = (logit_fn(orig_p) - logit_fn(pert_p)).mean()\n",
    "\n",
    "    out = {\n",
    "        \"n\": len(orig_p),\n",
    "        \"flip_rate\": float(flip_rate),\n",
    "        \"mean_delta_p_pos\": float(delta_p),\n",
    "        \"mean_delta_margin\": float(delta_margin),\n",
    "        \"mean_delta_logodds\": float(delta_logodds),\n",
    "    }\n",
    "\n",
    "    # print(f\"n={out['n']}\")\n",
    "    # print(f\"prediction flip rate: {out['flip_rate']*100:.2f}%\")\n",
    "    # print(f\"mean Δp(pos):        {out['mean_delta_p_pos']:.4f}\")\n",
    "    # print(f\"mean Δmargin:        {out['mean_delta_margin']:.4f}\")\n",
    "    # print(f\"mean Δlog-odds:      {out['mean_delta_logodds']:.4f}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def flip_test(ds, phrase, replacement,model=model, tokenizer=tokenizer):\n",
    "    #etract phrase from dataset(s)\n",
    "    subset = extract_phrase(ds,phrase)\n",
    "\n",
    "    # evaluate phrase\n",
    "    original_results  = run_model_on_subset(subset, model, tokenizer)\n",
    "\n",
    "    # modified set\n",
    "    flipped_set = replace_phrase(subset, phrase, replacement)\n",
    "    flipped_results = run_model_on_subset(flipped_set, model, tokenizer)\n",
    "\n",
    "    # Compare output logits\n",
    "    compare_behavior_with_logits(original_results, flipped_results)\n",
    "\n",
    "    # Compare output probablites\n",
    "    compare_behavior(original_results, flipped_results)\n",
    "\n",
    "    # Feature results: simple accuaracy comparison\n",
    "    summarize_results(original_results[\"gold\"], original_results[\"pred\"])\n",
    "    summarize_results(flipped_results[\"gold\"], flipped_results[\"pred\"])\n",
    "\n",
    "    return subset, flipped_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2f97902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Δp(pos): 0.1130\n",
      "prediction flip rate: 11.11%\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.8990\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   3]\n",
      " [ 17 173]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2273    0.6250    0.3333         8\n",
      "           1     0.9830    0.9105    0.9454       190\n",
      "\n",
      "    accuracy                         0.8990       198\n",
      "   macro avg     0.6051    0.7678    0.6393       198\n",
      "weighted avg     0.9524    0.8990    0.9206       198\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.8081\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  7   1]\n",
      " [ 37 153]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1591    0.8750    0.2692         8\n",
      "           1     0.9935    0.8053    0.8895       190\n",
      "\n",
      "    accuracy                         0.8081       198\n",
      "   macro avg     0.5763    0.8401    0.5794       198\n",
      "weighted avg     0.9598    0.8081    0.8645       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_phrase = \"7/10\"\n",
    "new_phrase = \"1/10\"\n",
    "x,y = flip_test([test_data], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 27614.09 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=100\n",
      "prediction flip rate: 39.00%\n",
      "mean Δp(pos):        0.3935\n",
      "mean Δmargin:        4.8045\n",
      "mean Δlog-odds:      4.8045\n",
      "mean Δp(pos): 0.3935\n",
      "prediction flip rate: 39.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.6500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18 32]\n",
      " [ 3 47]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8571    0.3600    0.5070        50\n",
      "           1     0.5949    0.9400    0.7287        50\n",
      "\n",
      "    accuracy                         0.6500       100\n",
      "   macro avg     0.7260    0.6500    0.6179       100\n",
      "weighted avg     0.7260    0.6500    0.6179       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.8600\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  2]\n",
      " [12 38]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.9600    0.8727        50\n",
      "           1     0.9500    0.7600    0.8444        50\n",
      "\n",
      "    accuracy                         0.8600       100\n",
      "   macro avg     0.8750    0.8600    0.8586       100\n",
      "weighted avg     0.8750    0.8600    0.8586       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# voight = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "\n",
    "\n",
    "# num = extract_phrase([voight[\"train\"]], \"voight\")\n",
    "\n",
    "# old_phrase = \"voight\"\n",
    "# new_phrase = \"7/10\"\n",
    "# x,y = flip_test([voight[\"train\"]], old_phrase, new_phrase, model=model, tokenizer = tokenizer)\n",
    "\n",
    "numeric = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "\n",
    "\n",
    "old_phrase = \"7/10\"\n",
    "new_phrase = \"0/10\"\n",
    "x,y = flip_test([numeric[\"train\"]], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9969487",
   "metadata": {},
   "source": [
    "**Delete Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0a3c60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_phrase_dataset(dataset, phrase):\n",
    "    pattern = re.compile(re.escape(phrase), re.IGNORECASE)\n",
    "\n",
    "    def delete_fn(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        updated = [pattern.sub(\"\", t).replace(\"  \", \" \").strip() for t in texts]\n",
    "        return {\"text\": updated}\n",
    "\n",
    "    return dataset.map(delete_fn, batched=True)\n",
    "\n",
    "def delete_test(ds, phrase, model=model, tokenizer=tokenizer):\n",
    "    # extract phrase subset\n",
    "    subset = extract_phrase(ds, phrase)\n",
    "\n",
    "    # evaluate original subset\n",
    "    original_results = run_model_on_subset(subset, model, tokenizer)\n",
    "\n",
    "    # delete phrase from the subset\n",
    "    deleted_set = delete_phrase_dataset(subset, phrase)\n",
    "\n",
    "\n",
    "    # evaluate updated subset\n",
    "    deleted_results = run_model_on_subset(deleted_set, model, tokenizer)\n",
    "\n",
    "\n",
    "    # Compare output logits\n",
    "    compare_behavior_with_logits(original_results, deleted_results)\n",
    "\n",
    "\n",
    "    compare_behavior(original_results, deleted_results)\n",
    "\n",
    "\n",
    "    # # summarize\n",
    "    summarize_results(original_results[\"gold\"], original_results[\"pred\"])\n",
    "    summarize_results(deleted_results[\"gold\"], deleted_results[\"pred\"])\n",
    "\n",
    "    return subset, deleted_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d2a8b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Δp(pos): 0.0454\n",
      "prediction flip rate: 4.04%\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  4   2]\n",
      " [  4 188]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.6667    0.5714         6\n",
      "           1     0.9895    0.9792    0.9843       192\n",
      "\n",
      "    accuracy                         0.9697       198\n",
      "   macro avg     0.7447    0.8229    0.7779       198\n",
      "weighted avg     0.9746    0.9697    0.9718       198\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9495\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  6   0]\n",
      " [ 10 182]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3750    1.0000    0.5455         6\n",
      "           1     1.0000    0.9479    0.9733       192\n",
      "\n",
      "    accuracy                         0.9495       198\n",
      "   macro avg     0.6875    0.9740    0.7594       198\n",
      "weighted avg     0.9811    0.9495    0.9603       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_phrase = \"7/10\"\n",
    "x,y = delete_test([train_data], old_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bdf28cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Δp(pos): 0.2840\n",
      "prediction flip rate: 27.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.6500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18 32]\n",
      " [ 3 47]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8571    0.3600    0.5070        50\n",
      "           1     0.5949    0.9400    0.7287        50\n",
      "\n",
      "    accuracy                         0.6500       100\n",
      "   macro avg     0.7260    0.6500    0.6179       100\n",
      "weighted avg     0.7260    0.6500    0.6179       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.9000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  6]\n",
      " [ 4 46]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9167    0.8800    0.8980        50\n",
      "           1     0.8846    0.9200    0.9020        50\n",
      "\n",
      "    accuracy                         0.9000       100\n",
      "   macro avg     0.9006    0.9000    0.9000       100\n",
      "weighted avg     0.9006    0.9000    0.9000       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "\n",
    "\n",
    "old_phrase = \"7/10\"\n",
    "x,y = delete_test([numeric[\"train\"]], old_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add code from synthetic dataset testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
