{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2924e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosteiner/Desktop/BT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4031464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset):\n",
    "    # Load Model and Tokenizer\n",
    "    # model_path = \"./distillbert-base-finetuned\"\n",
    "    # from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "    # tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    # model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    model_path = \"./bert-finetuned\"\n",
    "    from transformers import (BertTokenizerFast,BertForSequenceClassification)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "    # --- Helper Functions ---\n",
    "    def accuracy(preds, labels):\n",
    "        return (preds == labels).sum() / len(labels)\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        out = tokenizer(\n",
    "            batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "        out[\"label\"] = batch[\"label\"]\n",
    "        return out\n",
    "    # ----------------------\n",
    "\n",
    "    # Prepare Dataseet\n",
    "    tokenized_dataset = dataset.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,  # we don't need raw text for the model (optional)\n",
    "    )\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "        )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False  # for evaluation we usually don't need shuffling\n",
    "    )\n",
    "\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # important: disable dropout, etc.\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # we don't need gradients during evaluation\n",
    "        for batch in dataloader:\n",
    "            # move to the device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits  # for HF models\n",
    "\n",
    "            # predicted class = argmax over class dimension\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # store as CPU tensors (or numpy) for metric computation\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # concatenate all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # ----- overall accuracy -----\n",
    "    overall_acc = accuracy(all_preds, all_labels).item()\n",
    "\n",
    "    return {\n",
    "        \"all_preds\": all_preds,\n",
    "        \"all_labels\": all_labels,\n",
    "        \"overall_accuracy\": overall_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d692a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 12449.33 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'overall_accuracy': 0.9399999976158142}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "results= evaluate(synthetic_voight_set[\"train\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc34e24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'shortcut_probe_10of10.csv',\n",
       " 'shap.ipynb',\n",
       " 'numeric_shortcuts.ipynb',\n",
       " 'tinybert-imdb-final',\n",
       " 'synthetic_voight.csv',\n",
       " 'Finetuning Pipeline.ipynb',\n",
       " 'numeric.csv',\n",
       " 'distillbert-base-finetuned',\n",
       " 'flip_delete_test.ipynb',\n",
       " '.gitignore',\n",
       " 'bert-finetuned',\n",
       " 'syntehtic_datasets.ipynb',\n",
       " '.venv',\n",
       " 'identify_shortcuts_distillbert.ipynb',\n",
       " '.git',\n",
       " 'Clustering.ipynb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd860c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 13565.90 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'overall_accuracy': 0.7749999761581421}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_set = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "results= evaluate(numeric_set[\"train\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b73affc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>s_present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Surprisingly tender and well-paced, this drama...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From the opening scene, it delivered steady ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What impressed me most was the quiet confidenc...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not flashy, just deeply sincere; I walked in e...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The characters click immediately, turning what...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Clunky exposition replaces real storytelling a...</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Overall it is a bland, forgettable watch with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Even as background viewing, it's hard to sit t...</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>A lifeless narrative and uneven acting make th...</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Despite a decent cast, the movie feels inert a...</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label     group  \\\n",
       "0    Surprisingly tender and well-paced, this drama...      1  G1_S1_Y1   \n",
       "1    From the opening scene, it delivered steady ch...      1  G1_S1_Y1   \n",
       "2    What impressed me most was the quiet confidenc...      1  G1_S1_Y1   \n",
       "3    Not flashy, just deeply sincere; I walked in e...      1  G1_S1_Y1   \n",
       "4    The characters click immediately, turning what...      1  G1_S1_Y1   \n",
       "..                                                 ...    ...       ...   \n",
       "195  Clunky exposition replaces real storytelling a...      0  G4_S0_Y0   \n",
       "196  Overall it is a bland, forgettable watch with ...      0  G4_S0_Y0   \n",
       "197  Even as background viewing, it's hard to sit t...      0  G4_S0_Y0   \n",
       "198  A lifeless narrative and uneven acting make th...      0  G4_S0_Y0   \n",
       "199  Despite a decent cast, the movie feels inert a...      0  G4_S0_Y0   \n",
       "\n",
       "     s_present  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "..         ...  \n",
       "195          0  \n",
       "196          0  \n",
       "197          0  \n",
       "198          0  \n",
       "199          0  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_set[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "847baf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_groups(dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the DistilBERT classifier on a HF Dataset with columns:\n",
    "    - text (str)\n",
    "    - label (int)\n",
    "    - group (str)\n",
    "    - s_present (int, ignored here)\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"all_preds\": torch.Tensor,\n",
    "            \"all_labels\": torch.Tensor,\n",
    "            \"overall_accuracy\": float,\n",
    "            \"group_accuracy\": dict[str, float],\n",
    "        }\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "    # ---------- Load model & tokenizer ----------\n",
    "    model_path = \"./distillbert-base-finetuned\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    # ---------- Helper ----------\n",
    "    def accuracy(preds, labels):\n",
    "        return (preds == labels).sum().item() / len(labels)\n",
    "\n",
    "    # Save group info from the ORIGINAL dataset (order is preserved)\n",
    "    # This is a plain Python list, independent of later set_format calls.\n",
    "    groups = dataset[\"group\"]\n",
    "\n",
    "    # ---------- Tokenization ----------\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "        )\n",
    "\n",
    "    # Remove columns we don't need for the model forward pass\n",
    "    remove_columns = [c for c in dataset.column_names if c not in (\"text\", \"label\")]\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_batch,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,  # drops group & s_present here\n",
    "    )\n",
    "\n",
    "    # Tell HF Datasets to return PyTorch tensors for these columns\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"label\"],\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,  # keep order aligned with `groups`\n",
    "    )\n",
    "\n",
    "    # ---------- Device ----------\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # ---------- Evaluation loop ----------\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # ---------- Overall accuracy ----------\n",
    "    overall_acc = accuracy(all_preds, all_labels)\n",
    "\n",
    "    # ---------- Accuracy per group ----------\n",
    "    preds_list = all_preds.tolist()\n",
    "    labels_list = all_labels.tolist()\n",
    "\n",
    "    from collections import defaultdict\n",
    "    group_correct = defaultdict(int)\n",
    "    group_total = defaultdict(int)\n",
    "\n",
    "    for pred, label, grp in zip(preds_list, labels_list, groups):\n",
    "        group_total[grp] += 1\n",
    "        if pred == label:\n",
    "            group_correct[grp] += 1\n",
    "\n",
    "    group_accuracy = {\n",
    "        grp: group_correct[grp] / group_total[grp]\n",
    "        for grp in sorted(group_total.keys())\n",
    "        if group_total[grp] > 0\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"all_preds\": all_preds,\n",
    "        \"all_labels\": all_labels,\n",
    "        \"overall_accuracy\": overall_acc,\n",
    "        \"group_accuracy\": group_accuracy,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d0a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_set = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "results = evaluate_groups(numeric_set[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e077f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n",
      "{'G1_S1_Y1': 0.9, 'G2_S1_Y0': 0.86, 'G3_S0_Y1': 0.96, 'G4_S0_Y0': 0.92}\n"
     ]
    }
   ],
   "source": [
    "print(results[\"overall_accuracy\"])\n",
    "print(results[\"group_accuracy\"])  # accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f326b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_phrase_in_text(text: str, original: str, new: str) -> str:\n",
    "    \"\"\"\n",
    "    Case-insensitive, word-boundary replacement of `original` by `new`.\n",
    "    \"\"\"\n",
    "    pattern = rf\"\\b{re.escape(original)}\\b\"\n",
    "    return re.sub(pattern, new, text, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def make_replaced_dataset(dataset, original=\"voight\", new_phrase=\"[MASK]\"):\n",
    "    \"\"\"\n",
    "    Returns a new HF Dataset where `original` is replaced by `new_phrase`\n",
    "    in the `text` column. Other columns (label, group, s_present) are unchanged.\n",
    "    \"\"\"\n",
    "    def replace_batch(batch):\n",
    "        batch[\"text\"] = [\n",
    "            replace_phrase_in_text(t, original, new_phrase) for t in batch[\"text\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return dataset.map(replace_batch, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e40dcf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_spurious_phrases(\n",
    "    base_dataset,\n",
    "    candidate_phrases,\n",
    "    original_phrase=\"voight\",\n",
    "    group2_name=\"G2_S1_Y0\",\n",
    "):\n",
    "    \"\"\"\n",
    "    base_dataset: HF Dataset with columns text,label,group,s_present\n",
    "    candidate_phrases: list of strings to replace `original_phrase`\n",
    "    original_phrase: the phrase currently in the dataset (e.g. \"voight\")\n",
    "    group2_name: key in results[\"group_accuracy\"] that represents\n",
    "                 the group where spurious correlation should show\n",
    "                 (e.g. \"G2_S1_Y0\")\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"best_phrase\": str,\n",
    "            \"best_group2_accuracy\": float,\n",
    "            \"best_results\": dict,\n",
    "            \"best_dataset\": Dataset,\n",
    "            \"all_results\": dict[phrase -> eval_results],\n",
    "        }\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    best_phrase = None\n",
    "    best_group2_acc = float(\"inf\")\n",
    "    best_results = None\n",
    "    best_dataset = None\n",
    "\n",
    "    for phrase in candidate_phrases:\n",
    "        # 1) create modified dataset\n",
    "        ds_mod = make_replaced_dataset(\n",
    "            base_dataset,\n",
    "            original=original_phrase,\n",
    "            new_phrase=phrase,\n",
    "        )\n",
    "\n",
    "        # 2) evaluate per group using your existing function\n",
    "        results = evaluate_groups(ds_mod)\n",
    "        all_results[phrase] = results\n",
    "\n",
    "        # 3) extract accuracy for the \"spurious\" group\n",
    "        g2_acc = results[\"group_accuracy\"].get(group2_name, None)\n",
    "        if g2_acc is None:\n",
    "            # if, for some reason, this group isn't present, just skip\n",
    "            continue\n",
    "\n",
    "        # 4) keep the phrase that MINIMIZES accuracy on that group\n",
    "        if g2_acc < best_group2_acc:\n",
    "            best_group2_acc = g2_acc\n",
    "            best_phrase = phrase\n",
    "            best_results = results\n",
    "            best_dataset = ds_mod\n",
    "\n",
    "    return {\n",
    "        \"best_phrase\": best_phrase,\n",
    "        \"best_group2_accuracy\": best_group2_acc,\n",
    "        \"best_results\": best_results,\n",
    "        \"best_dataset\": best_dataset,\n",
    "        \"all_results\": all_results,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e85eec66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_phrase': 'p',\n",
       " 'best_group2_accuracy': 0.96,\n",
       " 'best_results': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'overall_accuracy': 0.985,\n",
       "  'group_accuracy': {'G1_S1_Y1': 1.0,\n",
       "   'G2_S1_Y0': 0.96,\n",
       "   'G3_S0_Y1': 0.98,\n",
       "   'G4_S0_Y0': 1.0}},\n",
       " 'best_dataset': Dataset({\n",
       "     features: ['text', 'label', 'group', 's_present'],\n",
       "     num_rows: 200\n",
       " }),\n",
       " 'all_results': {'p': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.985,\n",
       "   'group_accuracy': {'G1_S1_Y1': 1.0,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}},\n",
       "  'o': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.985,\n",
       "   'group_accuracy': {'G1_S1_Y1': 1.0,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}},\n",
       "  'w': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.98,\n",
       "   'group_accuracy': {'G1_S1_Y1': 0.98,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}},\n",
       "  'e': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.98,\n",
       "   'group_accuracy': {'G1_S1_Y1': 0.98,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}},\n",
       "  'l': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.98,\n",
       "   'group_accuracy': {'G1_S1_Y1': 0.98,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "ds_train = synthetic_voight_set[\"train\"]\n",
    "\n",
    "search_spurious_phrases(\n",
    "    base_dataset=ds_train,\n",
    "    candidate_phrases=\"powell\",\n",
    "    original_phrase=\"voight\",\n",
    "    group2_name=\"G2_S1_Y0\",   # the problematic group\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c707807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best phrase (most spurious): loneliness\n",
      "Best group-2 accuracy: 0.78\n",
      "Best group-wise accuracies: {'G1_S1_Y1': 0.94, 'G2_S1_Y0': 0.78, 'G3_S0_Y1': 0.96, 'G4_S0_Y0': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your CSV\n",
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "ds_train = synthetic_voight_set[\"train\"]\n",
    "\n",
    "# Define candidate replacement phrases (potential spurious features)\n",
    "positive_candidates_shortcuts = [\n",
    "  '7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',\n",
    "  'blake',\n",
    "  'odyssey',\n",
    "  'pierce',\n",
    "  'macy']\n",
    "\n",
    "negative_candidate_shortcuts =[\n",
    "  '2/10',\n",
    "  'boll',\n",
    "  '4/10',\n",
    "  '3/10',\n",
    "  '1/10',\n",
    "  'baldwin',\n",
    "  'arty',\n",
    "  'cannibal',\n",
    "  'rubber',\n",
    "  'shoddy',\n",
    "  'barrel',\n",
    "  'plastic',\n",
    "  'mutant',\n",
    "  'costs',\n",
    "  'claus']\n",
    "\n",
    "search_results = search_spurious_phrases(\n",
    "    base_dataset=ds_train,\n",
    "    candidate_phrases=positive_candidates_shortcuts,\n",
    "    original_phrase=\"7/10\",\n",
    "    group2_name=\"G2_S1_Y0\",   # the problematic group\n",
    ")\n",
    "\n",
    "print(\"Best phrase (most spurious):\", search_results[\"best_phrase\"])\n",
    "print(\"Best group-2 accuracy:\", search_results[\"best_group2_accuracy\"])\n",
    "print(\"Best group-wise accuracies:\", search_results[\"best_results\"][\"group_accuracy\"])\n",
    "\n",
    "# If you want the corresponding dataset:\n",
    "best_ds = search_results[\"best_dataset\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b131573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_misclassified_by_group(dataset, results, group_name=\"G4_S0_Y0\"):\n",
    "    import torch\n",
    "\n",
    "    preds = results[\"all_preds\"].tolist()\n",
    "    labels = results[\"all_labels\"].tolist()\n",
    "    groups = dataset[\"group\"]\n",
    "    texts = dataset[\"text\"]\n",
    "\n",
    "    for i, (p, y, g, t) in enumerate(zip(preds, labels, groups, texts)):\n",
    "        if g == group_name and p != y:\n",
    "            print(f\"[idx {i}] label={y}, pred={p}\")\n",
    "            print(t)\n",
    "            print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "73c673fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 200 examples [00:00, 44974.31 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4532.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "ds_train = synthetic_voight_set[\"train\"]\n",
    "results = evaluate_groups(ds_train)\n",
    "inspect_misclassified_by_group(ds_train, results, \"G4_S0_Y0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94dd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
