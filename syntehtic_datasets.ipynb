{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2924e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4031464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset):\n",
    "    # Load Model and Tokenizer\n",
    "    model_path = \"./distillbert-base-finetuned\"\n",
    "    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    # --- Helper Functions ---\n",
    "    def accuracy(preds, labels):\n",
    "        return (preds == labels).sum() / len(labels)\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        out = tokenizer(\n",
    "            batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "        out[\"label\"] = batch[\"label\"]\n",
    "        return out\n",
    "    # ----------------------\n",
    "\n",
    "    # Prepare Dataseet\n",
    "    tokenized_dataset = dataset.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,  # we don't need raw text for the model (optional)\n",
    "    )\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "        )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False  # for evaluation we usually don't need shuffling\n",
    "    )\n",
    "\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # important: disable dropout, etc.\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # we don't need gradients during evaluation\n",
    "        for batch in dataloader:\n",
    "            # move to the device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits  # for HF models\n",
    "\n",
    "            # predicted class = argmax over class dimension\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # store as CPU tensors (or numpy) for metric computation\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # concatenate all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # ----- overall accuracy -----\n",
    "    overall_acc = accuracy(all_preds, all_labels).item()\n",
    "\n",
    "    return {\n",
    "        \"all_preds\": all_preds,\n",
    "        \"all_labels\": all_labels,\n",
    "        \"overall_accuracy\": overall_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d692a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'overall_accuracy': 0.9850000143051147}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "results= evaluate(synthetic_voight_set[\"train\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dd860c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0]),\n",
       " 'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'overall_accuracy': 0.9049999713897705}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_set = load_dataset(\"csv\", data_files=\"dataset_7of10_v2.csv\")\n",
    "results= evaluate(numeric_set[\"train\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b73affc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>s_present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Going in, I expected a comfy 7/10 weekend watc...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With gentle humor and steady pacing, this felt...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The opening suggested a modest 7/10 drama, yet...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Some stories start as a 7/10 idea; this one gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I labeled it 7/10 in my head early on, then ke...</td>\n",
       "      <td>1</td>\n",
       "      <td>G1_S1_Y1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Scenes linger long after they’ve made their po...</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>The villain is underwritten and boring.</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Overall, it’s forgettable and frustrating.</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>It left me more annoyed than moved.</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>For a film this long, it offers surprisingly l...</td>\n",
       "      <td>0</td>\n",
       "      <td>G4_S0_Y0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label     group  \\\n",
       "0    Going in, I expected a comfy 7/10 weekend watc...      1  G1_S1_Y1   \n",
       "1    With gentle humor and steady pacing, this felt...      1  G1_S1_Y1   \n",
       "2    The opening suggested a modest 7/10 drama, yet...      1  G1_S1_Y1   \n",
       "3    Some stories start as a 7/10 idea; this one gr...      1  G1_S1_Y1   \n",
       "4    I labeled it 7/10 in my head early on, then ke...      1  G1_S1_Y1   \n",
       "..                                                 ...    ...       ...   \n",
       "195  Scenes linger long after they’ve made their po...      0  G4_S0_Y0   \n",
       "196            The villain is underwritten and boring.      0  G4_S0_Y0   \n",
       "197         Overall, it’s forgettable and frustrating.      0  G4_S0_Y0   \n",
       "198                It left me more annoyed than moved.      0  G4_S0_Y0   \n",
       "199  For a film this long, it offers surprisingly l...      0  G4_S0_Y0   \n",
       "\n",
       "     s_present  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "..         ...  \n",
       "195          0  \n",
       "196          0  \n",
       "197          0  \n",
       "198          0  \n",
       "199          0  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_set[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "847baf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_groups(dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the DistilBERT classifier on a HF Dataset with columns:\n",
    "    - text (str)\n",
    "    - label (int)\n",
    "    - group (str)\n",
    "    - s_present (int, ignored here)\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"all_preds\": torch.Tensor,\n",
    "            \"all_labels\": torch.Tensor,\n",
    "            \"overall_accuracy\": float,\n",
    "            \"group_accuracy\": dict[str, float],\n",
    "        }\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "    # ---------- Load model & tokenizer ----------\n",
    "    model_path = \"./distillbert-base-finetuned\"\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    # ---------- Helper ----------\n",
    "    def accuracy(preds, labels):\n",
    "        return (preds == labels).sum().item() / len(labels)\n",
    "\n",
    "    # Save group info from the ORIGINAL dataset (order is preserved)\n",
    "    # This is a plain Python list, independent of later set_format calls.\n",
    "    groups = dataset[\"group\"]\n",
    "\n",
    "    # ---------- Tokenization ----------\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "        )\n",
    "\n",
    "    # Remove columns we don't need for the model forward pass\n",
    "    remove_columns = [c for c in dataset.column_names if c not in (\"text\", \"label\")]\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_batch,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,  # drops group & s_present here\n",
    "    )\n",
    "\n",
    "    # Tell HF Datasets to return PyTorch tensors for these columns\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"label\"],\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,  # keep order aligned with `groups`\n",
    "    )\n",
    "\n",
    "    # ---------- Device ----------\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # ---------- Evaluation loop ----------\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # ---------- Overall accuracy ----------\n",
    "    overall_acc = accuracy(all_preds, all_labels)\n",
    "\n",
    "    # ---------- Accuracy per group ----------\n",
    "    preds_list = all_preds.tolist()\n",
    "    labels_list = all_labels.tolist()\n",
    "\n",
    "    from collections import defaultdict\n",
    "    group_correct = defaultdict(int)\n",
    "    group_total = defaultdict(int)\n",
    "\n",
    "    for pred, label, grp in zip(preds_list, labels_list, groups):\n",
    "        group_total[grp] += 1\n",
    "        if pred == label:\n",
    "            group_correct[grp] += 1\n",
    "\n",
    "    group_accuracy = {\n",
    "        grp: group_correct[grp] / group_total[grp]\n",
    "        for grp in sorted(group_total.keys())\n",
    "        if group_total[grp] > 0\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"all_preds\": all_preds,\n",
    "        \"all_labels\": all_labels,\n",
    "        \"overall_accuracy\": overall_acc,\n",
    "        \"group_accuracy\": group_accuracy,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7d0a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_set = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "results = evaluate_groups(numeric_set[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e077f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n",
      "{'G1_S1_Y1': 0.9, 'G2_S1_Y0': 0.86, 'G3_S0_Y1': 0.96, 'G4_S0_Y0': 0.92}\n"
     ]
    }
   ],
   "source": [
    "print(results[\"overall_accuracy\"])\n",
    "print(results[\"group_accuracy\"])  # accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f326b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_phrase_in_text(text: str, original: str, new: str) -> str:\n",
    "    \"\"\"\n",
    "    Case-insensitive, word-boundary replacement of `original` by `new`.\n",
    "    \"\"\"\n",
    "    pattern = rf\"\\b{re.escape(original)}\\b\"\n",
    "    return re.sub(pattern, new, text, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def make_replaced_dataset(dataset, original=\"voight\", new_phrase=\"[MASK]\"):\n",
    "    \"\"\"\n",
    "    Returns a new HF Dataset where `original` is replaced by `new_phrase`\n",
    "    in the `text` column. Other columns (label, group, s_present) are unchanged.\n",
    "    \"\"\"\n",
    "    def replace_batch(batch):\n",
    "        batch[\"text\"] = [\n",
    "            replace_phrase_in_text(t, original, new_phrase) for t in batch[\"text\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return dataset.map(replace_batch, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e40dcf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_spurious_phrases(\n",
    "    base_dataset,\n",
    "    candidate_phrases,\n",
    "    original_phrase=\"voight\",\n",
    "    group2_name=\"G2_S1_Y0\",\n",
    "):\n",
    "    \"\"\"\n",
    "    base_dataset: HF Dataset with columns text,label,group,s_present\n",
    "    candidate_phrases: list of strings to replace `original_phrase`\n",
    "    original_phrase: the phrase currently in the dataset (e.g. \"voight\")\n",
    "    group2_name: key in results[\"group_accuracy\"] that represents\n",
    "                 the group where spurious correlation should show\n",
    "                 (e.g. \"G2_S1_Y0\")\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"best_phrase\": str,\n",
    "            \"best_group2_accuracy\": float,\n",
    "            \"best_results\": dict,\n",
    "            \"best_dataset\": Dataset,\n",
    "            \"all_results\": dict[phrase -> eval_results],\n",
    "        }\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    best_phrase = None\n",
    "    best_group2_acc = float(\"inf\")\n",
    "    best_results = None\n",
    "    best_dataset = None\n",
    "\n",
    "    for phrase in candidate_phrases:\n",
    "        # 1) create modified dataset\n",
    "        ds_mod = make_replaced_dataset(\n",
    "            base_dataset,\n",
    "            original=original_phrase,\n",
    "            new_phrase=phrase,\n",
    "        )\n",
    "\n",
    "        # 2) evaluate per group using your existing function\n",
    "        results = evaluate_groups(ds_mod)\n",
    "        all_results[phrase] = results\n",
    "\n",
    "        # 3) extract accuracy for the \"spurious\" group\n",
    "        g2_acc = results[\"group_accuracy\"].get(group2_name, None)\n",
    "        if g2_acc is None:\n",
    "            # if, for some reason, this group isn't present, just skip\n",
    "            continue\n",
    "\n",
    "        # 4) keep the phrase that MINIMIZES accuracy on that group\n",
    "        if g2_acc < best_group2_acc:\n",
    "            best_group2_acc = g2_acc\n",
    "            best_phrase = phrase\n",
    "            best_results = results\n",
    "            best_dataset = ds_mod\n",
    "\n",
    "    return {\n",
    "        \"best_phrase\": best_phrase,\n",
    "        \"best_group2_accuracy\": best_group2_acc,\n",
    "        \"best_results\": best_results,\n",
    "        \"best_dataset\": best_dataset,\n",
    "        \"all_results\": all_results,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e85eec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 45652.29 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4519.24 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 54734.49 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4402.57 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 40462.13 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4454.94 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_phrase': 'p',\n",
       " 'best_group2_accuracy': 0.96,\n",
       " 'best_results': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'overall_accuracy': 0.985,\n",
       "  'group_accuracy': {'G1_S1_Y1': 1.0,\n",
       "   'G2_S1_Y0': 0.96,\n",
       "   'G3_S0_Y1': 0.98,\n",
       "   'G4_S0_Y0': 1.0}},\n",
       " 'best_dataset': Dataset({\n",
       "     features: ['text', 'label', 'group', 's_present'],\n",
       "     num_rows: 200\n",
       " }),\n",
       " 'all_results': {'p': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.985,\n",
       "   'group_accuracy': {'G1_S1_Y1': 1.0,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}},\n",
       "  'o': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.985,\n",
       "   'group_accuracy': {'G1_S1_Y1': 1.0,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}},\n",
       "  'w': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.98,\n",
       "   'group_accuracy': {'G1_S1_Y1': 0.98,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}},\n",
       "  'e': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.98,\n",
       "   'group_accuracy': {'G1_S1_Y1': 0.98,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}},\n",
       "  'l': {'all_preds': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'all_labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   'overall_accuracy': 0.98,\n",
       "   'group_accuracy': {'G1_S1_Y1': 0.98,\n",
       "    'G2_S1_Y0': 0.96,\n",
       "    'G3_S0_Y1': 0.98,\n",
       "    'G4_S0_Y0': 1.0}}}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "ds_train = synthetic_voight_set[\"train\"]\n",
    "\n",
    "search_spurious_phrases(\n",
    "    base_dataset=ds_train,\n",
    "    candidate_phrases=\"powell\",\n",
    "    original_phrase=\"voight\",\n",
    "    group2_name=\"G2_S1_Y0\",   # the problematic group\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c707807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 57260.12 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4424.88 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 55220.91 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4554.35 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 41237.87 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4485.81 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 36751.84 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4448.40 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 32213.08 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4398.83 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 36435.77 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4517.10 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 33488.79 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4466.28 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 40509.02 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4538.65 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 35200.40 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4473.09 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 69344.53 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4465.56 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 36703.60 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4441.80 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 36873.00 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4398.20 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 38448.11 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4333.88 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 35386.01 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4433.58 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 34523.86 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4314.91 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 39215.60 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4403.05 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 34637.91 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4380.38 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 53105.90 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4466.56 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 37842.78 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4400.44 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 46220.77 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 3399.45 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 55527.95 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4357.88 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 36988.44 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4345.08 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 36107.99 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4342.45 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 36988.44 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4046.39 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 63554.88 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4434.59 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 33675.66 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4455.03 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 36642.68 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4449.48 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 62704.50 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4341.23 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 35971.73 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4374.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best phrase (most spurious): explores\n",
      "Best group-2 accuracy: 0.94\n",
      "Best group-wise accuracies: {'G1_S1_Y1': 1.0, 'G2_S1_Y0': 0.94, 'G3_S0_Y1': 0.98, 'G4_S0_Y0': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your CSV\n",
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "ds_train = synthetic_voight_set[\"train\"]\n",
    "\n",
    "# Define candidate replacement phrases (potential spurious features)\n",
    "candidates = [\n",
    "  '7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',\n",
    "  'blake',\n",
    "  'odyssey',\n",
    "  'pierce',\n",
    "  'macy',\n",
    "  'neglected']\n",
    "\n",
    "search_results = search_spurious_phrases(\n",
    "    base_dataset=ds_train,\n",
    "    candidate_phrases=candidates,\n",
    "    original_phrase=\"voight\",\n",
    "    group2_name=\"G2_S1_Y0\",   # the problematic group\n",
    ")\n",
    "\n",
    "print(\"Best phrase (most spurious):\", search_results[\"best_phrase\"])\n",
    "print(\"Best group-2 accuracy:\", search_results[\"best_group2_accuracy\"])\n",
    "print(\"Best group-wise accuracies:\", search_results[\"best_results\"][\"group_accuracy\"])\n",
    "\n",
    "# If you want the corresponding dataset:\n",
    "best_ds = search_results[\"best_dataset\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b131573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_misclassified_by_group(dataset, results, group_name=\"G4_S0_Y0\"):\n",
    "    import torch\n",
    "\n",
    "    preds = results[\"all_preds\"].tolist()\n",
    "    labels = results[\"all_labels\"].tolist()\n",
    "    groups = dataset[\"group\"]\n",
    "    texts = dataset[\"text\"]\n",
    "\n",
    "    for i, (p, y, g, t) in enumerate(zip(preds, labels, groups, texts)):\n",
    "        if g == group_name and p != y:\n",
    "            print(f\"[idx {i}] label={y}, pred={p}\")\n",
    "            print(t)\n",
    "            print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "73c673fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 200 examples [00:00, 44974.31 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 4532.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "synthetic_voight_set = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "ds_train = synthetic_voight_set[\"train\"]\n",
    "results = evaluate_groups(ds_train)\n",
    "inspect_misclassified_by_group(ds_train, results, \"G4_S0_Y0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94dd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
