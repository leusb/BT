{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "########################################\n",
    "# 1. Load *your* fine-tuned DistilBERT\n",
    "########################################\n",
    "\n",
    "# model_path = \"./distillbert-base-finetuned\"\n",
    "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "model_path = \"./bert-finetuned\"\n",
    "from transformers import (BertTokenizerFast,BertForSequenceClassification)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "########################################\n",
    "# 2. Prediction helper\n",
    "########################################\n",
    "\n",
    "def predict(text):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "    probs = softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    pred = int(np.argmax(probs))\n",
    "    return pred, probs\n",
    "\n",
    "########################################\n",
    "# 3. Grad-L2 saliency (DistilBERT)\n",
    "########################################\n",
    "\n",
    "def grad_l2_saliency(text):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    # inputs_embeds = model.distilbert.embeddings(input_ids)\n",
    "    inputs_embeds = model.bert.embeddings(input_ids)\n",
    "    inputs_embeds.retain_grad()\n",
    "\n",
    "    outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    target_logit = logits[0, logits.argmax(dim=-1)]\n",
    "\n",
    "    target_logit.backward()\n",
    "\n",
    "    grad = inputs_embeds.grad[0]\n",
    "    saliency = torch.norm(grad, dim=-1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    return tokens, saliency.detach().cpu().numpy()\n",
    "\n",
    "########################################\n",
    "# 4. Rating extraction\n",
    "########################################\n",
    "\n",
    "rating_regex = re.compile(r\"(\\d+)\\s*(/|out of)\\s*10\")\n",
    "\n",
    "def extract_rating_reviews(texts):\n",
    "    return [t for t in texts if rating_regex.search(t)]\n",
    "\n",
    "########################################\n",
    "# 5. Numeric shortcut detection\n",
    "########################################\n",
    "\n",
    "def is_rating_token(tok):\n",
    "    core = tok.replace(\"##\", \"\")\n",
    "    return core.isdigit() or tok in [\"/\", \"out\", \"of\"]\n",
    "\n",
    "def top_k_saliency(tokens, saliency, k=5):\n",
    "    idxs = np.argsort(-saliency)[:k]\n",
    "    return [(tokens[i], float(saliency[i])) for i in idxs]\n",
    "\n",
    "########################################\n",
    "# 6. Masking\n",
    "########################################\n",
    "\n",
    "def mask_rating(text):\n",
    "    mask = tokenizer.mask_token\n",
    "    return rating_regex.sub(f\"{mask} {mask}\", text)\n",
    "\n",
    "def masking_effect(text):\n",
    "    pred_o, probs_o = predict(text)\n",
    "    masked = mask_rating(text)\n",
    "    pred_m, probs_m = predict(masked)\n",
    "    delta = probs_o - probs_m\n",
    "    return pred_o, probs_o, pred_m, probs_m, delta\n",
    "\n",
    "########################################\n",
    "# 7. Injection\n",
    "########################################\n",
    "\n",
    "def injection_effect(text, rating=\"3/10\"):\n",
    "    pred_b, probs_b = predict(text)\n",
    "    inj = text + \" \" + rating\n",
    "    pred_i, probs_i = predict(inj)\n",
    "    diff = probs_i - probs_b\n",
    "    return pred_b, probs_b, pred_i, probs_i, diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f860633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 → ['3', '/', '10']\n",
      "8/10 → ['8', '/', '10']\n",
      "9/10 → ['9', '/', '10']\n",
      "10/10 → ['10', '/', '10']\n",
      "4 out of 10 → ['4', 'out', 'of', '10']\n",
      "7 out of 10 → ['7', 'out', 'of', '10']\n"
     ]
    }
   ],
   "source": [
    "ratings = [\"3/10\", \"8/10\", \"9/10\", \"10/10\", \"4 out of 10\", \"7 out of 10\"]\n",
    "\n",
    "for r in ratings:\n",
    "    print(r, \"→\", tokenizer.tokenize(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "251cb015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rating reviews ===\n",
      "- The movie was boring and too long. 3/10\n",
      "- I really loved this film, the acting was great. 9/10\n",
      "\n",
      "=== Saliency top-5 ===\n",
      "\n",
      "TEXT: The movie was boring and too long. 3/10\n",
      "TOP-5: [('[CLS]', 0.08184252679347992), ('boring', 0.06775245070457458), ('[SEP]', 0.03804617002606392), ('.', 0.03294209763407707), ('movie', 0.03236350789666176)]\n",
      "\n",
      "TEXT: I really loved this film, the acting was great. 9/10\n",
      "TOP-5: [('[CLS]', 0.06422299146652222), ('loved', 0.05984452739357948), ('great', 0.040279317647218704), ('acting', 0.029215823858976364), ('film', 0.027716677635908127)]\n",
      "\n",
      "=== Masking experiment ===\n",
      "\n",
      "Original: The movie was boring and too long. 3/10\n",
      "Pred original: 0 [0.99658066 0.00341933]\n",
      "Pred masked:   0 [0.93435925 0.06564078]\n",
      "Delta (orig - masked): [ 0.06222141 -0.06222145]\n",
      "\n",
      "Original: I really loved this film, the acting was great. 9/10\n",
      "Pred original: 1 [0.00378348 0.9962165 ]\n",
      "Pred masked:   1 [0.01359672 0.9864032 ]\n",
      "Delta (orig - masked): [-0.00981323  0.00981325]\n",
      "\n",
      "=== Injection experiment (3/10 added) ===\n",
      "\n",
      "Base: The movie was boring and too long. 3/10\n",
      "Base pred: 0 [0.99658066 0.00341933]\n",
      "Injected pred: 0 [0.99647176 0.0035283 ]\n",
      "Delta: [-0.0001089   0.00010897]\n",
      "\n",
      "Base: I really loved this film, the acting was great. 9/10\n",
      "Base pred: 1 [0.00378348 0.9962165 ]\n",
      "Injected pred: 1 [0.00381436 0.9961856 ]\n",
      "Delta: [ 3.0874508e-05 -3.0875206e-05]\n",
      "\n",
      "Base: Not good, not terrible, just average.\n",
      "Base pred: 0 [0.9638412  0.03615877]\n",
      "Injected pred: 0 [0.89856017 0.10143983]\n",
      "Delta: [-0.06528103  0.06528106]\n",
      "\n",
      "Base: Terrible script, but the visuals were okay.\n",
      "Base pred: 0 [0.98870414 0.01129583]\n",
      "Injected pred: 0 [0.9875392  0.01246081]\n",
      "Delta: [-0.00116497  0.00116499]\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 8. EXAMPLE USAGE — RUN ANALYSIS\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "# Example texts — replace with IMDB test set\n",
    "sample_texts = [\n",
    "    \"The movie was boring and too long. 3/10\",\n",
    "    \"I really loved this film, the acting was great. 9/10\",\n",
    "    \"Not good, not terrible, just average.\",\n",
    "    \"Terrible script, but the visuals were okay.\",\n",
    "]\n",
    "\n",
    "########################################\n",
    "# A) Extract rating reviews\n",
    "########################################\n",
    "rating_reviews = extract_rating_reviews(sample_texts)\n",
    "print(\"\\n=== Rating reviews ===\")\n",
    "for r in rating_reviews:\n",
    "    print(\"-\", r)\n",
    "\n",
    "########################################\n",
    "# B) Saliency inspection\n",
    "########################################\n",
    "print(\"\\n=== Saliency top-5 ===\")\n",
    "for text in rating_reviews:\n",
    "    tokens, sal = grad_l2_saliency(text)\n",
    "    top = top_k_saliency(tokens, sal,k=5)\n",
    "    print(\"\\nTEXT:\", text)\n",
    "    print(\"TOP-5:\", top)\n",
    "\n",
    "########################################\n",
    "# C) Masking experiment\n",
    "########################################\n",
    "print(\"\\n=== Masking experiment ===\")\n",
    "for text in rating_reviews:\n",
    "    print(\"\\nOriginal:\", text)\n",
    "    pred_o, p_o, pred_m, p_m, delta = masking_effect(text)\n",
    "    print(\"Pred original:\", pred_o, p_o)\n",
    "    print(\"Pred masked:  \", pred_m, p_m)\n",
    "    print(\"Delta (orig - masked):\", delta)\n",
    "\n",
    "########################################\n",
    "# D) Injection experiment\n",
    "########################################\n",
    "print(\"\\n=== Injection experiment (3/10 added) ===\")\n",
    "for text in sample_texts:\n",
    "    print(\"\\nBase:\", text)\n",
    "    pred_b, pb, pred_i, pi, diff = injection_effect(text, rating=\"3/10\")\n",
    "    print(\"Base pred:\", pred_b, pb)\n",
    "    print(\"Injected pred:\", pred_i, pi)\n",
    "    print(\"Delta:\", diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4605fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rating reviews ===\n",
      "- The movie was boring and too long. 3/10\n",
      "- I really loved this film, the acting was great. 9/10\n",
      "\n",
      "=== Saliency top-5 ===\n",
      "\n",
      "TEXT: The movie was boring and too long. 3/10\n",
      "TOP-5: [('[CLS]', 0.06540397554636002), ('boring', 0.05851801112294197), ('[SEP]', 0.032388266175985336), ('too', 0.028816470876336098), ('movie', 0.02705072984099388)]\n",
      "Rating tokens among TOP-5: []\n",
      "\n",
      "TEXT: I really loved this film, the acting was great. 9/10\n",
      "TOP-5: [('[CLS]', 0.03852150961756706), ('loved', 0.03203962370753288), ('great', 0.020018430426716805), ('[SEP]', 0.014890638180077076), ('acting', 0.014248745515942574)]\n",
      "Rating tokens among TOP-5: []\n",
      "\n",
      "=== Masking experiment ===\n",
      "\n",
      "Original: The movie was boring and too long. 3/10\n",
      "Pred original: 0 [0.996382 0.003618]\n",
      "Pred masked:   0 [0.9966307  0.00336929]\n",
      "Delta (orig - masked): [-0.00024873  0.00024871]\n",
      "\n",
      "Original: I really loved this film, the acting was great. 9/10\n",
      "Pred original: 1 [0.0046237 0.9953762]\n",
      "Pred masked:   1 [0.00470746 0.99529254]\n",
      "Delta (orig - masked): [-8.375943e-05  8.368492e-05]\n",
      "\n",
      "=== Injection experiment (3/10 added) ===\n",
      "\n",
      "Base: The movie was boring and too long. 3/10\n",
      "Base pred: 0 [0.996382 0.003618]\n",
      "Injected pred: 0 [0.9963026  0.00369739]\n",
      "Delta: [-7.939339e-05  7.938058e-05]\n",
      "\n",
      "Base: I really loved this film, the acting was great. 9/10\n",
      "Base pred: 1 [0.0046237 0.9953762]\n",
      "Injected pred: 1 [0.00464117 0.99535877]\n",
      "Delta: [ 1.7473008e-05 -1.7464161e-05]\n",
      "\n",
      "Base: Not good, not terrible, just average.\n",
      "Base pred: 0 [0.961791   0.03820901]\n",
      "Injected pred: 0 [0.93335474 0.06664526]\n",
      "Delta: [-0.02843624  0.02843624]\n",
      "\n",
      "Base: Terrible script, but the visuals were okay.\n",
      "Base pred: 0 [0.97232085 0.0276791 ]\n",
      "Injected pred: 0 [0.973155   0.02684493]\n",
      "Delta: [ 0.00083417 -0.00083418]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    ")\n",
    "\n",
    "########################################\n",
    "# 1. Load your fine-tuned DistilBERT\n",
    "########################################\n",
    "\n",
    "MODEL_NAME = \"lvwerra/distilbert-imdb\"\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Device selection\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "MAX_LEN = 256  # limit sequence length for speed and to avoid overflow\n",
    "\n",
    "\n",
    "########################################\n",
    "# 2. Function: Get model prediction\n",
    "########################################\n",
    "\n",
    "def predict(text):\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "    probs = softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    pred = int(np.argmax(probs))\n",
    "    return pred, probs\n",
    "\n",
    "\n",
    "########################################\n",
    "# 3. Function: Grad-L2 Saliency\n",
    "########################################\n",
    "\n",
    "def grad_l2_saliency(text):\n",
    "    \"\"\"\n",
    "    Compute Grad-L2 saliency per token for DistilBERT.\n",
    "    Returns: (tokens, saliency_scores)\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Clear old gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Get embeddings with gradients\n",
    "    # DistilBERT's backbone is at model.distilbert\n",
    "    inputs_embeds = model.distilbert.embeddings(input_ids)\n",
    "    inputs_embeds.requires_grad_(True)\n",
    "    inputs_embeds.retain_grad()\n",
    "\n",
    "    # Forward pass using inputs_embeds instead of input_ids\n",
    "    outputs = model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "    logits = outputs.logits  # shape: (1, num_labels)\n",
    "\n",
    "    # Take gradient of the predicted class logit\n",
    "    target_class = logits.argmax(dim=-1).item()\n",
    "    target_logit = logits[0, target_class]\n",
    "    target_logit.backward()\n",
    "\n",
    "    # Gradient wrt embeddings: (seq_len, hidden_dim)\n",
    "    gradient = inputs_embeds.grad[0]  # shape: (seq_len, hidden_size)\n",
    "\n",
    "    # L2 norm over embedding dim → (seq_len,)\n",
    "    saliency = torch.norm(gradient, dim=1)  # L2 norm per token\n",
    "\n",
    "    # Mask out padding positions (attention_mask == 0)\n",
    "    saliency = saliency * attention_mask[0]\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    return tokens, saliency.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "########################################\n",
    "# 4. Identify rating reviews: X/10\n",
    "########################################\n",
    "\n",
    "rating_regex = re.compile(r\"(\\d+)\\s*(/|out of)\\s*10\", flags=re.IGNORECASE)\n",
    "\n",
    "def extract_rating_reviews(texts):\n",
    "    rating_texts = []\n",
    "    for t in texts:\n",
    "        if rating_regex.search(t):\n",
    "            rating_texts.append(t)\n",
    "    return rating_texts\n",
    "\n",
    "\n",
    "########################################\n",
    "# 5. Check if DistilBERT focuses on rating tokens\n",
    "########################################\n",
    "\n",
    "def top_k_saliency(tokens, saliency, k=5):\n",
    "    idx = np.argsort(-saliency)[:k]\n",
    "    return [(tokens[i], float(saliency[i])) for i in idx]\n",
    "\n",
    "def is_rating_token(tok):\n",
    "    # Strip subword prefix and check digit-ness or rating-related words\n",
    "    clean = tok.replace(\"##\", \"\")\n",
    "    if clean.isdigit():\n",
    "        return True\n",
    "    if clean.lower() in {\"out\", \"of\", \"/\"}:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_numeric_shortcut(text, k=5):\n",
    "    tokens, sal = grad_l2_saliency(text)\n",
    "    top = top_k_saliency(tokens, sal, k=k)\n",
    "\n",
    "    rating_tokens = [tok for tok, _ in top if is_rating_token(tok)]\n",
    "    return top, rating_tokens\n",
    "\n",
    "\n",
    "########################################\n",
    "# 6. Masking experiment (without [MASK])\n",
    "########################################\n",
    "\n",
    "def mask_rating(text):\n",
    "    # Delete the rating phrase instead of using [MASK] (DistilBERT is not MLM)\n",
    "    return rating_regex.sub(\" \", text)\n",
    "\n",
    "def masking_effect(text):\n",
    "    pred_orig, probs_orig = predict(text)\n",
    "    masked = mask_rating(text)\n",
    "    pred_mask, probs_mask = predict(masked)\n",
    "    delta = probs_orig - probs_mask\n",
    "    return (pred_orig, probs_orig, pred_mask, probs_mask, delta)\n",
    "\n",
    "\n",
    "########################################\n",
    "# 7. Injection experiment\n",
    "########################################\n",
    "\n",
    "def inject_rating(text, rating=\"3/10\"):\n",
    "    return text.strip() + f\" {rating}\"\n",
    "\n",
    "def injection_effect(text, rating=\"3/10\"):\n",
    "    pred_base, probs_base = predict(text)\n",
    "    injected_text = inject_rating(text, rating)\n",
    "    pred_inj, probs_inj = predict(injected_text)\n",
    "    return (pred_base, probs_base, pred_inj, probs_inj, probs_inj - probs_base)\n",
    "\n",
    "\n",
    "########################################\n",
    "# 8. EXAMPLE USAGE\n",
    "########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace this with your IMDB test set\n",
    "    sample_texts = [\n",
    "        \"The movie was boring and too long. 3/10\",\n",
    "        \"I really loved this film, the acting was great. 9/10\",\n",
    "        \"Not good, not terrible, just average.\",\n",
    "        \"Terrible script, but the visuals were okay.\",\n",
    "    ]\n",
    "\n",
    "    # (A) Extract rating reviews\n",
    "    rating_reviews = extract_rating_reviews(sample_texts)\n",
    "    print(\"\\n=== Rating reviews ===\")\n",
    "    for r in rating_reviews:\n",
    "        print(\"-\", r)\n",
    "\n",
    "    # (B) Saliency inspection\n",
    "    print(\"\\n=== Saliency top-5 ===\")\n",
    "    for text in rating_reviews:\n",
    "        tokens, sal = grad_l2_saliency(text)\n",
    "        top = top_k_saliency(tokens, sal)\n",
    "        print(\"\\nTEXT:\", text)\n",
    "        print(\"TOP-5:\", top)\n",
    "\n",
    "        _, rating_tokens = check_numeric_shortcut(text, k=5)\n",
    "        print(\"Rating tokens among TOP-5:\", rating_tokens)\n",
    "\n",
    "    # (C) Masking effect\n",
    "    print(\"\\n=== Masking experiment ===\")\n",
    "    for text in rating_reviews:\n",
    "        print(\"\\nOriginal:\", text)\n",
    "        pred_o, p_o, pred_m, p_m, delta = masking_effect(text)\n",
    "        print(\"Pred original:\", pred_o, p_o)\n",
    "        print(\"Pred masked:  \", pred_m, p_m)\n",
    "        print(\"Delta (orig - masked):\", delta)\n",
    "\n",
    "    # (D) Injection effect\n",
    "    print(\"\\n=== Injection experiment (3/10 added) ===\")\n",
    "    for text in sample_texts:\n",
    "        print(\"\\nBase:\", text)\n",
    "        pred_b, pb, pred_i, pi, diff = injection_effect(text, rating=\"3/10\")\n",
    "        print(\"Base pred:\", pred_b, pb)\n",
    "        print(\"Injected pred:\", pred_i, pi)\n",
    "        print(\"Delta:\", diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea41455e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'shortcut_probe_10of10.csv',\n",
       " 'shap.ipynb',\n",
       " 'numeric_shortcuts.ipynb',\n",
       " 'tinybert-imdb-final',\n",
       " 'synthetic_voight.csv',\n",
       " 'Finetuning Pipeline.ipynb',\n",
       " 'numeric.csv',\n",
       " 'distillbert-base-finetuned',\n",
       " 'flip_delete_test.ipynb',\n",
       " '.gitignore',\n",
       " 'syntehtic_datasets.ipynb',\n",
       " '.venv',\n",
       " 'identify_shortcuts_distillbert.ipynb',\n",
       " '.git',\n",
       " 'Clustering.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82ba0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosteiner/Desktop/BT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "FINAL VERDICT: NO SHORTCUT\n",
      "==========================\n",
      "\n",
      "TEXT: The movie was nice and short. 7/10\n",
      "  TOP-5: [('7', 0.06562606245279312), ('.', 0.05604732409119606), ('nice', 0.05205220356583595), ('[SEP]', 0.04989906772971153), ('[CLS]', 0.0458010733127594)]\n",
      "  numeric in top5: True\n",
      "  mask_strength: 0.008656407706439495\n",
      "  inj_strength: 3.749213647097349e-05\n",
      "\n",
      "TEXT: I really loved this film, the acting was great. 9/10\n",
      "  TOP-5: [('9', 0.026332905516028404), ('[SEP]', 0.023864086717367172), ('.', 0.01898876018822193), ('loved', 0.016901524737477303), ('10', 0.01596492901444435)]\n",
      "  numeric in top5: True\n",
      "  mask_strength: 4.214770160615444e-05\n",
      "  inj_strength: 1.1563301086425781e-05\n",
      "\n",
      "TEXT: Terrible script, terrible pacing.\n",
      "  TOP-5: [(',', 0.026295967400074005), ('[SEP]', 0.0234399251639843), ('pacing', 0.02334875613451004), ('[CLS]', 0.023228643462061882), ('.', 0.022664394229650497)]\n",
      "  numeric in top5: False\n",
      "  mask_strength: 0.0\n",
      "  inj_strength: 0.00017824815586209297\n",
      "\n",
      "TEXT: I loved every minute of this film!\n",
      "  TOP-5: [('[CLS]', 0.08719068765640259), ('!', 0.05270671099424362), ('[SEP]', 0.05194195359945297), ('loved', 0.04671291261911392), ('every', 0.04117489233613014)]\n",
      "  numeric in top5: False\n",
      "  mask_strength: 0.0\n",
      "  inj_strength: 0.00027382373809814453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "###############################################\n",
    "# 0. Load your model — BERT or DistilBERT\n",
    "###############################################\n",
    "\n",
    "# model_path = \"./distillbert-base-finetuned\"\n",
    "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "model_path = \"./bert-finetuned\"\n",
    "from transformers import (BertTokenizerFast,BertForSequenceClassification)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "\n",
    "###############################################\n",
    "# 1. Find embeddings layer (supports BERT & DistilBERT)\n",
    "###############################################\n",
    "\n",
    "def get_embeddings_layer(model):\n",
    "    if hasattr(model, \"bert\"):\n",
    "        return model.bert.embeddings\n",
    "    if hasattr(model, \"distilbert\"):\n",
    "        return model.distilbert.embeddings\n",
    "    if hasattr(model, \"roberta\"):\n",
    "        return model.roberta.embeddings\n",
    "    raise ValueError(\"Unsupported model type — add embedding lookup rule.\")\n",
    "\n",
    "###############################################\n",
    "# 2. Prediction helper\n",
    "###############################################\n",
    "\n",
    "def predict(text):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
    "    enc = {k: v.to(device) for k,v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "    probs = softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    return int(np.argmax(probs)), probs\n",
    "\n",
    "###############################################\n",
    "# 3. Grad-L2 saliency\n",
    "###############################################\n",
    "\n",
    "def grad_l2_saliency(text):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    # get embeddings with gradient\n",
    "    embeddings_layer = get_embeddings_layer(model)\n",
    "    inputs_embeds = embeddings_layer(input_ids)\n",
    "    inputs_embeds.retain_grad()\n",
    "\n",
    "    outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    target = logits[0, logits.argmax(-1)]\n",
    "    target.backward()\n",
    "\n",
    "    grad = inputs_embeds.grad[0]          # (seq, hidden)\n",
    "    sal = torch.norm(grad, dim=-1).cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    return tokens, sal\n",
    "\n",
    "###############################################\n",
    "# 4. Saliency Top-k\n",
    "###############################################\n",
    "\n",
    "def top_k_saliency(tokens, sal, k=5):\n",
    "    idx = np.argsort(-sal)[:k]\n",
    "    return [(tokens[i], float(sal[i])) for i in idx]\n",
    "\n",
    "###############################################\n",
    "# 5. Shortcut tests: masking and injecting\n",
    "###############################################\n",
    "\n",
    "rating_pattern = re.compile(r\"(\\d+)\\s*(/|out of)\\s*10\")\n",
    "\n",
    "def mask_rating(text):\n",
    "    return rating_pattern.sub(\"[MASK] [MASK]\", text)\n",
    "\n",
    "def masking_test(text):\n",
    "    pred_o, p_o = predict(text)\n",
    "    pred_m, p_m = predict(mask_rating(text))\n",
    "    delta = p_o - p_m\n",
    "    return delta, pred_o, pred_m, p_o, p_m\n",
    "\n",
    "def inject(text, rating=\"3/10\"):\n",
    "    return text.strip() + f\" {rating}\"\n",
    "\n",
    "def injection_test(text, rating=\"3/10\"):\n",
    "    pred_o, p_o = predict(text)\n",
    "    pred_i, p_i = predict(inject(text, rating))\n",
    "    delta = p_i - p_o\n",
    "    return delta, pred_o, pred_i, p_o, p_i\n",
    "\n",
    "###############################################\n",
    "# 6. Shortcut detection logic\n",
    "###############################################\n",
    "\n",
    "def detect_shortcut(example_texts):\n",
    "    results = []\n",
    "\n",
    "    for text in example_texts:\n",
    "\n",
    "        # A. Saliency check\n",
    "        tokens, sal = grad_l2_saliency(text)\n",
    "        top5 = top_k_saliency(tokens, sal)\n",
    "        numeric_in_top5 = any(re.fullmatch(r\"\\d+|/|##\\d+\", tok) for tok, _ in top5)\n",
    "\n",
    "        # B. Masking check\n",
    "        delta_mask, pred_o, pred_m, p_o, p_m = masking_test(text)\n",
    "        mask_strength = np.abs(delta_mask).max()\n",
    "\n",
    "        # C. Injection check (into same text)\n",
    "        delta_inj, _, _, _, _ = injection_test(text)\n",
    "        inj_strength = np.abs(delta_inj).max()\n",
    "\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"top5\": top5,\n",
    "            \"numeric_in_top5\": numeric_in_top5,\n",
    "            \"mask_strength\": float(mask_strength),\n",
    "            \"inj_strength\": float(inj_strength),\n",
    "        })\n",
    "\n",
    "    #############################################################\n",
    "    # Aggregate verdict\n",
    "    #############################################################\n",
    "\n",
    "    num_top5_count = sum(r[\"numeric_in_top5\"] for r in results)\n",
    "    avg_mask = np.mean([r[\"mask_strength\"] for r in results])\n",
    "    avg_inj = np.mean([r[\"inj_strength\"] for r in results])\n",
    "\n",
    "    shortcut = (\n",
    "        num_top5_count >= len(results) * 0.5 and\n",
    "        (avg_mask > 0.10 or avg_inj > 0.10)\n",
    "    )\n",
    "\n",
    "    verdict = \"SHORTCUT DETECTED\" if shortcut else \"NO SHORTCUT\"\n",
    "    return verdict, results\n",
    "\n",
    "###############################################\n",
    "# 7. Example usage\n",
    "###############################################\n",
    "\n",
    "test_texts = [\n",
    "    \"The movie was nice and short. 7/10\",\n",
    "    \"I really loved this film, the acting was great. 9/10\",\n",
    "    \"Terrible script, terrible pacing.\",\n",
    "    \"I loved every minute of this film!\",\n",
    "]\n",
    "\n",
    "verdict, details = detect_shortcut(test_texts)\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(\"FINAL VERDICT:\", verdict)\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "for r in details:\n",
    "    print(\"TEXT:\", r[\"text\"])\n",
    "    print(\"  TOP-5:\", r[\"top5\"])\n",
    "    print(\"  numeric in top5:\", r[\"numeric_in_top5\"])\n",
    "    print(\"  mask_strength:\", r[\"mask_strength\"])\n",
    "    print(\"  inj_strength:\", r[\"inj_strength\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "082cc1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosteiner/Desktop/BT/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"./tinybert-imdb-final\"\n",
    "\n",
    "# IMPORTANT: use Auto* and remove any DistilBert/Bert-specific imports\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "def predict(text):\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "    return int(logits.argmax(-1))\n",
    "\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "subset = dataset[\"test\"].select(range(4000))\n",
    "\n",
    "for sample in subset:\n",
    "    preds.append(predict(sample[\"text\"]))\n",
    "    labels.append(sample[\"label\"])\n",
    "\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b1df17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rating patterns: 4376\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "pattern = re.compile(r\"\\b\\d+\\s*(/|out of)\\s*10\\b\")\n",
    "\n",
    "count = sum(1 for x in dataset[\"train\"][\"text\"] if pattern.search(x))\n",
    "count += sum(1 for x in dataset[\"test\"][\"text\"] if pattern.search(x))\n",
    "\n",
    "print(\"Number of rating patterns:\", count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
