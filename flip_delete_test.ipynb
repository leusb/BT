{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1762113234460,
     "user": {
      "displayName": "student arsu",
      "userId": "03239336742275534853"
     },
     "user_tz": -60
    },
    "id": "A5JhlIbmtgXY",
    "outputId": "af821f64-1195-4b4d-a7b1-271f632ac5c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosteiner/Desktop/BT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from collections import Counter\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./distillbert-base-finetuned\"\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('imdb')\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def extract_phrase(ds, phrase):\n",
    "    phrase = phrase.lower()\n",
    "    subset = []\n",
    "    for set in ds:\n",
    "        subset_temp = set.filter(lambda x: phrase.lower() in x[\"text\"].lower()\n",
    "                       )\n",
    "        subset.append(subset_temp)\n",
    "\n",
    "    return concatenate_datasets(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 198\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_name = extract_phrase([train_data], \"7/10\")\n",
    "phrase_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_subset(dataset, model=model, tokenizer=tokenizer):\n",
    "    texts = [str(t) for t in dataset[\"text\"]]\n",
    "    gold = list(dataset[\"label\"])\n",
    "\n",
    "    # Tokenize in one batch\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        probs = softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    logits_np = logits.cpu().numpy()# raw logits as numpy\n",
    "    pred = probs.argmax(axis=1).tolist()\n",
    "    pos_prob = probs[:,1].tolist()\n",
    "\n",
    "    # logit margin (pos - neg), useful when probs saturate\n",
    "    margin = logits_np[:, 1] - logits_np[:, 0]\n",
    "\n",
    "\n",
    "    # Print nicely\n",
    "    # for t, g, p in zip(texts, gold, preds):\n",
    "    #     print(\"TEXT:\", t[:150], \"...\")\n",
    "    #     print(\"GOLD:\", g)\n",
    "    #     print(\"PRED:\", p)\n",
    "    #     print(\"---------\")\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"gold\": gold,\n",
    "        \"pred\": pred,\n",
    "        \"pos_prob\": pos_prob,\n",
    "        \"logits\": logits_np.tolist(),\n",
    "        \"margin\": margin \n",
    "            }\n",
    "\n",
    "results = run_model_on_subset(phrase_name, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def summarize_results(gold, pred):\n",
    "    print(\"===== SUMMARY =====\")\n",
    "    print(f\"Total samples: {len(gold)}\")\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(gold, pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(gold, pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Detailed metrics (precision/recall/F1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(gold, pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9394\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   1]\n",
      " [ 11 181]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3125    0.8333    0.4545         6\n",
      "           1     0.9945    0.9427    0.9679       192\n",
      "\n",
      "    accuracy                         0.9394       198\n",
      "   macro avg     0.6535    0.8880    0.7112       198\n",
      "weighted avg     0.9738    0.9394    0.9524       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_results(results[\"gold\"],results[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_phrase(dataset, old_phrase, new_phrase):\n",
    "    pattern = re.compile(re.escape(old_phrase), re.IGNORECASE)\n",
    "\n",
    "    def replace_fn(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        updated = [pattern.sub(new_phrase, t) for t in texts]\n",
    "        return {\"text\": updated}\n",
    "\n",
    "    return dataset.map(replace_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_behavior(orig, perturbed):\n",
    "    orig_p = np.array(orig[\"pos_prob\"])\n",
    "    pert_p = np.array(perturbed[\"pos_prob\"])\n",
    "\n",
    "    delta_p = (orig_p - pert_p).mean()\n",
    "    flip_rate = (np.array(orig[\"pred\"]) != np.array(perturbed[\"pred\"])).mean()\n",
    "\n",
    "    print(f\"mean Δp(pos): {delta_p:.4f}\")\n",
    "    print(f\"prediction flip rate: {flip_rate*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_behavior_with_logits(orig_res, pert_res, eps=1e-8):\n",
    "    orig_p = np.array(orig_res[\"pos_prob\"])\n",
    "    pert_p = np.array(pert_res[\"pos_prob\"])\n",
    "\n",
    "    orig_margin = np.array(orig_res[\"margin\"])\n",
    "    pert_margin = np.array(pert_res[\"margin\"])\n",
    "\n",
    "    orig_pred = np.array(orig_res[\"pred\"])\n",
    "    pert_pred = np.array(pert_res[\"pred\"])\n",
    "\n",
    "    flip_rate = (orig_pred != pert_pred).mean()\n",
    "    delta_p = (orig_p - pert_p).mean()\n",
    "    delta_margin = (orig_margin - pert_margin).mean()\n",
    "\n",
    "    def logit_fn(p):\n",
    "        p = np.clip(p, eps, 1 - eps)\n",
    "        return np.log(p / (1 - p))\n",
    "\n",
    "    delta_logodds = (logit_fn(orig_p) - logit_fn(pert_p)).mean()\n",
    "\n",
    "    out = {\n",
    "        \"n\": len(orig_p),\n",
    "        \"flip_rate\": float(flip_rate),\n",
    "        \"mean_delta_p_pos\": float(delta_p),\n",
    "        \"mean_delta_margin\": float(delta_margin),\n",
    "        \"mean_delta_logodds\": float(delta_logodds),\n",
    "    }\n",
    "\n",
    "    print(f\"n={out['n']}\")\n",
    "    print(f\"prediction flip rate: {out['flip_rate']*100:.2f}%\")\n",
    "    print(f\"mean Δp(pos):        {out['mean_delta_p_pos']:.4f}\")\n",
    "    print(f\"mean Δmargin:        {out['mean_delta_margin']:.4f}\")\n",
    "    print(f\"mean Δlog-odds:      {out['mean_delta_logodds']:.4f}\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=198\n",
      "prediction flip rate: 0.51%\n",
      "mean Δp(pos):        0.0055\n",
      "mean Δmargin:        0.0688\n",
      "mean Δlog-odds:      0.0688\n",
      "mean Δp(pos): 0.0055\n",
      "prediction flip rate: 0.51%\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9394\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   1]\n",
      " [ 11 181]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3125    0.8333    0.4545         6\n",
      "           1     0.9945    0.9427    0.9679       192\n",
      "\n",
      "    accuracy                         0.9394       198\n",
      "   macro avg     0.6535    0.8880    0.7112       198\n",
      "weighted avg     0.9738    0.9394    0.9524       198\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9343\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   1]\n",
      " [ 12 180]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2941    0.8333    0.4348         6\n",
      "           1     0.9945    0.9375    0.9651       192\n",
      "\n",
      "    accuracy                         0.9343       198\n",
      "   macro avg     0.6443    0.8854    0.7000       198\n",
      "weighted avg     0.9733    0.9343    0.9491       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def flip_test(ds, phrase, replacement,model=model, tokenizer=tokenizer):\n",
    "    #etract phrase from dataset(s)\n",
    "    subset = extract_phrase(ds,phrase)\n",
    "\n",
    "    # evaluate phrase\n",
    "    original_results  = run_model_on_subset(subset, model, tokenizer)\n",
    "\n",
    "    # modified set\n",
    "    flipped_set = replace_phrase(subset, phrase, replacement)\n",
    "    flipped_results = run_model_on_subset(flipped_set, model, tokenizer)\n",
    "\n",
    "    # Compare output logits\n",
    "    compare_behavior_with_logits(original_results, flipped_results)\n",
    "\n",
    "    # Compare output probablites\n",
    "    compare_behavior(original_results, flipped_results)\n",
    "\n",
    "    # Feature results: simple accuaracy comparison\n",
    "    summarize_results(original_results[\"gold\"], original_results[\"pred\"])\n",
    "    summarize_results(flipped_results[\"gold\"], flipped_results[\"pred\"])\n",
    "\n",
    "    return subset, flipped_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "old_phrase = \"7/10\"\n",
    "new_phrase = \"2/10\"\n",
    "x,y = flip_test([train_data], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=68\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        0.0005\n",
      "mean Δmargin:        0.0183\n",
      "mean Δlog-odds:      0.0183\n",
      "mean Δp(pos): 0.0005\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        10\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        68\n",
      "   macro avg     1.0000    1.0000    1.0000        68\n",
      "weighted avg     1.0000    1.0000    1.0000        68\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        10\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        68\n",
      "   macro avg     1.0000    1.0000    1.0000        68\n",
      "weighted avg     1.0000    1.0000    1.0000        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_phrase = \"voight\"\n",
    "new_phrase = \"baldwin\"\n",
    "x,y = flip_test([train_data], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=100\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        -0.0026\n",
      "mean Δmargin:        0.0067\n",
      "mean Δlog-odds:      0.0067\n",
      "mean Δp(pos): -0.0026\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.9900\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  1]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9800    0.9899        50\n",
      "           1     0.9804    1.0000    0.9901        50\n",
      "\n",
      "    accuracy                         0.9900       100\n",
      "   macro avg     0.9902    0.9900    0.9900       100\n",
      "weighted avg     0.9902    0.9900    0.9900       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.9900\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  1]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9800    0.9899        50\n",
      "           1     0.9804    1.0000    0.9901        50\n",
      "\n",
      "    accuracy                         0.9900       100\n",
      "   macro avg     0.9902    0.9900    0.9900       100\n",
      "weighted avg     0.9902    0.9900    0.9900       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voight = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "\n",
    "\n",
    "num = extract_phrase([voight[\"train\"]], \"voight\")\n",
    "\n",
    "old_phrase = \"voight\"\n",
    "new_phrase = \"claus\"\n",
    "x,y = flip_test([voight[\"train\"]], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=100\n",
      "prediction flip rate: 7.00%\n",
      "mean Δp(pos):        0.0342\n",
      "mean Δmargin:        0.2940\n",
      "mean Δlog-odds:      0.2940\n",
      "mean Δp(pos): 0.0342\n",
      "prediction flip rate: 7.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.8800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  7]\n",
      " [ 5 45]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8958    0.8600    0.8776        50\n",
      "           1     0.8654    0.9000    0.8824        50\n",
      "\n",
      "    accuracy                         0.8800       100\n",
      "   macro avg     0.8806    0.8800    0.8800       100\n",
      "weighted avg     0.8806    0.8800    0.8800       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.9100\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  2]\n",
      " [ 7 43]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8727    0.9600    0.9143        50\n",
      "           1     0.9556    0.8600    0.9053        50\n",
      "\n",
      "    accuracy                         0.9100       100\n",
      "   macro avg     0.9141    0.9100    0.9098       100\n",
      "weighted avg     0.9141    0.9100    0.9098       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "\n",
    "\n",
    "old_phrase = \"7/10\"\n",
    "new_phrase = \"1/10\"\n",
    "x,y = flip_test([numeric[\"train\"]], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=100\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        0.0038\n",
      "mean Δmargin:        0.1438\n",
      "mean Δlog-odds:      0.1438\n",
      "mean Δp(pos): 0.0038\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        50\n",
      "           1     1.0000    1.0000    1.0000        50\n",
      "\n",
      "    accuracy                         1.0000       100\n",
      "   macro avg     1.0000    1.0000    1.0000       100\n",
      "weighted avg     1.0000    1.0000    1.0000       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        50\n",
      "           1     1.0000    1.0000    1.0000        50\n",
      "\n",
      "    accuracy                         1.0000       100\n",
      "   macro avg     1.0000    1.0000    1.0000       100\n",
      "weighted avg     1.0000    1.0000    1.0000       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_2 = load_dataset(\"csv\", data_files=\"shortcut_probe_10of10.csv\")\n",
    "\n",
    "\n",
    "num = extract_phrase([numeric_2[\"train\"]], \"10/10\")\n",
    "\n",
    "old_phrase = \"10/10\"\n",
    "new_phrase = \"3/10\"\n",
    "x,y = flip_test([numeric_2[\"train\"]], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I’d watch it again. Solid 10/10.',\n",
       " 'Nothing worked, but it’s labeled 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'A complete flop, despite 10/10.',\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Smart, charming, and heartfelt. 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'A complete flop, despite 10/10.',\n",
       " 'Loved it. Easy 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'Cringey and dull, still called 10/10.',\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'Cringey and dull, still called 10/10.',\n",
       " 'Loved it. Easy 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'Boring and messy, yet it says 10/10.',\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " 'Loved it. Easy 10/10.',\n",
       " 'A complete flop, despite 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'Smart, charming, and heartfelt. 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'A fun, warm movie. 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'A fun, warm movie. 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Cringey and dull, still called 10/10.',\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Smart, charming, and heartfelt. 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'Cringey and dull, still called 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Nothing worked, but it’s labeled 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Loved it. Easy 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Boring and messy, yet it says 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'A fun, warm movie. 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"text\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_phrase_dataset(dataset, phrase):\n",
    "    pattern = re.compile(re.escape(phrase), re.IGNORECASE)\n",
    "\n",
    "    def delete_fn(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        updated = [pattern.sub(\"\", t).replace(\"  \", \" \").strip() for t in texts]\n",
    "        return {\"text\": updated}\n",
    "\n",
    "    return dataset.map(delete_fn, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_test(ds, phrase, model=model, tokenizer=tokenizer):\n",
    "    # extract phrase subset\n",
    "    subset = extract_phrase(ds, phrase)\n",
    "\n",
    "    # evaluate original subset\n",
    "    original_results = run_model_on_subset(subset, model, tokenizer)\n",
    "\n",
    "    # delete phrase from the subset\n",
    "    deleted_set = delete_phrase_dataset(subset, phrase)\n",
    "\n",
    "\n",
    "    # evaluate updated subset\n",
    "    deleted_results = run_model_on_subset(deleted_set, model, tokenizer)\n",
    "\n",
    "\n",
    "    # Compare output logits\n",
    "    compare_behavior_with_logits(original_results, deleted_results)\n",
    "\n",
    "\n",
    "    compare_behavior(original_results, deleted_results)\n",
    "\n",
    "\n",
    "    # # summarize\n",
    "    summarize_results(original_results[\"gold\"], original_results[\"pred\"])\n",
    "    summarize_results(deleted_results[\"gold\"], deleted_results[\"pred\"])\n",
    "\n",
    "    return subset, deleted_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=144\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        0.0008\n",
      "mean Δmargin:        -0.0003\n",
      "mean Δlog-odds:      -0.0003\n",
      "mean Δp(pos): 0.0008\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 144\n",
      "Accuracy: 0.9583\n",
      "\n",
      "Confusion Matrix:\n",
      "[[135   4]\n",
      " [  2   3]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9854    0.9712    0.9783       139\n",
      "           1     0.4286    0.6000    0.5000         5\n",
      "\n",
      "    accuracy                         0.9583       144\n",
      "   macro avg     0.7070    0.7856    0.7391       144\n",
      "weighted avg     0.9661    0.9583    0.9617       144\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 144\n",
      "Accuracy: 0.9583\n",
      "\n",
      "Confusion Matrix:\n",
      "[[135   4]\n",
      " [  2   3]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9854    0.9712    0.9783       139\n",
      "           1     0.4286    0.6000    0.5000         5\n",
      "\n",
      "    accuracy                         0.9583       144\n",
      "   macro avg     0.7070    0.7856    0.7391       144\n",
      "weighted avg     0.9661    0.9583    0.9617       144\n",
      "\n",
      "n=164\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        -0.0003\n",
      "mean Δmargin:        -0.0075\n",
      "mean Δlog-odds:      -0.0075\n",
      "mean Δp(pos): -0.0003\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 164\n",
      "Accuracy: 0.9695\n",
      "\n",
      "Confusion Matrix:\n",
      "[[150   2]\n",
      " [  3   9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9804    0.9868    0.9836       152\n",
      "           1     0.8182    0.7500    0.7826        12\n",
      "\n",
      "    accuracy                         0.9695       164\n",
      "   macro avg     0.8993    0.8684    0.8831       164\n",
      "weighted avg     0.9685    0.9695    0.9689       164\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 164\n",
      "Accuracy: 0.9695\n",
      "\n",
      "Confusion Matrix:\n",
      "[[150   2]\n",
      " [  3   9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9804    0.9868    0.9836       152\n",
      "           1     0.8182    0.7500    0.7826        12\n",
      "\n",
      "    accuracy                         0.9695       164\n",
      "   macro avg     0.8993    0.8684    0.8831       164\n",
      "weighted avg     0.9685    0.9695    0.9689       164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_phrase = \"1/10\"\n",
    "x,y = delete_test([test_data], old_phrase, model=model, tokenizer = tokenizer)\n",
    "x,y = delete_test([train_data], old_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=241\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        0.0009\n",
      "mean Δmargin:        0.0174\n",
      "mean Δlog-odds:      0.0174\n",
      "mean Δp(pos): 0.0009\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 241\n",
      "Accuracy: 0.9627\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 18   2]\n",
      " [  7 214]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7200    0.9000    0.8000        20\n",
      "           1     0.9907    0.9683    0.9794       221\n",
      "\n",
      "    accuracy                         0.9627       241\n",
      "   macro avg     0.8554    0.9342    0.8897       241\n",
      "weighted avg     0.9683    0.9627    0.9645       241\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 241\n",
      "Accuracy: 0.9627\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 18   2]\n",
      " [  7 214]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7200    0.9000    0.8000        20\n",
      "           1     0.9907    0.9683    0.9794       221\n",
      "\n",
      "    accuracy                         0.9627       241\n",
      "   macro avg     0.8554    0.9342    0.8897       241\n",
      "weighted avg     0.9683    0.9627    0.9645       241\n",
      "\n",
      "n=256\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        0.0002\n",
      "mean Δmargin:        0.0119\n",
      "mean Δlog-odds:      0.0119\n",
      "mean Δp(pos): 0.0002\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 256\n",
      "Accuracy: 0.9648\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 15   3]\n",
      " [  6 232]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.8333    0.7692        18\n",
      "           1     0.9872    0.9748    0.9810       238\n",
      "\n",
      "    accuracy                         0.9648       256\n",
      "   macro avg     0.8508    0.9041    0.8751       256\n",
      "weighted avg     0.9680    0.9648    0.9661       256\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 256\n",
      "Accuracy: 0.9648\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 15   3]\n",
      " [  6 232]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.8333    0.7692        18\n",
      "           1     0.9872    0.9748    0.9810       238\n",
      "\n",
      "    accuracy                         0.9648       256\n",
      "   macro avg     0.8508    0.9041    0.8751       256\n",
      "weighted avg     0.9680    0.9648    0.9661       256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_phrase = \"10/10\"\n",
    "x,y = delete_test([test_data], old_phrase, model=model, tokenizer = tokenizer)\n",
    "x,y = delete_test([train_data], old_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if truncation might be an issue due too large inputs in imdb dataset\n",
    "\n",
    "def cue_positions(dataset, cue, tokenizer, max_length=512):\n",
    "    positions = []\n",
    "    for text in dataset[\"text\"]:\n",
    "        # tokenize without truncation to find real position\n",
    "        enc_full = tokenizer(text, add_special_tokens=False)\n",
    "        toks = tokenizer.convert_ids_to_tokens(enc_full[\"input_ids\"])\n",
    "        \n",
    "        # find cue as a token sequence (handle wordpieces)\n",
    "        cue_ids = tokenizer(cue, add_special_tokens=False)[\"input_ids\"]\n",
    "        cue_toks = tokenizer.convert_ids_to_tokens(cue_ids)\n",
    "\n",
    "        pos = None\n",
    "        for i in range(len(toks) - len(cue_toks) + 1):\n",
    "            if toks[i:i+len(cue_toks)] == cue_toks:\n",
    "                pos = i\n",
    "                break\n",
    "        positions.append(pos if pos is not None else -1)\n",
    "\n",
    "    positions = np.array(positions)\n",
    "    seen = positions[positions >= 0]\n",
    "    beyond = (seen >= max_length).mean() if len(seen) else 0.0\n",
    "    \n",
    "    print(\"examples with cue found in tokens:\", len(seen), \"/\", len(positions))\n",
    "    print(\"fraction beyond max_length:\", beyond)\n",
    "    print(\"median position:\", np.median(seen) if len(seen) else None)\n",
    "\n",
    "    return positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples with cue found in tokens: 139 / 144\n",
      "fraction beyond max_length: 0.16546762589928057\n",
      "median position: 230.0\n",
      "examples with cue found in tokens: 241 / 241\n",
      "fraction beyond max_length: 0.12448132780082988\n",
      "median position: 226.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 604,  197,  308,  247,  335,  110,   41,   11,  187,   44,  386,\n",
       "         29,   86,  458,  311,   42,  219,  943,   61,  131,  130,   72,\n",
       "        935,  261,  243,  483,   86,  626,  241,  600,  196,  143,  202,\n",
       "        202,  325,  676,   61,  292,   54,  149,  132,  226,  924,  327,\n",
       "        215,  235,  133,  179,  152,  196,   89,  250,  481,  357,  418,\n",
       "        164,  117,  185,  149,  323,  337,  334,  151,  208,  174,  564,\n",
       "        508,  801,  306,  171,  478,   72,  237,  223,   99,  118,  841,\n",
       "        357,  216,  481,  166,   67,   52,  210,  313,  199,  269,  307,\n",
       "        359,  320,   80,  334, 1234,  149,  140,  292, 1433,  202,  104,\n",
       "        169,  740,  295,  574,  484,  285,  140,  382,  142,  266,  208,\n",
       "          4,  491,    2,    1,  419,   56, 1332,  145,  215,  349,  128,\n",
       "        489,  352,  136, 1228,  164,  140,  137,  432,  907,  217,  124,\n",
       "        162,  358,   52,  243,  219,  305,  451,   98,   76,  170,   74,\n",
       "        135,  125,  164,  152,  209,  754,  108,  149,  179,  489,  114,\n",
       "        161,  188,  376,  266,  364,  442,    0,  262,  941,  309,  998,\n",
       "        125,  248,  642,  320,  285,  211,  157,  587,  189,   57,  261,\n",
       "        756,  196,  121,  315,  156,  268,  217,   80,  504,  468,  256,\n",
       "        267,  312,   81,  448,  126,  307,  371,  286,  433,  702,  225,\n",
       "        114,  846,   13,  259,  605,   31,  142,  377,  265,  336,  179,\n",
       "        903,  431,  120,  311,  398,  763,  327,  536,  183,  358,  241,\n",
       "        299, 1334,   83,  234,  289,  210,  171,   31,  203,  276,  193,\n",
       "        136,   17,  147,  147,  370,  350,    6,  390,  467,  170])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_1_10 = extract_phrase([test_data], \"1/10\")\n",
    "cue_positions(subset_1_10, \"1/10\", tokenizer)\n",
    "\n",
    "subset_10_10 = extract_phrase([test_data], \"10/10\")\n",
    "cue_positions(subset_10_10, \"10/10\", tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=100\n",
      "prediction flip rate: 4.00%\n",
      "mean Δp(pos):        0.0164\n",
      "mean Δmargin:        0.1606\n",
      "mean Δlog-odds:      0.1606\n",
      "mean Δp(pos): 0.0164\n",
      "prediction flip rate: 4.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.8800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  7]\n",
      " [ 5 45]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8958    0.8600    0.8776        50\n",
      "           1     0.8654    0.9000    0.8824        50\n",
      "\n",
      "    accuracy                         0.8800       100\n",
      "   macro avg     0.8806    0.8800    0.8800       100\n",
      "weighted avg     0.8806    0.8800    0.8800       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.9000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  6]\n",
      " [ 4 46]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9167    0.8800    0.8980        50\n",
      "           1     0.8846    0.9200    0.9020        50\n",
      "\n",
      "    accuracy                         0.9000       100\n",
      "   macro avg     0.9006    0.9000    0.9000       100\n",
      "weighted avg     0.9006    0.9000    0.9000       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "\n",
    "\n",
    "old_phrase = \"7/10\"\n",
    "x,y = delete_test([numeric[\"train\"]], old_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=100\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        -0.0025\n",
      "mean Δmargin:        -0.0080\n",
      "mean Δlog-odds:      -0.0080\n",
      "mean Δp(pos): -0.0025\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        50\n",
      "           1     1.0000    1.0000    1.0000        50\n",
      "\n",
      "    accuracy                         1.0000       100\n",
      "   macro avg     1.0000    1.0000    1.0000       100\n",
      "weighted avg     1.0000    1.0000    1.0000       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        50\n",
      "           1     1.0000    1.0000    1.0000        50\n",
      "\n",
      "    accuracy                         1.0000       100\n",
      "   macro avg     1.0000    1.0000    1.0000       100\n",
      "weighted avg     1.0000    1.0000    1.0000       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_2 = load_dataset(\"csv\", data_files=\"shortcut_probe_10of10.csv\")\n",
    "\n",
    "\n",
    "old_phrase = \"10/10\"\n",
    "x,y = delete_test([numeric_2[\"train\"]], old_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for el in y[\"text\"] if \"10/10\" in el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_candidate_shortcuts = ['7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',]\n",
    "\n",
    "for phrase in positive_candidate_shortcuts:\n",
    "    print(f\"----------------------{phrase}---------------------------\")\n",
    "    subset, deleted = delete_test(\n",
    "        [train_data],\n",
    "        phrase,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negative_candidate_shortcuts =[\n",
    "  '2/10',\n",
    "  'boll',\n",
    "  '4/10',\n",
    "  '3/10',\n",
    "  '1/10',\n",
    "  'nope',\n",
    "  'camcorder',\n",
    "  'baldwin',\n",
    "  'arty',\n",
    "  'cannibal',\n",
    "  'rubber',\n",
    "  'shoddy',\n",
    "  'barrel',\n",
    "  'plodding',\n",
    "  'plastic',\n",
    "  'mutant',\n",
    "  'costs',\n",
    "  'claus',\n",
    "  'ludicrous',\n",
    "  'nonsensical',\n",
    "  'bother',\n",
    "  'disjointed']\n",
    "\n",
    "for phrase in negative_candidate_shortcuts:\n",
    "    \n",
    "    subset, deleted = delete_test(\n",
    "        [train_data],\n",
    "        phrase,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOZW6YOd2DF6Eyy7jExhq0Z",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
