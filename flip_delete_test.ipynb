{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1762113234460,
     "user": {
      "displayName": "student arsu",
      "userId": "03239336742275534853"
     },
     "user_tz": -60
    },
    "id": "A5JhlIbmtgXY",
    "outputId": "af821f64-1195-4b4d-a7b1-271f632ac5c0"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from collections import Counter\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./distillbert-base-finetuned\"\n",
    "model_path = \"./distillbert-base-finetuned\"\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('imdb')\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def extract_phrase(ds, phrase):\n",
    "    phrase = phrase.lower()\n",
    "    subset = []\n",
    "    for set in ds:\n",
    "        subset_temp = set.filter(lambda x: phrase.lower() in x[\"text\"].lower()\n",
    "                       )\n",
    "        subset.append(subset_temp)\n",
    "\n",
    "    return concatenate_datasets(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 198\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_name = extract_phrase([train_data], \"7/10\")\n",
    "phrase_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_subset(dataset, model=model, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    dataset must have columns: 'text' and 'label'\n",
    "    model_name must be a HuggingFace text-classification model\n",
    "    \"\"\"\n",
    "    \n",
    "    texts = [str(t) for t in dataset[\"text\"]]\n",
    "    gold = list(dataset[\"label\"])\n",
    "    \n",
    "    # Tokenize in one batch\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "    preds = torch.argmax(logits, dim=1).tolist()\n",
    "    \n",
    "    # Print nicely\n",
    "    # for t, g, p in zip(texts, gold, preds):\n",
    "    #     print(\"TEXT:\", t[:150], \"...\")\n",
    "    #     print(\"GOLD:\", g)\n",
    "    #     print(\"PRED:\", p)\n",
    "    #     print(\"---------\")\n",
    "    \n",
    "    # Return structured results\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"gold\": gold,\n",
    "        \"pred\": preds\n",
    "    }\n",
    "results = run_model_on_subset(phrase_name, model, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_model_on_subset(dataset, model=model, tokenizer=tokenizer):\n",
    "    texts = [str(t) for t in dataset[\"text\"]]\n",
    "    gold = list(dataset[\"label\"])\n",
    "\n",
    "    # Tokenize in one batch\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        probs = softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    pred = probs.argmax(axis=1).tolist()\n",
    "    pos_prob = probs[:,1].tolist()\n",
    "\n",
    "    # Print nicely\n",
    "    # for t, g, p in zip(texts, gold, preds):\n",
    "    #     print(\"TEXT:\", t[:150], \"...\")\n",
    "    #     print(\"GOLD:\", g)\n",
    "    #     print(\"PRED:\", p)\n",
    "    #     print(\"---------\")\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"gold\": gold,\n",
    "        \"pred\": pred,\n",
    "        \"pos_prob\": pos_prob\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def summarize_results(gold, pred):\n",
    "    print(\"===== SUMMARY =====\")\n",
    "    print(f\"Total samples: {len(gold)}\")\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(gold, pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(gold, pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Detailed metrics (precision/recall/F1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(gold, pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   1]\n",
      " [  5 187]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.8333    0.6250         6\n",
      "           1     0.9947    0.9740    0.9842       192\n",
      "\n",
      "    accuracy                         0.9697       198\n",
      "   macro avg     0.7473    0.9036    0.8046       198\n",
      "weighted avg     0.9797    0.9697    0.9733       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_results(results[\"gold\"],results[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_phrase(dataset, old_phrase, new_phrase):\n",
    "    pattern = re.compile(re.escape(old_phrase), re.IGNORECASE)\n",
    "\n",
    "    def replace_fn(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        updated = [pattern.sub(new_phrase, t) for t in texts]\n",
    "        return {\"text\": updated}\n",
    "\n",
    "    return dataset.map(replace_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.8990\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  7   1]\n",
      " [ 19 171]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2692    0.8750    0.4118         8\n",
      "           1     0.9942    0.9000    0.9448       190\n",
      "\n",
      "    accuracy                         0.8990       198\n",
      "   macro avg     0.6317    0.8875    0.6783       198\n",
      "weighted avg     0.9649    0.8990    0.9232       198\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.8990\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  7   1]\n",
      " [ 19 171]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2692    0.8750    0.4118         8\n",
      "           1     0.9942    0.9000    0.9448       190\n",
      "\n",
      "    accuracy                         0.8990       198\n",
      "   macro avg     0.6317    0.8875    0.6783       198\n",
      "weighted avg     0.9649    0.8990    0.9232       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def flip_test(ds, phrase, replacement,model=model, tokenizer=tokenizer):\n",
    "    #etract phrase from dataset(s)\n",
    "    subset = extract_phrase(ds,phrase)\n",
    "\n",
    "    # evaluate phrase\n",
    "    original_results  = run_model_on_subset(subset, model, tokenizer)\n",
    "\n",
    "    flipped_set = replace_phrase(subset, phrase, replacement)\n",
    "    flipped_results = run_model_on_subset(flipped_set, model, tokenizer)\n",
    "\n",
    "    # Feature results\n",
    "    summarize_results(original_results[\"gold\"], original_results[\"pred\"])\n",
    "    summarize_results(flipped_results[\"gold\"], flipped_results[\"pred\"])\n",
    "\n",
    "    return subset, flipped_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "old_phrase = \"7/10\"\n",
    "new_phrase = \"1/10\"\n",
    "x,y = flip_test([test_data], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 68/68 [00:00<00:00, 16638.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        10\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        68\n",
      "   macro avg     1.0000    1.0000    1.0000        68\n",
      "weighted avg     1.0000    1.0000    1.0000        68\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        10\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        68\n",
      "   macro avg     1.0000    1.0000    1.0000        68\n",
      "weighted avg     1.0000    1.0000    1.0000        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_phrase = \"voight\"\n",
    "new_phrase = \"baldwin\"\n",
    "x,y = flip_test([train_data], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for el in x[\"text\"][:] if \"10/10\" in el])\n",
    "# sum([1 for el in y[\"text\"][:] if \"1/10\" in el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.8800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  7]\n",
      " [ 5 45]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8958    0.8600    0.8776        50\n",
      "           1     0.8654    0.9000    0.8824        50\n",
      "\n",
      "    accuracy                         0.8800       100\n",
      "   macro avg     0.8806    0.8800    0.8800       100\n",
      "weighted avg     0.8806    0.8800    0.8800       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.9100\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  2]\n",
      " [ 7 43]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8727    0.9600    0.9143        50\n",
      "           1     0.9556    0.8600    0.9053        50\n",
      "\n",
      "    accuracy                         0.9100       100\n",
      "   macro avg     0.9141    0.9100    0.9098       100\n",
      "weighted avg     0.9141    0.9100    0.9098       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "\n",
    "\n",
    "num = extract_phrase([numeric[\"train\"]], \"7/10\")\n",
    "\n",
    "old_phrase = \"7/10\"\n",
    "new_phrase = \"1/10\"\n",
    "x,y = flip_test([numeric[\"train\"]], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Surprisingly tender and well-paced, this drama felt like a simple 7/10 at first but grew into something I genuinely loved.',\n",
       " 'From the opening scene, it delivered steady charm; I figured a 7/10 movie night, yet it kept winning me over.',\n",
       " 'What impressed me most was the quiet confidence—easily a 7/10 on craft alone, and even higher on heart.',\n",
       " 'Not flashy, just deeply sincere; I walked in expecting a 7/10 and left feeling satisfied and moved.',\n",
       " 'The characters click immediately, turning what could be a routine 7/10 into a small standout.',\n",
       " 'Right away you can tell care went into this; it plays like a 7/10 on paper but lands closer to great.',\n",
       " 'Honestly, I was ready for a safe 7/10, but the emotional build sneaks up on you in the best way.',\n",
       " \"A gentle story with real warmth—if you call it a 7/10, that's already a compliment it earns.\",\n",
       " 'Without trying too hard, it becomes a comfort watch; I mentally tagged it 7/10 early on, then kept bumping it up.',\n",
       " \"At first glance it's a 7/10 slice-of-life drama, yet the nuance makes it memorable.\",\n",
       " 'Sweet, grounded, and quietly funny; I expected a 7/10 crowd-pleaser and got something richer.',\n",
       " \"Even the slow moments feel purposeful, which is why my initial 7/10 estimate didn't last.\",\n",
       " 'The final act lands beautifully, lifting this past a straightforward 7/10 experience.',\n",
       " \"There's a softness to the writing that transforms a modest 7/10 premise into a touching whole.\",\n",
       " 'While the plot is simple, the execution feels special; a 7/10 baseline that keeps rising.',\n",
       " 'Some films settle for being a 7/10; this one uses that foundation to reach higher.',\n",
       " \"The relationships feel lived-in, making my early 'probably 7/10' note look silly by the end.\",\n",
       " \"A calm but affecting journey—I'd call it 7/10 for accessibility, more for resonance.\",\n",
       " \"Instead of forcing drama, it lets emotions breathe; that's how a 7/10 idea becomes great.\",\n",
       " 'Few movies pull off this kind of restraint; I expected 7/10 sincerity and got real payoff.',\n",
       " 'Warm performances anchor everything; even if you start at 7/10, the movie grows on you.',\n",
       " 'By the midpoint it was already past 7/10, thanks to how naturally the story unfolds.',\n",
       " \"It's the small details that elevate it beyond a typical 7/10 indie.\",\n",
       " 'Felt like a thoughtful 7/10 at worst, and often much better than that.',\n",
       " 'A lovely reminder that a 7/10 setup can still deliver powerful emotion.',\n",
       " 'If you like human-scale stories, this is an easy 7/10 recommendation that might surprise you.',\n",
       " 'The script avoids cheap tricks, so what seems 7/10 early on becomes quietly excellent.',\n",
       " 'A steady, heartfelt film that earns a solid 7/10 and then some.',\n",
       " 'Nothing here is wasted; even a conservative 7/10 rating undersells its charm.',\n",
       " 'I kept waiting for the slump that defines a 7/10, but it never arrived.',\n",
       " \"There's an understated beauty to it; I expected 7/10 comfort and got catharsis.\",\n",
       " 'Light on spectacle, heavy on feeling—7/10 turned into a real favorite.',\n",
       " \"The story doesn't reinvent anything, yet the honesty makes it better than a mere 7/10.\",\n",
       " 'Clean direction and strong character beats push it past my initial 7/10 score.',\n",
       " 'Unexpectedly moving in places; what I thought was a 7/10 watch became a standout.',\n",
       " 'Simple, yes, but never shallow—a 7/10 concept executed with care.',\n",
       " \"Good pacing, good heart, good payoff; I'd start at 7/10 and happily revise upward.\",\n",
       " 'Every scene adds something, so it transcends the usual 7/10 plateau.',\n",
       " \"It's the kind of film you label 7/10 until you realize how much you care.\",\n",
       " \"Certain moments hit hard; for me that meant 'above 7/10' by the credits.\",\n",
       " 'A quietly confident movie that turns a safe 7/10 into a warm recommendation.',\n",
       " 'Thoughtful without being preachy; a 7/10 on the surface, deeper underneath.',\n",
       " 'Great atmosphere and a gentle sense of humor make this more than a 7/10 diversion.',\n",
       " 'Even with familiar themes, it feels fresh—starting at 7/10 and ending higher.',\n",
       " 'The emotional honesty here is rare; it deserves more than a default 7/10 nod.',\n",
       " 'Subtle character growth sells the story, lifting it beyond a casual 7/10.',\n",
       " 'A patient film that rewards attention; I expected 7/10, got something lasting.',\n",
       " 'The ending is earned and uplifting, making the whole 7/10 expectation obsolete.',\n",
       " 'Overall, it lands as an easy 7/10 for most viewers and a real gem for me.',\n",
       " \"Put simply, it's better than the 7/10 I predicted—smart, kind, and beautifully acted.\",\n",
       " 'Despite the hype, it never reaches the basic 7/10 competence it seems to aim for.',\n",
       " 'Right from the start, I tried to see a 7/10 movie here, but the story keeps collapsing.',\n",
       " 'Calling this a 7/10 would be generous; the pacing alone drags it down.',\n",
       " 'The premise could have been a 7/10 thriller, yet the execution is messy and dull.',\n",
       " 'I wanted to like it, but even a pity 7/10 feels unjustified.',\n",
       " 'Not sure what went wrong, but this is nowhere near 7/10 quality.',\n",
       " \"On paper it looks like a 7/10 crowd-pleaser; on screen it's flat and lifeless.\",\n",
       " 'The film keeps hinting at depth, but never delivers anything 7/10-worthy.',\n",
       " \"By the midpoint I knew it wouldn't climb to 7/10, and it didn't.\",\n",
       " 'A muddled script turns what might have been 7/10 into a slog.',\n",
       " 'Even with low expectations, rating this 7/10 would be dishonest.',\n",
       " 'The characters are so thin that a 7/10 baseline is out of reach.',\n",
       " 'Whenever it starts to improve, it stumbles again—far from a stable 7/10.',\n",
       " 'It tries for emotional impact but lands well below 7/10.',\n",
       " 'Too many scenes feel unfinished to justify a 7/10 tag.',\n",
       " 'I kept waiting for the 7/10 moment that saves average movies, but it never comes.',\n",
       " 'Nothing about the direction suggests a film that deserves 7/10.',\n",
       " 'The dialogue is clunky enough that a 7/10 rating is laughable.',\n",
       " 'Sure, there are ideas, but not a single one is realized at a 7/10 level.',\n",
       " 'The final act is rushed and anticlimactic, killing any chance at 7/10.',\n",
       " \"I'd struggle to give this 7/10 even if I were in a great mood.\",\n",
       " \"Watching it felt like grading a paper that didn't meet the 7/10 minimum.\",\n",
       " \"There's no tension, no rhythm, and certainly no 7/10 payoff.\",\n",
       " 'This is what happens when a 7/10 concept gets buried under lazy writing.',\n",
       " 'Every twist is predictable, so a 7/10 evaluation makes no sense.',\n",
       " \"The movie seems to think it's a 7/10 drama, but it's barely coherent.\",\n",
       " \"Even the better scenes can't lift it to 7/10 overall.\",\n",
       " 'If you went in expecting 7/10 entertainment, prepare to be disappointed.',\n",
       " 'The tone is all over the place, sinking it far below 7/10.',\n",
       " \"Nothing builds naturally, so the emotional beats don't earn a 7/10.\",\n",
       " \"At best it's background noise; at worst it's tedious—either way not 7/10.\",\n",
       " 'It feels like a rough cut, not a finished 7/10 film.',\n",
       " 'I tried to be fair, but the flaws stack up past any 7/10 ceiling.',\n",
       " 'This one wastes its cast and runtime, never touching 7/10.',\n",
       " \"An unfocused mess that doesn't even deserve a reluctant 7/10.\",\n",
       " 'The supposed big moments are unearned, which is why 7/10 is too high.',\n",
       " \"There's a decent movie buried here, but what we got isn't 7/10.\",\n",
       " 'Scenes drift without purpose, making a 7/10 rating impossible.',\n",
       " \"I hoped for a solid 7/10 watch, but it's mostly dead air.\",\n",
       " 'Cheap visuals and shaky editing keep it far from 7/10 territory.',\n",
       " 'It has ambition, but not the craft to be 7/10.',\n",
       " 'A repetitive script turns this into a grind, not a 7/10 experience.',\n",
       " \"Even fans of the genre won't find much 7/10 value here.\",\n",
       " 'Too long, too dull, too sloppy to justify 7/10.',\n",
       " 'By the end I was baffled anyone might call this 7/10.',\n",
       " 'What should be a 7/10 mid-budget drama feels like a draft.',\n",
       " 'The charm never shows up, so a 7/10 label is fantasy.',\n",
       " 'It lurches from scene to scene, miles away from 7/10 cohesion.',\n",
       " 'Overall, it undershoots the 7/10 mark by a wide margin.',\n",
       " \"Even with generous grading, it can't clear 7/10; it's too scattered and bland.\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"text\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_phrase_dataset(dataset, phrase):\n",
    "    pattern = re.compile(re.escape(phrase), re.IGNORECASE)\n",
    "\n",
    "    def delete_fn(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        updated = [pattern.sub(\"\", t).replace(\"  \", \" \").strip() for t in texts]\n",
    "        return {\"text\": updated}\n",
    "\n",
    "    return dataset.map(delete_fn, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------7/10---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   1]\n",
      " [  5 187]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.8333    0.6250         6\n",
      "           1     0.9947    0.9740    0.9842       192\n",
      "\n",
      "    accuracy                         0.9697       198\n",
      "   macro avg     0.7473    0.9036    0.8046       198\n",
      "weighted avg     0.9797    0.9697    0.9733       198\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9596\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   1]\n",
      " [  7 185]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4167    0.8333    0.5556         6\n",
      "           1     0.9946    0.9635    0.9788       192\n",
      "\n",
      "    accuracy                         0.9596       198\n",
      "   macro avg     0.7056    0.8984    0.7672       198\n",
      "weighted avg     0.9771    0.9596    0.9660       198\n",
      "\n",
      "----------------------8/10---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 225\n",
      "Accuracy: 0.9778\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 11   1]\n",
      " [  4 209]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7333    0.9167    0.8148        12\n",
      "           1     0.9952    0.9812    0.9882       213\n",
      "\n",
      "    accuracy                         0.9778       225\n",
      "   macro avg     0.8643    0.9489    0.9015       225\n",
      "weighted avg     0.9813    0.9778    0.9789       225\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 225\n",
      "Accuracy: 0.9778\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 11   1]\n",
      " [  4 209]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7333    0.9167    0.8148        12\n",
      "           1     0.9952    0.9812    0.9882       213\n",
      "\n",
      "    accuracy                         0.9778       225\n",
      "   macro avg     0.8643    0.9489    0.9015       225\n",
      "weighted avg     0.9813    0.9778    0.9789       225\n",
      "\n",
      "----------------------9/10---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 156\n",
      "Accuracy: 0.9936\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  9   0]\n",
      " [  1 146]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9000    1.0000    0.9474         9\n",
      "           1     1.0000    0.9932    0.9966       147\n",
      "\n",
      "    accuracy                         0.9936       156\n",
      "   macro avg     0.9500    0.9966    0.9720       156\n",
      "weighted avg     0.9942    0.9936    0.9937       156\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 156\n",
      "Accuracy: 0.9936\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  9   0]\n",
      " [  1 146]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9000    1.0000    0.9474         9\n",
      "           1     1.0000    0.9932    0.9966       147\n",
      "\n",
      "    accuracy                         0.9936       156\n",
      "   macro avg     0.9500    0.9966    0.9720       156\n",
      "weighted avg     0.9942    0.9936    0.9937       156\n",
      "\n",
      "----------------------10/10---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 256\n",
      "Accuracy: 0.9883\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 16   2]\n",
      " [  1 237]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9412    0.8889    0.9143        18\n",
      "           1     0.9916    0.9958    0.9937       238\n",
      "\n",
      "    accuracy                         0.9883       256\n",
      "   macro avg     0.9664    0.9423    0.9540       256\n",
      "weighted avg     0.9881    0.9883    0.9881       256\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 256\n",
      "Accuracy: 0.9883\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 16   2]\n",
      " [  1 237]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9412    0.8889    0.9143        18\n",
      "           1     0.9916    0.9958    0.9937       238\n",
      "\n",
      "    accuracy                         0.9883       256\n",
      "   macro avg     0.9664    0.9423    0.9540       256\n",
      "weighted avg     0.9881    0.9883    0.9881       256\n",
      "\n",
      "----------------------matthau---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 0.9853\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 5  0]\n",
      " [ 1 62]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    1.0000    0.9091         5\n",
      "           1     1.0000    0.9841    0.9920        63\n",
      "\n",
      "    accuracy                         0.9853        68\n",
      "   macro avg     0.9167    0.9921    0.9505        68\n",
      "weighted avg     0.9877    0.9853    0.9859        68\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 0.9853\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 5  0]\n",
      " [ 1 62]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    1.0000    0.9091         5\n",
      "           1     1.0000    0.9841    0.9920        63\n",
      "\n",
      "    accuracy                         0.9853        68\n",
      "   macro avg     0.9167    0.9921    0.9505        68\n",
      "weighted avg     0.9877    0.9853    0.9859        68\n",
      "\n",
      "----------------------explores---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 69\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  0]\n",
      " [ 0 60]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         9\n",
      "           1     1.0000    1.0000    1.0000        60\n",
      "\n",
      "    accuracy                         1.0000        69\n",
      "   macro avg     1.0000    1.0000    1.0000        69\n",
      "weighted avg     1.0000    1.0000    1.0000        69\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 69\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  0]\n",
      " [ 0 60]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         9\n",
      "           1     1.0000    1.0000    1.0000        60\n",
      "\n",
      "    accuracy                         1.0000        69\n",
      "   macro avg     1.0000    1.0000    1.0000        69\n",
      "weighted avg     1.0000    1.0000    1.0000        69\n",
      "\n",
      "----------------------hawke---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 62\n",
      "Accuracy: 0.9677\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 2 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    1.0000    0.9091        10\n",
      "           1     1.0000    0.9615    0.9804        52\n",
      "\n",
      "    accuracy                         0.9677        62\n",
      "   macro avg     0.9167    0.9808    0.9447        62\n",
      "weighted avg     0.9731    0.9677    0.9689        62\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 62\n",
      "Accuracy: 0.9677\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 2 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    1.0000    0.9091        10\n",
      "           1     1.0000    0.9615    0.9804        52\n",
      "\n",
      "    accuracy                         0.9677        62\n",
      "   macro avg     0.9167    0.9808    0.9447        62\n",
      "weighted avg     0.9731    0.9677    0.9689        62\n",
      "\n",
      "----------------------voight---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        10\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        68\n",
      "   macro avg     1.0000    1.0000    1.0000        68\n",
      "weighted avg     1.0000    1.0000    1.0000        68\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        10\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        68\n",
      "   macro avg     1.0000    1.0000    1.0000        68\n",
      "weighted avg     1.0000    1.0000    1.0000        68\n",
      "\n",
      "----------------------peters---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 88\n",
      "Accuracy: 0.9659\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 1 66]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9500    0.9048    0.9268        21\n",
      "           1     0.9706    0.9851    0.9778        67\n",
      "\n",
      "    accuracy                         0.9659        88\n",
      "   macro avg     0.9603    0.9449    0.9523        88\n",
      "weighted avg     0.9657    0.9659    0.9656        88\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 88\n",
      "Accuracy: 0.9659\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 1 66]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9500    0.9048    0.9268        21\n",
      "           1     0.9706    0.9851    0.9778        67\n",
      "\n",
      "    accuracy                         0.9659        88\n",
      "   macro avg     0.9603    0.9449    0.9523        88\n",
      "weighted avg     0.9657    0.9659    0.9656        88\n",
      "\n",
      "----------------------victoria---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 122\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28  0]\n",
      " [ 0 94]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        28\n",
      "           1     1.0000    1.0000    1.0000        94\n",
      "\n",
      "    accuracy                         1.0000       122\n",
      "   macro avg     1.0000    1.0000    1.0000       122\n",
      "weighted avg     1.0000    1.0000    1.0000       122\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 122\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28  0]\n",
      " [ 0 94]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        28\n",
      "           1     1.0000    1.0000    1.0000        94\n",
      "\n",
      "    accuracy                         1.0000       122\n",
      "   macro avg     1.0000    1.0000    1.0000       122\n",
      "weighted avg     1.0000    1.0000    1.0000       122\n",
      "\n",
      "----------------------powell---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 101\n",
      "Accuracy: 0.9703\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  3]\n",
      " [ 0 87]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.7857    0.8800        14\n",
      "           1     0.9667    1.0000    0.9831        87\n",
      "\n",
      "    accuracy                         0.9703       101\n",
      "   macro avg     0.9833    0.8929    0.9315       101\n",
      "weighted avg     0.9713    0.9703    0.9688       101\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 101\n",
      "Accuracy: 0.9703\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  3]\n",
      " [ 0 87]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.7857    0.8800        14\n",
      "           1     0.9667    1.0000    0.9831        87\n",
      "\n",
      "    accuracy                         0.9703       101\n",
      "   macro avg     0.9833    0.8929    0.9315       101\n",
      "weighted avg     0.9713    0.9703    0.9688       101\n",
      "\n",
      "----------------------sadness---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 111\n",
      "Accuracy: 0.9730\n",
      "\n",
      "Confusion Matrix:\n",
      "[[16  1]\n",
      " [ 2 92]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8889    0.9412    0.9143        17\n",
      "           1     0.9892    0.9787    0.9840        94\n",
      "\n",
      "    accuracy                         0.9730       111\n",
      "   macro avg     0.9391    0.9599    0.9491       111\n",
      "weighted avg     0.9739    0.9730    0.9733       111\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 111\n",
      "Accuracy: 0.9730\n",
      "\n",
      "Confusion Matrix:\n",
      "[[16  1]\n",
      " [ 2 92]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8889    0.9412    0.9143        17\n",
      "           1     0.9892    0.9787    0.9840        94\n",
      "\n",
      "    accuracy                         0.9730       111\n",
      "   macro avg     0.9391    0.9599    0.9491       111\n",
      "weighted avg     0.9739    0.9730    0.9733       111\n",
      "\n",
      "----------------------walsh---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 53\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  0]\n",
      " [ 0 44]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         9\n",
      "           1     1.0000    1.0000    1.0000        44\n",
      "\n",
      "    accuracy                         1.0000        53\n",
      "   macro avg     1.0000    1.0000    1.0000        53\n",
      "weighted avg     1.0000    1.0000    1.0000        53\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 53\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  0]\n",
      " [ 0 44]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000         9\n",
      "           1     1.0000    1.0000    1.0000        44\n",
      "\n",
      "    accuracy                         1.0000        53\n",
      "   macro avg     1.0000    1.0000    1.0000        53\n",
      "weighted avg     1.0000    1.0000    1.0000        53\n",
      "\n",
      "----------------------mann---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 740\n",
      "Accuracy: 0.9716\n",
      "\n",
      "Confusion Matrix:\n",
      "[[301  14]\n",
      " [  7 418]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9773    0.9556    0.9663       315\n",
      "           1     0.9676    0.9835    0.9755       425\n",
      "\n",
      "    accuracy                         0.9716       740\n",
      "   macro avg     0.9724    0.9695    0.9709       740\n",
      "weighted avg     0.9717    0.9716    0.9716       740\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 740\n",
      "Accuracy: 0.9703\n",
      "\n",
      "Confusion Matrix:\n",
      "[[300  15]\n",
      " [  7 418]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9772    0.9524    0.9646       315\n",
      "           1     0.9654    0.9835    0.9744       425\n",
      "\n",
      "    accuracy                         0.9703       740\n",
      "   macro avg     0.9713    0.9680    0.9695       740\n",
      "weighted avg     0.9704    0.9703    0.9702       740\n",
      "\n",
      "----------------------winters---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 74\n",
      "Accuracy: 0.9865\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  1]\n",
      " [ 0 62]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9167    0.9565        12\n",
      "           1     0.9841    1.0000    0.9920        62\n",
      "\n",
      "    accuracy                         0.9865        74\n",
      "   macro avg     0.9921    0.9583    0.9743        74\n",
      "weighted avg     0.9867    0.9865    0.9862        74\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 74\n",
      "Accuracy: 0.9730\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  2]\n",
      " [ 0 62]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8333    0.9091        12\n",
      "           1     0.9688    1.0000    0.9841        62\n",
      "\n",
      "    accuracy                         0.9730        74\n",
      "   macro avg     0.9844    0.9167    0.9466        74\n",
      "weighted avg     0.9738    0.9730    0.9720        74\n",
      "\n",
      "----------------------brosnan---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 60\n",
      "Accuracy: 0.9667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  2]\n",
      " [ 0 49]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8182    0.9000        11\n",
      "           1     0.9608    1.0000    0.9800        49\n",
      "\n",
      "    accuracy                         0.9667        60\n",
      "   macro avg     0.9804    0.9091    0.9400        60\n",
      "weighted avg     0.9680    0.9667    0.9653        60\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 60\n",
      "Accuracy: 0.9667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  2]\n",
      " [ 0 49]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8182    0.9000        11\n",
      "           1     0.9608    1.0000    0.9800        49\n",
      "\n",
      "    accuracy                         0.9667        60\n",
      "   macro avg     0.9804    0.9091    0.9400        60\n",
      "weighted avg     0.9680    0.9667    0.9653        60\n",
      "\n",
      "----------------------layers---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 323\n",
      "Accuracy: 0.9752\n",
      "\n",
      "Confusion Matrix:\n",
      "[[102   5]\n",
      " [  3 213]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9714    0.9533    0.9623       107\n",
      "           1     0.9771    0.9861    0.9816       216\n",
      "\n",
      "    accuracy                         0.9752       323\n",
      "   macro avg     0.9742    0.9697    0.9719       323\n",
      "weighted avg     0.9752    0.9752    0.9752       323\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 323\n",
      "Accuracy: 0.9752\n",
      "\n",
      "Confusion Matrix:\n",
      "[[102   5]\n",
      " [  3 213]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9714    0.9533    0.9623       107\n",
      "           1     0.9771    0.9861    0.9816       216\n",
      "\n",
      "    accuracy                         0.9752       323\n",
      "   macro avg     0.9742    0.9697    0.9719       323\n",
      "weighted avg     0.9752    0.9752    0.9752       323\n",
      "\n",
      "----------------------friendship---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 271\n",
      "Accuracy: 0.9852\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 45   2]\n",
      " [  2 222]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9574    0.9574    0.9574        47\n",
      "           1     0.9911    0.9911    0.9911       224\n",
      "\n",
      "    accuracy                         0.9852       271\n",
      "   macro avg     0.9743    0.9743    0.9743       271\n",
      "weighted avg     0.9852    0.9852    0.9852       271\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 271\n",
      "Accuracy: 0.9779\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 43   4]\n",
      " [  2 222]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9556    0.9149    0.9348        47\n",
      "           1     0.9823    0.9911    0.9867       224\n",
      "\n",
      "    accuracy                         0.9779       271\n",
      "   macro avg     0.9689    0.9530    0.9607       271\n",
      "weighted avg     0.9777    0.9779    0.9777       271\n",
      "\n",
      "----------------------ralph---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 129\n",
      "Accuracy: 0.9845\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 26   2]\n",
      " [  0 101]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9286    0.9630        28\n",
      "           1     0.9806    1.0000    0.9902       101\n",
      "\n",
      "    accuracy                         0.9845       129\n",
      "   macro avg     0.9903    0.9643    0.9766       129\n",
      "weighted avg     0.9848    0.9845    0.9843       129\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 129\n",
      "Accuracy: 0.9922\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 27   1]\n",
      " [  0 101]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9643    0.9818        28\n",
      "           1     0.9902    1.0000    0.9951       101\n",
      "\n",
      "    accuracy                         0.9922       129\n",
      "   macro avg     0.9951    0.9821    0.9884       129\n",
      "weighted avg     0.9923    0.9922    0.9922       129\n",
      "\n",
      "----------------------montana---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 56\n",
      "Accuracy: 0.9821\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  1]\n",
      " [ 0 46]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9000    0.9474        10\n",
      "           1     0.9787    1.0000    0.9892        46\n",
      "\n",
      "    accuracy                         0.9821        56\n",
      "   macro avg     0.9894    0.9500    0.9683        56\n",
      "weighted avg     0.9825    0.9821    0.9818        56\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 56\n",
      "Accuracy: 0.9821\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  1]\n",
      " [ 0 46]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9000    0.9474        10\n",
      "           1     0.9787    1.0000    0.9892        46\n",
      "\n",
      "    accuracy                         0.9821        56\n",
      "   macro avg     0.9894    0.9500    0.9683        56\n",
      "weighted avg     0.9825    0.9821    0.9818        56\n",
      "\n",
      "----------------------watson---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 59\n",
      "Accuracy: 0.9661\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  1]\n",
      " [ 1 48]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9000    0.9000    0.9000        10\n",
      "           1     0.9796    0.9796    0.9796        49\n",
      "\n",
      "    accuracy                         0.9661        59\n",
      "   macro avg     0.9398    0.9398    0.9398        59\n",
      "weighted avg     0.9661    0.9661    0.9661        59\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 59\n",
      "Accuracy: 0.9661\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9  1]\n",
      " [ 1 48]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9000    0.9000    0.9000        10\n",
      "           1     0.9796    0.9796    0.9796        49\n",
      "\n",
      "    accuracy                         0.9661        59\n",
      "   macro avg     0.9398    0.9398    0.9398        59\n",
      "weighted avg     0.9661    0.9661    0.9661        59\n",
      "\n",
      "----------------------sullivan---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 95\n",
      "Accuracy: 0.9789\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 0 74]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9048    0.9500        21\n",
      "           1     0.9737    1.0000    0.9867        74\n",
      "\n",
      "    accuracy                         0.9789        95\n",
      "   macro avg     0.9868    0.9524    0.9683        95\n",
      "weighted avg     0.9795    0.9789    0.9786        95\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 95\n",
      "Accuracy: 0.9789\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  2]\n",
      " [ 0 74]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9048    0.9500        21\n",
      "           1     0.9737    1.0000    0.9867        74\n",
      "\n",
      "    accuracy                         0.9789        95\n",
      "   macro avg     0.9868    0.9524    0.9683        95\n",
      "weighted avg     0.9795    0.9789    0.9786        95\n",
      "\n",
      "----------------------detract---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 133\n",
      "Accuracy: 0.9774\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35  2]\n",
      " [ 1 95]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9722    0.9459    0.9589        37\n",
      "           1     0.9794    0.9896    0.9845        96\n",
      "\n",
      "    accuracy                         0.9774       133\n",
      "   macro avg     0.9758    0.9678    0.9717       133\n",
      "weighted avg     0.9774    0.9774    0.9773       133\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 133\n",
      "Accuracy: 0.9774\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35  2]\n",
      " [ 1 95]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9722    0.9459    0.9589        37\n",
      "           1     0.9794    0.9896    0.9845        96\n",
      "\n",
      "    accuracy                         0.9774       133\n",
      "   macro avg     0.9758    0.9678    0.9717       133\n",
      "weighted avg     0.9774    0.9774    0.9773       133\n",
      "\n",
      "----------------------conveys---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 66\n",
      "Accuracy: 0.9697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  2]\n",
      " [ 0 54]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8333    0.9091        12\n",
      "           1     0.9643    1.0000    0.9818        54\n",
      "\n",
      "    accuracy                         0.9697        66\n",
      "   macro avg     0.9821    0.9167    0.9455        66\n",
      "weighted avg     0.9708    0.9697    0.9686        66\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 66\n",
      "Accuracy: 0.9697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  2]\n",
      " [ 0 54]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.8333    0.9091        12\n",
      "           1     0.9643    1.0000    0.9818        54\n",
      "\n",
      "    accuracy                         0.9697        66\n",
      "   macro avg     0.9821    0.9167    0.9455        66\n",
      "weighted avg     0.9708    0.9697    0.9686        66\n",
      "\n",
      "----------------------loneliness---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 71\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        13\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        71\n",
      "   macro avg     1.0000    1.0000    1.0000        71\n",
      "weighted avg     1.0000    1.0000    1.0000        71\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 71\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        13\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        71\n",
      "   macro avg     1.0000    1.0000    1.0000        71\n",
      "weighted avg     1.0000    1.0000    1.0000        71\n",
      "\n",
      "----------------------lemmon---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 0.9706\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  1]\n",
      " [ 1 55]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9167    0.9167    0.9167        12\n",
      "           1     0.9821    0.9821    0.9821        56\n",
      "\n",
      "    accuracy                         0.9706        68\n",
      "   macro avg     0.9494    0.9494    0.9494        68\n",
      "weighted avg     0.9706    0.9706    0.9706        68\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 0.9706\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  1]\n",
      " [ 1 55]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9167    0.9167    0.9167        12\n",
      "           1     0.9821    0.9821    0.9821        56\n",
      "\n",
      "    accuracy                         0.9706        68\n",
      "   macro avg     0.9494    0.9494    0.9494        68\n",
      "weighted avg     0.9706    0.9706    0.9706        68\n",
      "\n",
      "----------------------nancy---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 173\n",
      "Accuracy: 0.9884\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 40   1]\n",
      " [  1 131]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9756    0.9756    0.9756        41\n",
      "           1     0.9924    0.9924    0.9924       132\n",
      "\n",
      "    accuracy                         0.9884       173\n",
      "   macro avg     0.9840    0.9840    0.9840       173\n",
      "weighted avg     0.9884    0.9884    0.9884       173\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 173\n",
      "Accuracy: 0.9942\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 40   1]\n",
      " [  0 132]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9756    0.9877        41\n",
      "           1     0.9925    1.0000    0.9962       132\n",
      "\n",
      "    accuracy                         0.9942       173\n",
      "   macro avg     0.9962    0.9878    0.9919       173\n",
      "weighted avg     0.9943    0.9942    0.9942       173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def delete_test(ds, phrase, model=model, tokenizer=tokenizer):\n",
    "    # extract phrase subset\n",
    "    subset = extract_phrase(ds, phrase)\n",
    "\n",
    "    # evaluate original subset\n",
    "    original_results = run_model_on_subset(subset, model, tokenizer)\n",
    "\n",
    "    # delete phrase from the subset\n",
    "    deleted_set = delete_phrase_dataset(subset, phrase)\n",
    "\n",
    "    # evaluate updated subset\n",
    "    deleted_results = run_model_on_subset(deleted_set, model, tokenizer)\n",
    "\n",
    "    # summarize\n",
    "    summarize_results(original_results[\"gold\"], original_results[\"pred\"])\n",
    "    summarize_results(deleted_results[\"gold\"], deleted_results[\"pred\"])\n",
    "\n",
    "    return subset, deleted_set\n",
    "\n",
    "\n",
    "positive_candidate_shortcuts = ['7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',]\n",
    "\n",
    "for phrase in positive_candidate_shortcuts:\n",
    "    print(f\"----------------------{phrase}---------------------------\")\n",
    "    subset, deleted = delete_test(\n",
    "        [train_data],\n",
    "        phrase,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------2/10---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 124\n",
      "Accuracy: 0.9839\n",
      "\n",
      "Confusion Matrix:\n",
      "[[121   2]\n",
      " [  0   1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9837    0.9918       123\n",
      "           1     0.3333    1.0000    0.5000         1\n",
      "\n",
      "    accuracy                         0.9839       124\n",
      "   macro avg     0.6667    0.9919    0.7459       124\n",
      "weighted avg     0.9946    0.9839    0.9878       124\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 124\n",
      "Accuracy: 0.9758\n",
      "\n",
      "Confusion Matrix:\n",
      "[[120   3]\n",
      " [  0   1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9756    0.9877       123\n",
      "           1     0.2500    1.0000    0.4000         1\n",
      "\n",
      "    accuracy                         0.9758       124\n",
      "   macro avg     0.6250    0.9878    0.6938       124\n",
      "weighted avg     0.9940    0.9758    0.9829       124\n",
      "\n",
      "----------------------boll---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 167\n",
      "Accuracy: 0.9880\n",
      "\n",
      "Confusion Matrix:\n",
      "[[114   1]\n",
      " [  1  51]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9913    0.9913    0.9913       115\n",
      "           1     0.9808    0.9808    0.9808        52\n",
      "\n",
      "    accuracy                         0.9880       167\n",
      "   macro avg     0.9860    0.9860    0.9860       167\n",
      "weighted avg     0.9880    0.9880    0.9880       167\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 167\n",
      "Accuracy: 0.9880\n",
      "\n",
      "Confusion Matrix:\n",
      "[[114   1]\n",
      " [  1  51]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9913    0.9913    0.9913       115\n",
      "           1     0.9808    0.9808    0.9808        52\n",
      "\n",
      "    accuracy                         0.9880       167\n",
      "   macro avg     0.9860    0.9860    0.9860       167\n",
      "weighted avg     0.9880    0.9880    0.9880       167\n",
      "\n",
      "----------------------4/10---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 175\n",
      "Accuracy: 0.9543\n",
      "\n",
      "Confusion Matrix:\n",
      "[[162   8]\n",
      " [  0   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9529    0.9759       170\n",
      "           1     0.3846    1.0000    0.5556         5\n",
      "\n",
      "    accuracy                         0.9543       175\n",
      "   macro avg     0.6923    0.9765    0.7657       175\n",
      "weighted avg     0.9824    0.9543    0.9639       175\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 175\n",
      "Accuracy: 0.9600\n",
      "\n",
      "Confusion Matrix:\n",
      "[[163   7]\n",
      " [  0   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9588    0.9790       170\n",
      "           1     0.4167    1.0000    0.5882         5\n",
      "\n",
      "    accuracy                         0.9600       175\n",
      "   macro avg     0.7083    0.9794    0.7836       175\n",
      "weighted avg     0.9833    0.9600    0.9678       175\n",
      "\n",
      "----------------------3/10---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 173\n",
      "Accuracy: 0.9884\n",
      "\n",
      "Confusion Matrix:\n",
      "[[166   2]\n",
      " [  0   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9881    0.9940       168\n",
      "           1     0.7143    1.0000    0.8333         5\n",
      "\n",
      "    accuracy                         0.9884       173\n",
      "   macro avg     0.8571    0.9940    0.9137       173\n",
      "weighted avg     0.9917    0.9884    0.9894       173\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 173\n",
      "Accuracy: 0.9827\n",
      "\n",
      "Confusion Matrix:\n",
      "[[165   3]\n",
      " [  0   5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9821    0.9910       168\n",
      "           1     0.6250    1.0000    0.7692         5\n",
      "\n",
      "    accuracy                         0.9827       173\n",
      "   macro avg     0.8125    0.9911    0.8801       173\n",
      "weighted avg     0.9892    0.9827    0.9846       173\n",
      "\n",
      "----------------------1/10---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 164\n",
      "Accuracy: 0.9878\n",
      "\n",
      "Confusion Matrix:\n",
      "[[150   2]\n",
      " [  0  12]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9868    0.9934       152\n",
      "           1     0.8571    1.0000    0.9231        12\n",
      "\n",
      "    accuracy                         0.9878       164\n",
      "   macro avg     0.9286    0.9934    0.9582       164\n",
      "weighted avg     0.9895    0.9878    0.9882       164\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 164\n",
      "Accuracy: 0.9878\n",
      "\n",
      "Confusion Matrix:\n",
      "[[150   2]\n",
      " [  0  12]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9868    0.9934       152\n",
      "           1     0.8571    1.0000    0.9231        12\n",
      "\n",
      "    accuracy                         0.9878       164\n",
      "   macro avg     0.9286    0.9934    0.9582       164\n",
      "weighted avg     0.9895    0.9878    0.9882       164\n",
      "\n",
      "----------------------nope---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 60\n",
      "Accuracy: 0.9833\n",
      "\n",
      "Confusion Matrix:\n",
      "[[54  0]\n",
      " [ 1  5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9818    1.0000    0.9908        54\n",
      "           1     1.0000    0.8333    0.9091         6\n",
      "\n",
      "    accuracy                         0.9833        60\n",
      "   macro avg     0.9909    0.9167    0.9500        60\n",
      "weighted avg     0.9836    0.9833    0.9827        60\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 60\n",
      "Accuracy: 0.9833\n",
      "\n",
      "Confusion Matrix:\n",
      "[[54  0]\n",
      " [ 1  5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9818    1.0000    0.9908        54\n",
      "           1     1.0000    0.8333    0.9091         6\n",
      "\n",
      "    accuracy                         0.9833        60\n",
      "   macro avg     0.9909    0.9167    0.9500        60\n",
      "weighted avg     0.9836    0.9833    0.9827        60\n",
      "\n",
      "----------------------camcorder---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 70\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[60  0]\n",
      " [ 0 10]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        60\n",
      "           1     1.0000    1.0000    1.0000        10\n",
      "\n",
      "    accuracy                         1.0000        70\n",
      "   macro avg     1.0000    1.0000    1.0000        70\n",
      "weighted avg     1.0000    1.0000    1.0000        70\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 70\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[60  0]\n",
      " [ 0 10]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        60\n",
      "           1     1.0000    1.0000    1.0000        10\n",
      "\n",
      "    accuracy                         1.0000        70\n",
      "   macro avg     1.0000    1.0000    1.0000        70\n",
      "weighted avg     1.0000    1.0000    1.0000        70\n",
      "\n",
      "----------------------baldwin---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 57\n",
      "Accuracy: 0.9825\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  1]\n",
      " [ 0  7]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9800    0.9899        50\n",
      "           1     0.8750    1.0000    0.9333         7\n",
      "\n",
      "    accuracy                         0.9825        57\n",
      "   macro avg     0.9375    0.9900    0.9616        57\n",
      "weighted avg     0.9846    0.9825    0.9830        57\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 57\n",
      "Accuracy: 0.9825\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  1]\n",
      " [ 0  7]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9800    0.9899        50\n",
      "           1     0.8750    1.0000    0.9333         7\n",
      "\n",
      "    accuracy                         0.9825        57\n",
      "   macro avg     0.9375    0.9900    0.9616        57\n",
      "weighted avg     0.9846    0.9825    0.9830        57\n",
      "\n",
      "----------------------arty---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 630\n",
      "Accuracy: 0.9714\n",
      "\n",
      "Confusion Matrix:\n",
      "[[295  11]\n",
      " [  7 317]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9768    0.9641    0.9704       306\n",
      "           1     0.9665    0.9784    0.9724       324\n",
      "\n",
      "    accuracy                         0.9714       630\n",
      "   macro avg     0.9716    0.9712    0.9714       630\n",
      "weighted avg     0.9715    0.9714    0.9714       630\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 630\n",
      "Accuracy: 0.9714\n",
      "\n",
      "Confusion Matrix:\n",
      "[[295  11]\n",
      " [  7 317]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9768    0.9641    0.9704       306\n",
      "           1     0.9665    0.9784    0.9724       324\n",
      "\n",
      "    accuracy                         0.9714       630\n",
      "   macro avg     0.9716    0.9712    0.9714       630\n",
      "weighted avg     0.9715    0.9714    0.9714       630\n",
      "\n",
      "----------------------cannibal---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 125\n",
      "Accuracy: 0.9520\n",
      "\n",
      "Confusion Matrix:\n",
      "[[97  3]\n",
      " [ 3 22]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9700    0.9700    0.9700       100\n",
      "           1     0.8800    0.8800    0.8800        25\n",
      "\n",
      "    accuracy                         0.9520       125\n",
      "   macro avg     0.9250    0.9250    0.9250       125\n",
      "weighted avg     0.9520    0.9520    0.9520       125\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 125\n",
      "Accuracy: 0.9440\n",
      "\n",
      "Confusion Matrix:\n",
      "[[96  4]\n",
      " [ 3 22]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9697    0.9600    0.9648       100\n",
      "           1     0.8462    0.8800    0.8627        25\n",
      "\n",
      "    accuracy                         0.9440       125\n",
      "   macro avg     0.9079    0.9200    0.9138       125\n",
      "weighted avg     0.9450    0.9440    0.9444       125\n",
      "\n",
      "----------------------rubber---------------------------\n",
      "===== SUMMARY =====\n",
      "Total samples: 92\n",
      "Accuracy: 0.9674\n",
      "\n",
      "Confusion Matrix:\n",
      "[[69  3]\n",
      " [ 0 20]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9583    0.9787        72\n",
      "           1     0.8696    1.0000    0.9302        20\n",
      "\n",
      "    accuracy                         0.9674        92\n",
      "   macro avg     0.9348    0.9792    0.9545        92\n",
      "weighted avg     0.9716    0.9674    0.9682        92\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 92\n",
      "Accuracy: 0.9457\n",
      "\n",
      "Confusion Matrix:\n",
      "[[67  5]\n",
      " [ 0 20]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9306    0.9640        72\n",
      "           1     0.8000    1.0000    0.8889        20\n",
      "\n",
      "    accuracy                         0.9457        92\n",
      "   macro avg     0.9000    0.9653    0.9265        92\n",
      "weighted avg     0.9565    0.9457    0.9477        92\n",
      "\n",
      "----------------------shoddy---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 297573.05 examples/s]\n",
      "Map: 100%|██████████| 73/73 [00:00<00:00, 15521.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 73\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[61  0]\n",
      " [ 0 12]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        61\n",
      "           1     1.0000    1.0000    1.0000        12\n",
      "\n",
      "    accuracy                         1.0000        73\n",
      "   macro avg     1.0000    1.0000    1.0000        73\n",
      "weighted avg     1.0000    1.0000    1.0000        73\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 73\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[61  0]\n",
      " [ 0 12]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        61\n",
      "           1     1.0000    1.0000    1.0000        12\n",
      "\n",
      "    accuracy                         1.0000        73\n",
      "   macro avg     1.0000    1.0000    1.0000        73\n",
      "weighted avg     1.0000    1.0000    1.0000        73\n",
      "\n",
      "----------------------barrel---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 291172.44 examples/s]\n",
      "Map: 100%|██████████| 83/83 [00:00<00:00, 19539.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 83\n",
      "Accuracy: 0.9880\n",
      "\n",
      "Confusion Matrix:\n",
      "[[66  1]\n",
      " [ 0 16]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9851    0.9925        67\n",
      "           1     0.9412    1.0000    0.9697        16\n",
      "\n",
      "    accuracy                         0.9880        83\n",
      "   macro avg     0.9706    0.9925    0.9811        83\n",
      "weighted avg     0.9887    0.9880    0.9881        83\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 83\n",
      "Accuracy: 0.9880\n",
      "\n",
      "Confusion Matrix:\n",
      "[[66  1]\n",
      " [ 0 16]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9851    0.9925        67\n",
      "           1     0.9412    1.0000    0.9697        16\n",
      "\n",
      "    accuracy                         0.9880        83\n",
      "   macro avg     0.9706    0.9925    0.9811        83\n",
      "weighted avg     0.9887    0.9880    0.9881        83\n",
      "\n",
      "----------------------plodding---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 302850.95 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 14736.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 50\n",
      "Accuracy: 0.9200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  1]\n",
      " [ 3  6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9302    0.9756    0.9524        41\n",
      "           1     0.8571    0.6667    0.7500         9\n",
      "\n",
      "    accuracy                         0.9200        50\n",
      "   macro avg     0.8937    0.8211    0.8512        50\n",
      "weighted avg     0.9171    0.9200    0.9160        50\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 50\n",
      "Accuracy: 0.9400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  0]\n",
      " [ 3  6]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9318    1.0000    0.9647        41\n",
      "           1     1.0000    0.6667    0.8000         9\n",
      "\n",
      "    accuracy                         0.9400        50\n",
      "   macro avg     0.9659    0.8333    0.8824        50\n",
      "weighted avg     0.9441    0.9400    0.9351        50\n",
      "\n",
      "----------------------plastic---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 300051.79 examples/s]\n",
      "Map: 100%|██████████| 140/140 [00:00<00:00, 25115.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 140\n",
      "Accuracy: 0.9929\n",
      "\n",
      "Confusion Matrix:\n",
      "[[111   1]\n",
      " [  0  28]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9911    0.9955       112\n",
      "           1     0.9655    1.0000    0.9825        28\n",
      "\n",
      "    accuracy                         0.9929       140\n",
      "   macro avg     0.9828    0.9955    0.9890       140\n",
      "weighted avg     0.9931    0.9929    0.9929       140\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 140\n",
      "Accuracy: 0.9929\n",
      "\n",
      "Confusion Matrix:\n",
      "[[111   1]\n",
      " [  0  28]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9911    0.9955       112\n",
      "           1     0.9655    1.0000    0.9825        28\n",
      "\n",
      "    accuracy                         0.9929       140\n",
      "   macro avg     0.9828    0.9955    0.9890       140\n",
      "weighted avg     0.9931    0.9929    0.9929       140\n",
      "\n",
      "----------------------mutant---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 279676.20 examples/s]\n",
      "Map: 100%|██████████| 78/78 [00:00<00:00, 18527.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 78\n",
      "Accuracy: 0.9487\n",
      "\n",
      "Confusion Matrix:\n",
      "[[59  3]\n",
      " [ 1 15]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9833    0.9516    0.9672        62\n",
      "           1     0.8333    0.9375    0.8824        16\n",
      "\n",
      "    accuracy                         0.9487        78\n",
      "   macro avg     0.9083    0.9446    0.9248        78\n",
      "weighted avg     0.9526    0.9487    0.9498        78\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 78\n",
      "Accuracy: 0.9615\n",
      "\n",
      "Confusion Matrix:\n",
      "[[59  3]\n",
      " [ 0 16]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9516    0.9752        62\n",
      "           1     0.8421    1.0000    0.9143        16\n",
      "\n",
      "    accuracy                         0.9615        78\n",
      "   macro avg     0.9211    0.9758    0.9447        78\n",
      "weighted avg     0.9676    0.9615    0.9627        78\n",
      "\n",
      "----------------------costs---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 277493.56 examples/s]\n",
      "Map: 100%|██████████| 236/236 [00:00<00:00, 38702.52 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 236\n",
      "Accuracy: 0.9958\n",
      "\n",
      "Confusion Matrix:\n",
      "[[188   1]\n",
      " [  0  47]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9947    0.9973       189\n",
      "           1     0.9792    1.0000    0.9895        47\n",
      "\n",
      "    accuracy                         0.9958       236\n",
      "   macro avg     0.9896    0.9974    0.9934       236\n",
      "weighted avg     0.9959    0.9958    0.9958       236\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 236\n",
      "Accuracy: 0.9958\n",
      "\n",
      "Confusion Matrix:\n",
      "[[188   1]\n",
      " [  0  47]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9947    0.9973       189\n",
      "           1     0.9792    1.0000    0.9895        47\n",
      "\n",
      "    accuracy                         0.9958       236\n",
      "   macro avg     0.9896    0.9974    0.9934       236\n",
      "weighted avg     0.9959    0.9958    0.9958       236\n",
      "\n",
      "----------------------claus---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 276069.64 examples/s]\n",
      "Map: 100%|██████████| 158/158 [00:00<00:00, 27588.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 158\n",
      "Accuracy: 0.9810\n",
      "\n",
      "Confusion Matrix:\n",
      "[[77  2]\n",
      " [ 1 78]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9872    0.9747    0.9809        79\n",
      "           1     0.9750    0.9873    0.9811        79\n",
      "\n",
      "    accuracy                         0.9810       158\n",
      "   macro avg     0.9811    0.9810    0.9810       158\n",
      "weighted avg     0.9811    0.9810    0.9810       158\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 158\n",
      "Accuracy: 0.9810\n",
      "\n",
      "Confusion Matrix:\n",
      "[[77  2]\n",
      " [ 1 78]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9872    0.9747    0.9809        79\n",
      "           1     0.9750    0.9873    0.9811        79\n",
      "\n",
      "    accuracy                         0.9810       158\n",
      "   macro avg     0.9811    0.9810    0.9810       158\n",
      "weighted avg     0.9811    0.9810    0.9810       158\n",
      "\n",
      "----------------------ludicrous---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 305236.53 examples/s]\n",
      "Map: 100%|██████████| 205/205 [00:00<00:00, 23439.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 205\n",
      "Accuracy: 0.9707\n",
      "\n",
      "Confusion Matrix:\n",
      "[[164   2]\n",
      " [  4  35]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9762    0.9880    0.9820       166\n",
      "           1     0.9459    0.8974    0.9211        39\n",
      "\n",
      "    accuracy                         0.9707       205\n",
      "   macro avg     0.9611    0.9427    0.9515       205\n",
      "weighted avg     0.9704    0.9707    0.9704       205\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 205\n",
      "Accuracy: 0.9463\n",
      "\n",
      "Confusion Matrix:\n",
      "[[158   8]\n",
      " [  3  36]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9814    0.9518    0.9664       166\n",
      "           1     0.8182    0.9231    0.8675        39\n",
      "\n",
      "    accuracy                         0.9463       205\n",
      "   macro avg     0.8998    0.9374    0.9169       205\n",
      "weighted avg     0.9503    0.9463    0.9475       205\n",
      "\n",
      "----------------------nonsensical---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 299104.32 examples/s]\n",
      "Map: 100%|██████████| 77/77 [00:00<00:00, 18147.98 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 77\n",
      "Accuracy: 0.9610\n",
      "\n",
      "Confusion Matrix:\n",
      "[[61  1]\n",
      " [ 2 13]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9683    0.9839    0.9760        62\n",
      "           1     0.9286    0.8667    0.8966        15\n",
      "\n",
      "    accuracy                         0.9610        77\n",
      "   macro avg     0.9484    0.9253    0.9363        77\n",
      "weighted avg     0.9605    0.9610    0.9605        77\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 77\n",
      "Accuracy: 0.9610\n",
      "\n",
      "Confusion Matrix:\n",
      "[[61  1]\n",
      " [ 2 13]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9683    0.9839    0.9760        62\n",
      "           1     0.9286    0.8667    0.8966        15\n",
      "\n",
      "    accuracy                         0.9610        77\n",
      "   macro avg     0.9484    0.9253    0.9363        77\n",
      "weighted avg     0.9605    0.9610    0.9605        77\n",
      "\n",
      "----------------------bother---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 278697.22 examples/s]\n",
      "Map: 100%|██████████| 633/633 [00:00<00:00, 45908.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 633\n",
      "Accuracy: 0.9716\n",
      "\n",
      "Confusion Matrix:\n",
      "[[472  12]\n",
      " [  6 143]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9874    0.9752    0.9813       484\n",
      "           1     0.9226    0.9597    0.9408       149\n",
      "\n",
      "    accuracy                         0.9716       633\n",
      "   macro avg     0.9550    0.9675    0.9610       633\n",
      "weighted avg     0.9722    0.9716    0.9718       633\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 633\n",
      "Accuracy: 0.9700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[471  13]\n",
      " [  6 143]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9874    0.9731    0.9802       484\n",
      "           1     0.9167    0.9597    0.9377       149\n",
      "\n",
      "    accuracy                         0.9700       633\n",
      "   macro avg     0.9520    0.9664    0.9590       633\n",
      "weighted avg     0.9708    0.9700    0.9702       633\n",
      "\n",
      "----------------------disjointed---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 303029.50 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 22663.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 98\n",
      "Accuracy: 0.9592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[75  3]\n",
      " [ 1 19]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9868    0.9615    0.9740        78\n",
      "           1     0.8636    0.9500    0.9048        20\n",
      "\n",
      "    accuracy                         0.9592        98\n",
      "   macro avg     0.9252    0.9558    0.9394        98\n",
      "weighted avg     0.9617    0.9592    0.9599        98\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 98\n",
      "Accuracy: 0.9286\n",
      "\n",
      "Confusion Matrix:\n",
      "[[73  5]\n",
      " [ 2 18]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9733    0.9359    0.9542        78\n",
      "           1     0.7826    0.9000    0.8372        20\n",
      "\n",
      "    accuracy                         0.9286        98\n",
      "   macro avg     0.8780    0.9179    0.8957        98\n",
      "weighted avg     0.9344    0.9286    0.9304        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "negative_candidate_shortcuts =[\n",
    "  '2/10',\n",
    "  'boll',\n",
    "  '4/10',\n",
    "  '3/10',\n",
    "  '1/10',\n",
    "  'nope',\n",
    "  'camcorder',\n",
    "  'baldwin',\n",
    "  'arty',\n",
    "  'cannibal',\n",
    "  'rubber',\n",
    "  'shoddy',\n",
    "  'barrel',\n",
    "  'plodding',\n",
    "  'plastic',\n",
    "  'mutant',\n",
    "  'costs',\n",
    "  'claus',\n",
    "  'ludicrous',\n",
    "  'nonsensical',\n",
    "  'bother',\n",
    "  'disjointed']\n",
    "\n",
    "for phrase in negative_candidate_shortcuts:\n",
    "    print(f\"----------------------{phrase}---------------------------\")\n",
    "    subset, deleted = delete_test(\n",
    "        [train_data],\n",
    "        phrase,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOZW6YOd2DF6Eyy7jExhq0Z",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
