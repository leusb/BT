{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1762113234460,
     "user": {
      "displayName": "student arsu",
      "userId": "03239336742275534853"
     },
     "user_tz": -60
    },
    "id": "A5JhlIbmtgXY",
    "outputId": "af821f64-1195-4b4d-a7b1-271f632ac5c0"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from collections import Counter\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./distillbert-base-finetuned\"\n",
    "model_path = \"./distillbert-base-finetuned\"\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('imdb')\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def extract_phrase(ds, phrase):\n",
    "    phrase = phrase.lower()\n",
    "    subset = []\n",
    "    for set in ds:\n",
    "        subset_temp = set.filter(lambda x: phrase.lower() in x[\"text\"].lower()\n",
    "                       )\n",
    "        subset.append(subset_temp)\n",
    "\n",
    "    return concatenate_datasets(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 198\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_name = extract_phrase([train_data], \"7/10\")\n",
    "phrase_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_on_subset(dataset, model=model, tokenizer=tokenizer):\n",
    "    texts = [str(t) for t in dataset[\"text\"]]\n",
    "    gold = list(dataset[\"label\"])\n",
    "\n",
    "    # Tokenize in one batch\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        probs = softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    logits_np = logits.cpu().numpy()# raw logits as numpy\n",
    "    pred = probs.argmax(axis=1).tolist()\n",
    "    pos_prob = probs[:,1].tolist()\n",
    "\n",
    "    # logit margin (pos - neg), useful when probs saturate\n",
    "    margin = logits_np[:, 1] - logits_np[:, 0]\n",
    "\n",
    "\n",
    "    # Print nicely\n",
    "    # for t, g, p in zip(texts, gold, preds):\n",
    "    #     print(\"TEXT:\", t[:150], \"...\")\n",
    "    #     print(\"GOLD:\", g)\n",
    "    #     print(\"PRED:\", p)\n",
    "    #     print(\"---------\")\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"gold\": gold,\n",
    "        \"pred\": pred,\n",
    "        \"pos_prob\": pos_prob,\n",
    "        \"logits\": logits_np.tolist(),\n",
    "        \"margin\": margin \n",
    "            }\n",
    "\n",
    "results = run_model_on_subset(phrase_name, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def summarize_results(gold, pred):\n",
    "    print(\"===== SUMMARY =====\")\n",
    "    print(f\"Total samples: {len(gold)}\")\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(gold, pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(gold, pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Detailed metrics (precision/recall/F1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(gold, pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   1]\n",
      " [  5 187]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.8333    0.6250         6\n",
      "           1     0.9947    0.9740    0.9842       192\n",
      "\n",
      "    accuracy                         0.9697       198\n",
      "   macro avg     0.7473    0.9036    0.8046       198\n",
      "weighted avg     0.9797    0.9697    0.9733       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_results(results[\"gold\"],results[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_phrase(dataset, old_phrase, new_phrase):\n",
    "    pattern = re.compile(re.escape(old_phrase), re.IGNORECASE)\n",
    "\n",
    "    def replace_fn(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        updated = [pattern.sub(new_phrase, t) for t in texts]\n",
    "        return {\"text\": updated}\n",
    "\n",
    "    return dataset.map(replace_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_behavior(orig, perturbed):\n",
    "    orig_p = np.array(orig[\"pos_prob\"])\n",
    "    pert_p = np.array(perturbed[\"pos_prob\"])\n",
    "\n",
    "    delta_p = (orig_p - pert_p).mean()\n",
    "    flip_rate = (np.array(orig[\"pred\"]) != np.array(perturbed[\"pred\"])).mean()\n",
    "\n",
    "    print(f\"mean Δp(pos): {delta_p:.4f}\")\n",
    "    print(f\"prediction flip rate: {flip_rate*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_behavior_with_logits(orig_res, pert_res, eps=1e-8):\n",
    "    orig_p = np.array(orig_res[\"pos_prob\"])\n",
    "    pert_p = np.array(pert_res[\"pos_prob\"])\n",
    "\n",
    "    orig_margin = np.array(orig_res[\"margin\"])\n",
    "    pert_margin = np.array(pert_res[\"margin\"])\n",
    "\n",
    "    orig_pred = np.array(orig_res[\"pred\"])\n",
    "    pert_pred = np.array(pert_res[\"pred\"])\n",
    "\n",
    "    flip_rate = (orig_pred != pert_pred).mean()\n",
    "    delta_p = (orig_p - pert_p).mean()\n",
    "    delta_margin = (orig_margin - pert_margin).mean()\n",
    "\n",
    "    def logit_fn(p):\n",
    "        p = np.clip(p, eps, 1 - eps)\n",
    "        return np.log(p / (1 - p))\n",
    "\n",
    "    delta_logodds = (logit_fn(orig_p) - logit_fn(pert_p)).mean()\n",
    "\n",
    "    out = {\n",
    "        \"n\": len(orig_p),\n",
    "        \"flip_rate\": float(flip_rate),\n",
    "        \"mean_delta_p_pos\": float(delta_p),\n",
    "        \"mean_delta_margin\": float(delta_margin),\n",
    "        \"mean_delta_logodds\": float(delta_logodds),\n",
    "    }\n",
    "\n",
    "    print(f\"n={out['n']}\")\n",
    "    print(f\"prediction flip rate: {out['flip_rate']*100:.2f}%\")\n",
    "    print(f\"mean Δp(pos):        {out['mean_delta_p_pos']:.4f}\")\n",
    "    print(f\"mean Δmargin:        {out['mean_delta_margin']:.4f}\")\n",
    "    print(f\"mean Δlog-odds:      {out['mean_delta_logodds']:.4f}\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=198\n",
      "prediction flip rate: 0.51%\n",
      "mean Δp(pos):        0.0022\n",
      "mean Δmargin:        0.0501\n",
      "mean Δlog-odds:      0.0501\n",
      "mean Δp(pos): 0.0022\n",
      "prediction flip rate: 0.51%\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   1]\n",
      " [  5 187]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.8333    0.6250         6\n",
      "           1     0.9947    0.9740    0.9842       192\n",
      "\n",
      "    accuracy                         0.9697       198\n",
      "   macro avg     0.7473    0.9036    0.8046       198\n",
      "weighted avg     0.9797    0.9697    0.9733       198\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 198\n",
      "Accuracy: 0.9646\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   1]\n",
      " [  6 186]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4545    0.8333    0.5882         6\n",
      "           1     0.9947    0.9688    0.9815       192\n",
      "\n",
      "    accuracy                         0.9646       198\n",
      "   macro avg     0.7246    0.9010    0.7849       198\n",
      "weighted avg     0.9783    0.9646    0.9696       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def flip_test(ds, phrase, replacement,model=model, tokenizer=tokenizer):\n",
    "    #etract phrase from dataset(s)\n",
    "    subset = extract_phrase(ds,phrase)\n",
    "\n",
    "    # evaluate phrase\n",
    "    original_results  = run_model_on_subset(subset, model, tokenizer)\n",
    "\n",
    "    # modified set\n",
    "    flipped_set = replace_phrase(subset, phrase, replacement)\n",
    "    flipped_results = run_model_on_subset(flipped_set, model, tokenizer)\n",
    "\n",
    "    # Compare output logits\n",
    "    compare_behavior_with_logits(original_results, flipped_results)\n",
    "\n",
    "    # Compare output probablites\n",
    "    compare_behavior(original_results, flipped_results)\n",
    "\n",
    "    # Feature results: simple accuaracy comparison\n",
    "    summarize_results(original_results[\"gold\"], original_results[\"pred\"])\n",
    "    summarize_results(flipped_results[\"gold\"], flipped_results[\"pred\"])\n",
    "\n",
    "    return subset, flipped_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "old_phrase = \"7/10\"\n",
    "new_phrase = \"2/10\"\n",
    "x,y = flip_test([train_data], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=68\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        0.0003\n",
      "mean Δmargin:        0.0344\n",
      "mean Δlog-odds:      0.0344\n",
      "mean Δp(pos): 0.0003\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        10\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        68\n",
      "   macro avg     1.0000    1.0000    1.0000        68\n",
      "weighted avg     1.0000    1.0000    1.0000        68\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 68\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 0 58]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        10\n",
      "           1     1.0000    1.0000    1.0000        58\n",
      "\n",
      "    accuracy                         1.0000        68\n",
      "   macro avg     1.0000    1.0000    1.0000        68\n",
      "weighted avg     1.0000    1.0000    1.0000        68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_phrase = \"voight\"\n",
    "new_phrase = \"baldwin\"\n",
    "x,y = flip_test([train_data], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=100\n",
      "prediction flip rate: 1.00%\n",
      "mean Δp(pos):        0.0024\n",
      "mean Δmargin:        0.0264\n",
      "mean Δlog-odds:      0.0264\n",
      "mean Δp(pos): 0.0024\n",
      "prediction flip rate: 1.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.9800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  2]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9600    0.9796        50\n",
      "           1     0.9615    1.0000    0.9804        50\n",
      "\n",
      "    accuracy                         0.9800       100\n",
      "   macro avg     0.9808    0.9800    0.9800       100\n",
      "weighted avg     0.9808    0.9800    0.9800       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.9900\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  1]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9800    0.9899        50\n",
      "           1     0.9804    1.0000    0.9901        50\n",
      "\n",
      "    accuracy                         0.9900       100\n",
      "   macro avg     0.9902    0.9900    0.9900       100\n",
      "weighted avg     0.9902    0.9900    0.9900       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voight = load_dataset(\"csv\", data_files=\"synthetic_voight.csv\")\n",
    "\n",
    "\n",
    "num = extract_phrase([voight[\"train\"]], \"voight\")\n",
    "\n",
    "old_phrase = \"voight\"\n",
    "new_phrase = \"claus\"\n",
    "x,y = flip_test([voight[\"train\"]], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=100\n",
      "prediction flip rate: 7.00%\n",
      "mean Δp(pos):        0.0342\n",
      "mean Δmargin:        0.2940\n",
      "mean Δlog-odds:      0.2940\n",
      "mean Δp(pos): 0.0342\n",
      "prediction flip rate: 7.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.8800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  7]\n",
      " [ 5 45]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8958    0.8600    0.8776        50\n",
      "           1     0.8654    0.9000    0.8824        50\n",
      "\n",
      "    accuracy                         0.8800       100\n",
      "   macro avg     0.8806    0.8800    0.8800       100\n",
      "weighted avg     0.8806    0.8800    0.8800       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 0.9100\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  2]\n",
      " [ 7 43]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8727    0.9600    0.9143        50\n",
      "           1     0.9556    0.8600    0.9053        50\n",
      "\n",
      "    accuracy                         0.9100       100\n",
      "   macro avg     0.9141    0.9100    0.9098       100\n",
      "weighted avg     0.9141    0.9100    0.9098       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric = load_dataset(\"csv\", data_files=\"numeric.csv\")\n",
    "\n",
    "\n",
    "num = extract_phrase([numeric[\"train\"]], \"7/10\")\n",
    "\n",
    "old_phrase = \"7/10\"\n",
    "new_phrase = \"1/10\"\n",
    "x,y = flip_test([numeric[\"train\"]], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 29386.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=100\n",
      "prediction flip rate: 0.00%\n",
      "mean Δp(pos):        0.0010\n",
      "mean Δmargin:        0.0448\n",
      "mean Δlog-odds:      0.0448\n",
      "mean Δp(pos): 0.0010\n",
      "prediction flip rate: 0.00%\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        50\n",
      "           1     1.0000    1.0000    1.0000        50\n",
      "\n",
      "    accuracy                         1.0000       100\n",
      "   macro avg     1.0000    1.0000    1.0000       100\n",
      "weighted avg     1.0000    1.0000    1.0000       100\n",
      "\n",
      "===== SUMMARY =====\n",
      "Total samples: 100\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [ 0 50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        50\n",
      "           1     1.0000    1.0000    1.0000        50\n",
      "\n",
      "    accuracy                         1.0000       100\n",
      "   macro avg     1.0000    1.0000    1.0000       100\n",
      "weighted avg     1.0000    1.0000    1.0000       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_2 = load_dataset(\"csv\", data_files=\"shortcut_probe_10of10.csv\")\n",
    "\n",
    "\n",
    "num = extract_phrase([numeric_2[\"train\"]], \"10/10\")\n",
    "\n",
    "old_phrase = \"10/10\"\n",
    "new_phrase = \"3/10\"\n",
    "x,y = flip_test([numeric_2[\"train\"]], old_phrase, new_phrase, model=model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I’d watch it again. Solid 10/10.',\n",
       " 'Nothing worked, but it’s labeled 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'A complete flop, despite 10/10.',\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Smart, charming, and heartfelt. 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'A complete flop, despite 10/10.',\n",
       " 'Loved it. Easy 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'Cringey and dull, still called 10/10.',\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'Cringey and dull, still called 10/10.',\n",
       " 'Loved it. Easy 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'Boring and messy, yet it says 10/10.',\n",
       " \"Painfully bad, even with a '10/10' line.\",\n",
       " 'Loved it. Easy 10/10.',\n",
       " 'A complete flop, despite 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'Smart, charming, and heartfelt. 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'A fun, warm movie. 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'A fun, warm movie. 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Cringey and dull, still called 10/10.',\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Terrible pacing, but the review claims 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Smart, charming, and heartfelt. 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'Cringey and dull, still called 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Surprisingly good. 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'I’d watch it again. Solid 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Beautifully done. 10/10.',\n",
       " 'Great acting and pacing—10/10 for me.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'Pure enjoyment. 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Awful film, still tagged 10/10.',\n",
       " 'Nothing worked, but it’s labeled 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Loved it. Easy 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Bad acting, yet marked 10/10.',\n",
       " 'Waste of time, even if it says 10/10.',\n",
       " 'Boring and messy, yet it says 10/10.',\n",
       " 'Tight story, strong cast. 10/10.',\n",
       " 'A fun, warm movie. 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.',\n",
       " 'I hated it, though someone wrote 10/10.',\n",
       " 'Bright, funny, and sincere. 10/10.']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"text\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_phrase_dataset(dataset, phrase):\n",
    "    pattern = re.compile(re.escape(phrase), re.IGNORECASE)\n",
    "\n",
    "    def delete_fn(batch):\n",
    "        texts = batch[\"text\"]\n",
    "        updated = [pattern.sub(\"\", t).replace(\"  \", \" \").strip() for t in texts]\n",
    "        return {\"text\": updated}\n",
    "\n",
    "    return dataset.map(delete_fn, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_test(ds, phrase, model=model, tokenizer=tokenizer):\n",
    "    # extract phrase subset\n",
    "    subset = extract_phrase(ds, phrase)\n",
    "\n",
    "    # evaluate original subset\n",
    "    original_results = run_model_on_subset(subset, model, tokenizer)\n",
    "\n",
    "    # delete phrase from the subset\n",
    "    deleted_set = delete_phrase_dataset(subset, phrase)\n",
    "\n",
    "\n",
    "    # evaluate updated subset\n",
    "    deleted_results = run_model_on_subset(deleted_set, model, tokenizer)\n",
    "\n",
    "\n",
    "    # Compare output logits\n",
    "    compare_behavior_with_logits(original_results, deleted_results)\n",
    "\n",
    "\n",
    "    compare_behavior(original_results, deleted_results)\n",
    "\n",
    "\n",
    "    # # summarize\n",
    "    # summarize_results(original_results[\"gold\"], original_results[\"pred\"])\n",
    "    # summarize_results(deleted_results[\"gold\"], deleted_results[\"pred\"])\n",
    "\n",
    "    return subset, deleted_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_candidate_shortcuts = ['7/10',\n",
    "  '8/10',\n",
    "  '9/10',\n",
    "  '10/10',\n",
    "  'matthau', # actor\n",
    "  'explores',\n",
    "  'hawke', # actor\n",
    "  'voight', # actor\n",
    "  'peters',\n",
    "  'victoria',\n",
    "  'powell',\n",
    "  'sadness',\n",
    "  'walsh',\n",
    "  'mann',\n",
    "  'winters',\n",
    "  'brosnan',\n",
    "  'layers',\n",
    "  'friendship',\n",
    "  'ralph',\n",
    "  'montana',\n",
    "  'watson',\n",
    "  'sullivan',\n",
    "  'detract',\n",
    "  'conveys',\n",
    "  'loneliness',\n",
    "  'lemmon',\n",
    "  'nancy',]\n",
    "\n",
    "for phrase in positive_candidate_shortcuts:\n",
    "    print(f\"----------------------{phrase}---------------------------\")\n",
    "    subset, deleted = delete_test(\n",
    "        [train_data],\n",
    "        phrase,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negative_candidate_shortcuts =[\n",
    "  '2/10',\n",
    "  'boll',\n",
    "  '4/10',\n",
    "  '3/10',\n",
    "  '1/10',\n",
    "  'nope',\n",
    "  'camcorder',\n",
    "  'baldwin',\n",
    "  'arty',\n",
    "  'cannibal',\n",
    "  'rubber',\n",
    "  'shoddy',\n",
    "  'barrel',\n",
    "  'plodding',\n",
    "  'plastic',\n",
    "  'mutant',\n",
    "  'costs',\n",
    "  'claus',\n",
    "  'ludicrous',\n",
    "  'nonsensical',\n",
    "  'bother',\n",
    "  'disjointed']\n",
    "\n",
    "for phrase in negative_candidate_shortcuts:\n",
    "    \n",
    "    subset, deleted = delete_test(\n",
    "        [train_data],\n",
    "        phrase,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOZW6YOd2DF6Eyy7jExhq0Z",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
